145,Zoltan Haindrich,2023-05-12 01:30:41-05,ab2cccb6e2b10008c2604af96be71ed8709ecb22,Post-release 2.11.0,Adjust the upgrade/downgrade scripts and add the tests.  (cherry picked from commit d5fea0a842cbd38d2d72db16e9e67f1c9b1ccf36)
239,Fabrízio de Royes Mello,2023-04-04 15:31:33-05,6440bb3477eef183459272cb90f6ffa8bf30b682,Remove unused function,Remove unused function `invalidation_threshold_htid_found`.
305,Maheedhar PV,2023-02-21 11:33:46-06,c8c50dad7eca4f7425bfb9980b872a1c44201ceb,Post-release fixes for 2.10.0,Bumping the previous version and adding tests for 2.10.0
308,Sven Klemm,2023-02-20 06:31:19-06,09766343997aa903f9d6a1ab14bcfc49a0045864,Set name for COPY insert buffer hash table,Having the hash table named makes debugging easier as the name is used for the MemoryContext used by the hash table.
379,Mats Kindahl,2023-01-16 02:24:32-06,8f4fa8e4cca73f11d3892ce6afde04ca104465d2,Add build matrix to Windows and Linux builds,"Build matrix is missing from the ignore workflows for the Windows and Linux builds, so this commit adds them."
429,Matvey Arye,2022-12-07 20:14:06-06,df16815009b6353383c720e364e1b3d2c82f8867,Fix memory leak for compression with merge chunks,The RelationInitIndexAccessInfo call leaks cache memory and seems to be unnecessary.
377,Jan Nidzwetzki,2023-01-17 01:46:21-06,19065bbdf39d2536a227041331adf24ffb38ffdc,Introduce a FDW option to mark reference tables,"With this patch, the ability to mark reference tables (tables that exist on all data nodes of a multi-node installation) via an FDW option has been added."
458,Fabrízio de Royes Mello,2022-11-22 14:35:00-06,35fa891013bcba87515b026e7e7b0abb728f1a54,Add missing gitignore entry,Pull request #4998 introduced a new template SQL test file but missed to add the properly `.gitignore` entry to ignore generated test files.
494,Markos Fountoulakis,2022-11-10 02:55:22-06,e2b7c76c9c1d53edf15ea3d4d01d666a675a7c5f,Disable MERGE when using hypertables,Fixes #4930  Co-authored-by: Lakshmi Narayanan Sreethar <lakshmi@timescale.com>
0,Lakshmi Narayanan Sreethar,2023-09-05 10:33:21-05,44e41c12ab25e36c202f58e068ced262eadc8d16,Fix segfault in set_integer_now_func,"When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  Fixes #6037"
1,Bharathy,2023-09-01 22:54:31-05,e66a40038e3c84fb1a68da67ad71caf75c64a027,Fix server crash on UPDATE of compressed chunk,UPDATE query with system attributes in WHERE clause causes server to crash. This patch fixes this issue by checking for system attributes and handle cases only for segmentby attributes in fill_predicate_context().  Fixes #6024
304,noctarius aka Christoph Engelbert,2023-01-09 10:20:38-06,0118e6b9520a4c45363e2c92d4664a91dd77e786,Support CAGG names in hypertable_(detailed_)size,This small patch adds support for continuous aggregates to the `hypertable_detailed_size` (and with that `hypertable_size`). It adds an additional check to see if a continuous aggregate exists if a hypertable with the given regclass name isn't found.
597,Sven Klemm,2022-09-27 12:49:42-05,940187936c93b6d78e702ded75d79d914f8fea96,Fix segfault when INNER JOINing hypertables,This fixing a segfault when INNER JOINing 2 hypertables that are ordered by time.
495,Fabrízio de Royes Mello,2022-11-11 11:42:22-06,9e276c58ee7b99ae479f4a544de12bbbcc84c211,"Revert ""Upload test results into the database""",This reverts commit 252cefb509153fadcb32741a27ec3fa977487049 because it broke our CI globally.
464,Fabrízio de Royes Mello,2022-11-18 13:26:05-06,a5b8c9b084aa6edd5eb8aeb38cc0ab16ecfd8ac4,Fix caggs on caggs tests on PG15,PR #4668 introduced the Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate) but unfortunately we miss to fix the regression tests on PG15.
683,Sven Klemm,2022-08-18 02:20:38-05,1f6d69720d1efd53ec42df5fca5e54a85898a5c6,Add link to forum to README,This also replaces appveyor badge with link to Windows GitHub actions.
740,Sven Klemm,2022-07-25 02:43:32-05,3ea06f4984b4141dba4209ad2d4e431f149a3f55,Remove ubuntu 21.10 (impish) from package tests,Ubuntu 21.10 is EOL and we no longer build packages for it.
306,Jan Nidzwetzki,2023-01-24 02:48:07-06,e0be9eaa281527ee1cc8569ce025cfe64dca8574,Allow pushdown of reference table joins,"This patch adds the functionality that is needed to perform distributed, parallel joins on reference tables on access nodes. This code allows the pushdown of a join if:   * (1) The setting ""ts_guc_enable_per_data_node_queries"" is enabled  * (2) The outer relation is a distributed hypertable  * (3) The inner relation is marked as a reference table  * (4) The join is a left join or an inner join"
307,Dmitry Simonenko,2023-02-21 11:17:20-06,f12a361ef7de9566113ea79617d49d62597b4bd2,Add timeout argument to the ping_data_node(),This PR introduces a timeout argument and a new logic to the timescale_internal.ping_data_node() function which allows to handle io timeouts for nodes being unresponsive.  Fix #5312
741,Sven Klemm,2022-07-25 02:47:01-05,661a6507edd0ec9c9c14b9317a6178aa4fd866b5,Fix coccinelle CI action,Switch coccinelle CI action to ubuntu 22.04 because 21.10 is EOL and stopped working.
575,Sven Klemm,2022-10-08 09:55:42-05,2defb2b0b374b150dc1e678c03f621fb2f59ecf8,Improve job_crash_log test,Older versions seens to have problems when undef is passed to poll_query_until so we change the call to pass explicit query instead.
2,Jan Nidzwetzki,2023-08-29 14:13:51-05,c6a930897e9f9e9878db031cc7fb6ea79d721a74,Use Debian Bookworm for 32-bit tests,"So far, we have used Debian Buster (10) for our 32-bit tests. This distribution is EOL in ~1 year and contains an old LLVM version (7.0). LLVM 7 contains a few bugs that break the JIT functionality of PostgreSQL (missing mixed-sign 64-bit operands on 32-bit architectures / failed to resolve name __mulodi4).  This patch changes the used Distribution for 32-bit tests to Debian Bookworm (12 / LLVM 14). Since the PostgreSQL download server no longer offers 32-bit Debian packages, PostgreSQL is built from source."
3,Lakshmi Narayanan Sreethar,2023-08-28 12:49:22-05,8e941b80ae1b0e0b6affe5431454cdc637628d99,Fix incorrect row count in EXPLAIN ANALYZE INSERT .. ON CONFLICT output,INSERT ... ON CONFLICT statements record few metrics in the ModifyTable node's instrument but they get overwritten by hypertable_modify_explain causing wrong output in EXPLAIN ANALYZE statments. Fix it by saving the metrics into HypertableModify node before replacing them.  Fixes #6014
901,Alexander Kuzmenkov,2022-04-21 08:56:05-05,a1e76d2e84a0b5c53bf8864f51c7cc365be2e231,Follow-up for compressed chunk collation #4236,Add a changelog message and add a check for broken chunks to the update script.
4,Lakshmi Narayanan Sreethar,2023-05-30 10:02:29-05,caada43454e25d3098744fa6b675ac7d07390550,PG16: Fix concurrent update issues with MERGE.,PG16 commit postgres/postgres@9321c79c fixes an issue with concurrent update issues in MERGE. This patch adapts that fix into the MERGE support for hypertables as well.  postgres/postgres@9321c79c
921,Sven Klemm,2022-04-11 14:46:57-05,bdaa4607d4fa2322e65f779a24f3fa42f47b5b52,Post release 2.6.1,Add 2.6.1 to update and downgrade tests.
786,Alexander Kuzmenkov,2022-06-21 02:56:09-05,5c69adfb7e0fcda27d89c1b98eee2f9f94639e5d,Add more tests for errors on data nodes,Use a data type with faulty send/recv functions to test various error handling paths.
5,Sven Klemm,2023-08-29 11:13:24-05,e4facda540286b0affba47ccc63959fefe2a7b26,Add compatibility layer for _timescaledb_internal functions,With timescaledb 2.12 all the functions present in _timescaledb_internal were moved into the _timescaledb_functions schema to improve schema security. This patch adds a compatibility layer so external callers of these internal functions will not break and allow for more flexibility when migrating.
859,Sven Klemm,2022-05-14 09:38:13-05,54f56af3efba2a3735a406ccb0211584055ee440,Support wildcards in IGNORES,Add support for wildcards in IGNORES for regression tests.
6,Jan Nidzwetzki,2023-08-31 02:56:38-05,77dc6ed42c6d0b65dd971566c2184e9bd7008e68,Fix non-deterministic cagg_insert isolation test,"One permutation of the cagg_insert isolation test contains an assumption about the execution order of three processes after a lock is released. However, this behavior is non-deterministic. This PR makes the assumption explicit and adds proper markers to the isolation test to make the output deterministic."
860,Sven Klemm,2022-05-17 07:00:07-05,a7c8641e04ca01d9e3a7973a7b3c817a430004dd,Bump postgres versions used in CI,"Bump postgres versions used in CI to 14.3, 13.7 and 12.11."
7,Jan Nidzwetzki,2023-08-29 07:47:57-05,08231c8aacd17152f315ad36d95c031fb46073aa,Export is_decompress_chunk_path / is_gapfill_path,This patch adds the 'ts_' prefix to the function names of is_decompress_chunk_path and is_gapfill_path and makes them available for use by other parts of TimescaleDB.
8,Jan Nidzwetzki,2023-08-30 16:49:34-05,fa04a067e0dce3a09b229cf2e6c127984cb9896f,Fix an invalid SPI result use after free,This PR fixes the invalid use of an SPI result after SPI_finish is called and the result is freed.
9,Sven Klemm,2023-08-30 02:15:34-05,e3437786ad7e3b5f152167ce0081e546178b8a12,Make multinode tests conditional,Since multinode tests take a long time to run we dont want to run them in CI on individual PRs but only on nightly runs.
1031,gayyappan,2022-01-20 12:23:12-06,9f64df8567abdbe6af11a9388acefb8237629ff5,Add ts_catalog subdirectory,Move files that are related to timescaledb catalog access to this subdirectory
10,Sven Klemm,2023-08-29 05:52:06-05,3b6dc7dc013572f91db181ccbbe4854f596714dd,Move partialize functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - finalize_agg_ffunc(internal,text,name,name,name[],bytea,anyelement) - finalize_agg_sfunc(internal,text,name,name,name[],bytea,anyelement) - partialize_agg(anyelement) - finalize_agg(text,name,name,name[][],bytea,anyelement)"
468,Fabrízio de Royes Mello,2022-11-16 13:34:53-06,b1742969d09114cd5138abe16f5c70dfbf864e4f,Add SQL test files to trailing whitespace CI check,In commit 1f807153 we added a CI check for trailing whitespaces over our source code files (.c and .h).  This commit add SQL test files (.sql and .sql.in) to this check.
11,Alexander Kuzmenkov,2023-08-29 05:39:17-05,623381ce99978b7f05f32ec1f5c117345ef6cd8e,Reread the catalog data after locking the chunk,"The compression status can change, and this is prevented by locking, so to keep our data consistent, we should reread the chunk metadata after we have locked it. Currently we have some code in place that masks this inconsistency by rereading the metadata in another place. This code is removed."
12,Sven Klemm,2023-08-29 03:49:47-05,a9751ccd5eb030026d7b975d22753f5964972389,Move partitioning functions to _timescaledb_functions schema,To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - get_partition_for_key(val anyelement) - get_partition_hash(val anyelement)
13,Sven Klemm,2023-08-28 16:26:23-05,b2a91494a11d8b82849b6f11f9ea6dc26ef8a8cb,Move ddl_internal functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - chunk_constraint_add_table_constraint(_timescaledb_catalog.chunk_constraint) - chunk_drop_replica(regclass,name) - chunk_index_clone(oid) - chunk_index_replace(oid,oid) - create_chunk_replica_table(regclass,name) - drop_stale_chunks(name,integer[]) - health() - hypertable_constraint_add_table_fk_constraint(name,name,name,integer) - process_ddl_event() - wait_subscription_sync(name,name,integer,numeric)"
14,Jan Nidzwetzki,2023-08-29 01:02:48-05,4516df285c962f801722019868fc0a982ed43a57,Make up/downgrade test deterministic,"Two queries in post.continuous_aggs.v3.sql had no ORDER BY specification. Therefore, the query output was not deterministic. This patch adds the missing ORDER BY."
15,Sven Klemm,2023-08-28 08:32:54-05,6576d969b319dac8e7fd08a9cf4cfc8197b34d1d,Move log invalidation functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - cagg_watermark(integer) - cagg_watermark_materialized(integer) - hypertable_invalidation_log_delete(integer) - invalidation_cagg_log_add_entry(integer,bigint,bigint) - invalidation_hyper_log_add_entry(integer,bigint,bigint) - invalidation_process_cagg_log(integer,integer,regtype,bigint,bigint,integer[],bigint[],bigint[]) - invalidation_process_cagg_log(integer,integer,regtype,bigint,bigint,integer[],bigint[],bigint[],text[]) - invalidation_process_hypertable_log(integer,integer,regtype,integer[],bigint[],bigint[]) - invalidation_process_hypertable_log(integer,integer,regtype,integer[],bigint[],bigint[],text[]) - materialization_invalidation_log_delete(integer)"
16,Sven Klemm,2023-08-28 01:38:26-05,28c7457faf9b909ea89b26b61cfa5a8428e2c23c,Move scheduler functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - restart_background_workers() - stop_background_workers() - start_background_workers() - alter_job_set_hypertable_id(integer,regclass)"
17,James Guthrie,2023-08-10 06:23:01-05,01e480d5d668f7fbfc81800d4aad6c5ee61ba227,Account for uncompressed rows in 'create_compressed_chunk',"`_timescaledb_internal.create_compressed_chunk` can be used to create a compressed chunk with existing compressed data. It did not account for the fact that the chunk can contain uncompressed data, in which case the chunk status must be set to partial.  Fixes #5946"
18,Fabrízio de Royes Mello,2023-08-27 06:20:04-05,a323547e691f7ca9ad5d0b3924071a51b0a365cd,Move cagg_migrate functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - cagg_migrate_create_plan(_timescaledb_catalog.continuous_agg,text,boolean,boolean) - cagg_migrate_execute_copy_data(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_execute_copy_policies(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_execute_create_new_cagg(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_execute_disable_policies(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_execute_drop_old_cagg(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_execute_enable_policies(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_execute_override_cagg(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_execute_plan(_timescaledb_catalog.continuous_agg) - cagg_migrate_execute_refresh_new_cagg(_timescaledb_catalog.continuous_agg,_timescaledb_catalog.continuous_agg_migrate_plan_step) - cagg_migrate_plan_exists(integer) - cagg_migrate_pre_validation(text,text,text)  Co-authored-by: Fabrízio de Royes Mello <fabriziomello@gmail.com Co-authored-by: Sven Klemm <sven@timescale.com>"
19,Sven Klemm,2023-08-27 06:20:04-05,e02b1f348eb4c48def00b7d5227238b4d9d41a4a,Simplify schema move update script,Use dynamic sql to create the ALTER FUNCTION statements for those functions that may not exist in previous versions.
20,Sven Klemm,2023-08-23 13:59:47-05,184e8398182fb3972137b4faaa64d3a08836dfd9,Move policy functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - policy_compression_check(jsonb) - policy_compression_execute(integer,integer,anyelement,integer,boolean,boolean) - policy_compression(integer,jsonb) - policy_job_error_retention_check(jsonb) - policy_job_error_retention(integer,jsonb) - policy_recompression(integer,jsonb) - policy_refresh_continuous_aggregate_check(jsonb) - policy_refresh_continuous_aggregate(integer,jsonb) - policy_reorder_check(jsonb) - policy_reorder(integer,jsonb) - policy_retention_check(jsonb) - policy_retention(integer,jsonb)"
21,Lakshmi Narayanan Sreethar,2023-07-05 13:19:07-05,6fb3c3f3f43fe70b7c5034ddfb11451df802890c,PG16: Handle updates to make_restrictinfo function,"While fixing a bug in filtering of ""cloned"" outer-join quals, PG16 adds 3 new parameters to the make_restrictinfo function. Updated the compat function to handle this change. This patch also cleans up the variants of make_restrictinfo from other versions to make it clear which args are passed.  postgres/postgres@991a3df227"
22,Lakshmi Narayanan Sreethar,2023-05-18 06:35:21-05,9425402264624f78fd052c700f53f7a736230007,PG16: Handle removed EquivalenceClass members,PG16 removes the outerjoin_delayed mechanism and RestrictInfo.nullable_relids.  postgres/postgres@b448f1c8 postgres/postgres@3bef56e1
23,Lakshmi Narayanan Sreethar,2023-08-24 09:41:44-05,3a493a47d1b5cc1a148c0040d531bf132f567e80,Remove unused functions,Removed the unused functions ts_make_pathkey_from_sortop and ts_make_pathkey_from_sortinfo.
24,Lakshmi Narayanan Sreethar,2023-05-13 01:50:42-05,040d45510430d13a9e1eda0ecb02981dfa1be39c,PG16: Datum macros are now inline functions,PG16 converted the *GetDatum() and DatumGet*() macros to inline functions. This doesn't affect most of our code but a few places like struct object initialisations cannot use inline functions. This commit updates all the places affected by this change.  postgres/postgres@c8b2ef05
491,Alexander Kuzmenkov,2022-11-09 10:01:58-06,0360812e3ccc627ab6e1cb6aab8e95f925956593,Simplify llvm configuration for linux/macos builds,Set it only in the matrixbuilder.
25,Sven Klemm,2023-08-22 07:15:13-05,0da18a93b5bd00c6b25ff3c485b2228d371b3326,Move chunk functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - calculate_chunk_interval(int, bigint, bigint) - chunk_status(regclass) - chunks_in(record, integer[]) - chunk_id_from_relid(oid) - show_chunk(regclass) - create_chunk(regclass, jsonb, name, name, regclass) - set_chunk_default_data_node(regclass, name) - get_chunk_relstats(regclass) - get_chunk_colstats(regclass) - create_chunk_table(regclass, jsonb, name, name) - freeze_chunk(regclass) - unfreeze_chunk(regclass) - drop_chunk(regclass) - attach_osm_table_chunk(regclass, regclass)"
26,Fabrízio de Royes Mello,2023-08-07 17:49:47-05,5bba74a2ec083728f8e93e09d03d102568fd72b5,Relax strong table lock when refreshing a CAGG,When refreshing a Continuous Aggregate we take a table lock on _timescaledb_catalog.continuous_aggs_invalidation_threshold when processing the invalidation logs (the first transaction of the refresh Continuous Aggregate procedure). It means that even two different Continuous Aggregates over two different hypertables will wait each other in the first phase of the refreshing procedure. Also it lead to problems when a pg_dump is running because it take an AccessShareLock on tables so Continuous Aggregate refresh execution will wait until the pg_dump finish.  Improved it by relaxing the strong table-level lock to a row-level lock so now the Continuous Aggregate refresh procedure can be executed in multiple sessions with less locks.  Fix #3554
27,Sven Klemm,2023-08-23 06:06:49-05,0f3d39574bf696e6627471ee16b5c26a12fb436c,Remove _timescaledb_internal.get_time_type,This function was used in an old version of a cagg informational view that was removed but the function itself was left in.
28,Alexander Kuzmenkov,2023-08-16 05:12:42-05,3373d43143ab8b17c49fdffd34fd35d551d33185,Don't look up entire Chunk struct for compressed chunks,It's not needed. Also add them to baserel cache.
29,Konstantina Skovola,2023-08-07 08:36:17-05,373c55662ca5f8a2993abf9b2aa7f5f4006b3229,Fix ordered append for partially compressed chunks,"In the exclusive presence of partially compressed chunks, this optimization was not applied because no pathkeys were supplied. Additionally, this patch makes sure that if applicable, the `enable_decompression_sorted_merge` optimization is chosen for the path, since it is more beneficial due to the ability to push down the sort below DecompressChunk."
30,Erik Nordström,2023-08-23 03:18:48-05,d1246ee2f30c171491c3f5b9efe0626314c37c13,Exclude PRs from bugs project board,"An `issue_comment` event is triggered by comments on both issues and PRs. However, the bugs project workflow did not filter out pull requests when the `issue_comment` event happened, so pull requests were also added to the bugs project board.  Add a conditional that filters out pull requests so that only issues are added to the board."
31,Sven Klemm,2023-08-23 02:07:14-05,e47832b51ac7b18e7441de3a1eeffe579057d8e2,Bump pgspot version to 0.6.0,pgspot 0.6.0 has a bugfix for function signature tracking to no longer consider default values as part of the function signature.
32,Sven Klemm,2023-08-22 05:01:19-05,cf04496e4b4237440274eb25e4e02472fc4e06fc,Move utility functions to _timescaledb_functions schema,To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - generate_uuid() - get_git_commit() - get_os_info() - tsl_loaded()
33,Sven Klemm,2023-08-22 02:37:40-05,183362e17baf71ae4f663709a9cb412a986aa4d8,Move size_util functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - relation_size(regclass) - data_node_hypertable_info(name, name, name) - data_node_chunk_info(name, name, name) - hypertable_local_size(name, name) - hypertable_remote_size(name, name) - chunks_local_size(name, name) - chunks_remote_size(name, name) - range_value_to_pretty(bigint, regtype) - get_approx_row_count(regclass) - data_node_compressed_chunk_stats(name, name, name) - compressed_chunk_local_stats(name, name) - compressed_chunk_remote_stats(name, name) - indexes_local_size(name, name) - data_node_index_size(name, name, name) - indexes_remote_size(name, name, name)"
34,Mats Kindahl,2023-08-21 10:35:19-05,3db692296028ea3c7407150a76aaa1feed6c5b4e,Call eq_func correctly in time_bucket_gapfill,"The equality comparison function is called using `DirectFunctionCall2Coll`, which do not set the `fcinfo->flinfo` when calling the PostgreSQL function.  Since `array_eq` uses `fcinfo->flinfo->fn_extra` for caching, and `flinfo` is null, this causes a crash.  Fix this issue by using `FunctionCall2Coll` instead, which sets `fcinfo->flinfo` before calling the PostgreSQL function.  Fixes #5981"
35,Sven Klemm,2023-08-21 11:55:35-05,4256009e4cfe3daebe199540ab8c32af34836e01,Move dist_internal functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - set_dist_id(uuid) - set_peer_dist_id(uuid) - validate_as_data_node() - show_connection_cache() - ping_data_node(name, interval) - remote_txn_heal_data_node(oid)"
36,Jan Nidzwetzki,2023-08-21 07:49:35-05,e99832727ac3f8aebd146015158490b28754ac1a,Place data in first/last function in correct mctx,"So far, the ts_bookend_deserializefunc() function has allocated the deserialized data in the current memory context. This data could be removed before the aggregation is finished. This patch moves the data into the aggregation memory context."
37,Lakshmi Narayanan Sreethar,2023-06-02 09:06:26-05,09dd20d7f73938473881b0c0d1fe2e1bb4659161,PG16: Align GUC variables initial value with boot values,"Any value hardcoded to a GUC variable will be overwritten by the boot_value when the GUC mechanism starts up. PG16 makes this more clear by checking that if a hardcoded value exists, it is same as the boot_value. The server asserts in debug builds if values are not equal.  Commit 09636092 already fixes most of our code to align the hardcoded and boot_values for various GUC variables. This patch updates only the ts_guc_max_cached_chunks_per_hypertable initialisation which had an initial value different from the boot_value.  postgres/postgres@a73952b79"
38,Lakshmi Narayanan Sreethar,2023-06-02 08:32:35-05,ba06c6eb2aac3be5ec1145b899cb747c82cca02a,PG16: Include guc header to use GetConfigOptionByName,postgres/postgres@0a20ff54f5
39,Lakshmi Narayanan Sreethar,2023-06-01 12:59:18-05,cf0f9b5bd8def79c346bf5462abe733bfeb4e192,PG16: Replace float8in_internal_opt_error with float8in_internal,PG16 updated float8in_internal_opt_error() function to use soft error reporting and renamed it to float8in_internal(). Updated the code to use the new mechanism.  postgres/postgres@ccff2d20e
40,Sven Klemm,2023-08-20 15:47:10-05,0a66bdb8d36a1879246bd652e4c28500c4b951ab,Move functions to _timescaledb_functions schema,"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - to_unix_microseconds(timestamptz) - to_timestamp(bigint) - to_timestamp_without_timezone(bigint) - to_date(bigint) - to_interval(bigint) - interval_to_usec(interval) - time_to_internal(anyelement) - subtract_integer_from_now(regclass, bigint)"
41,Sven Klemm,2023-08-21 03:10:53-05,a640d7ddf16f289c9472c2699952df1122642cd1,Fix psql \if expression,"The expression part of the psql \if cannot contain actual SQL expression and is instead much more limited.  A valid value is any unambiguous case-insensitive match for one of: true, false, 1, 0, on, off, yes, no.  See https://www.postgresql.org/docs/current/app-psql.html"
42,Sven Klemm,2023-08-17 02:59:25-05,0dd06e919f7cee9d7f7672b5244e7122d0c66436,Move get_create_command into _timescaledb_functions schema,To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the get_create_command function.
43,Sven Klemm,2023-04-13 06:16:14-05,56ea8b4de93cefc38e002202d8ac96947dcbaa77,Move trigger functions to _timescaledb_functions schema,To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for our trigger functions.
44,Konstantina Skovola,2023-08-17 06:15:21-05,2080c3c0f0d1fd8b90e9fb57491eaf936131f03f,Post-release fixes for 2.11.2,Bumping the previous version and adding tests for 2.11.2
45,Lakshmi Narayanan Sreethar,2023-06-01 12:31:46-05,beb3b39599940082a7b0f4bf0d1d76e4500e5ebb,PG16: vacuum_set_xid_limits is now vacuum_get_cutoffs,PG16 refactors how VACUUM passes around its XID cutoffs. A new dedicated struct is now used by VACUUM to maintain the XID/MXID cutoffs such as FreezeLimit and OldestXmin. The vacuum_set_xid_limits() function is also now replaced with a a new vacuum_get_cutoffs() function that uses the new struct.  postgres/postgres@4ce3afb8
46,Lakshmi Narayanan Sreethar,2023-06-01 12:47:57-05,f92cab8c2a81e5e0d7643e6fbaeed1a371b7bf07,PG16: Rename append_pathkeys to append_pathkeys_custom,Renamed append_pathkeys() to ts_append_pathkeys to prevent conflict with upstream changes  postgres/postgres@1349d279
47,Lakshmi Narayanan Sreethar,2023-07-05 12:16:44-05,3438636a05936f4d20f05b42ac6e6e20d8ca736f,PG16: Macro HeapKeyTest is now an inline function,postgres/postgres@4eb3b112
48,Lakshmi Narayanan Sreethar,2023-06-01 12:43:56-05,8d2dc760b7738d772f4d43e7ed8b32f9c7ed11e8,"PG16: When removing a relation from the query, drop its RelOptInfo.","PG16 removes the notion of ""dead relation"" and instead recommends deleting the relation's RelOptInfo from the planner's data structures when it is no longer needed.  postgres/postgres@e9a20e45"
49,Lakshmi Narayanan Sreethar,2023-05-18 11:05:56-05,82eeb6ec2a19f30d917ca8e51303e217d201b883,"PG16: Node tags T_Join, T_Plan and T_Scan have been removed","Node tags T_Join, T_Plan and T_Scan have been removed in PG16 as those nodes are of abstract type and never directly instantiated.  postgres/postgres@251154be postgres/postgres@8c73c11a"
50,Lakshmi Narayanan Sreethar,2023-05-30 11:42:39-05,b7b9a9b3ef0d1541388617db9e9b9237423eb4f1,PG16: Rename ri_RootToPartitionMap to ri_RootToChildMap,postgres/postgres@fb958b5d
51,Alexander Kuzmenkov,2023-08-15 16:18:13-05,d088b3a5d906f18f554b3322ec6d5cd00987cb4c,Do not add broken compressed join clauses,"We used to add join clauses that referenced a compressed column at the level of the compressed scan, and later remove them. This is wrong and useless, just don't add them."
52,Alexander Kuzmenkov,2023-08-11 06:22:30-05,22a2f49a2f09ef6b5809bf75bd93d016434e48f6,Fix filtering of the redundant decompress chunk clauses,"We just had a bad cast inside is_redundant_derived_clause before, because it doesn't work with IndexClauses. Filtering didn't work as a result."
53,Alexander Kuzmenkov,2023-08-15 07:28:58-05,3a27669c846dd641b5054281729a8a71c576257d,Simplify compressed pathkey lookup,"When looking up a pathkey for compressed scan, we used to do a lot of work, including a quadratic lookup through all the equivalence members, to always arrive at the same canonical pathkey we started from. Just remove this useless code for a significant planning speedup.  This uncovers two bugs in parameterization of decompressed paths and generation of equivalence members for segmentby columns, fix them as well."
54,Alexander Kuzmenkov,2023-08-10 08:32:40-05,04ce1bc498f8a2f7732a50bf673de175db97fd04,Use cached Chunk struct when considering compressed paths,"Full catalog lookups for a Chunk are expensive, avoiding them speeds up the planning."
55,Sven Klemm,2023-08-15 02:08:49-05,fb617e415058473457e26ab8e42b6637f0f4cf74,Bump Postgres versions used in CI,"Bump postgres version used to 13.12, 14.9 and 15.4"
56,Jan Nidzwetzki,2023-07-04 08:46:53-05,154bbbb01a14046c639c1b978dd1b4e004a1f3cc,Perform startup chunk exclusion in parallel leader,"The parallel version of the ChunkAppend node uses shared memory to coordinate the plan selection for the parallel workers. If the workers perform the startup exclusion individually, it may choose different subplans for each worker (e.g., due to a ""constant"" function that claims to be constant but returns different results). In that case, we have a disagreement about the plans between the workers.  This would lead to hard-to-debug problems and out-of-bounds reads when pstate->next_plan is used for subplan selection.  With this patch, startup exclusion is only performed in the parallel leader. The leader stores this information in shared memory. The parallel workers read the information from shared memory and don't perform startup exclusion."
57,Mats Kindahl,2023-08-10 06:53:57-05,1102d34f42daf24a6fdbbbcae2c6e46feb50ca8c,Remove telemetry isolation test,"The telemetry isolation test `telemetry_iso` does not test anything and does not seem to work, so it is removed. The debug waitpoint was taken in the same session, so the waitpoint was not waited on."
58,Mats Kindahl,2023-03-22 08:45:21-05,71b0168ab72b15fee539ecef3886a17d09e7d042,Add debug utilities to debug builds,"This will move the definitions of `debug_waitpoint_enable`, `debug_waitpoint_disable`, and `debug_waitpoint_id` to always be defined for debug builds and modify existing tests accordingly.  This means that it is no longer necessary to generate isolation test files from templates (in most cases), and it will be straightforward to use these functions in debug builds.  The debug utilities can be disabled by setting the option `ENABLE_DEBUG_UTILS` to `OFF`."
59,Lakshmi Narayanan Sreethar,2023-07-27 21:22:10-05,4bd704f3fcc1093641a8e772fe53bb0d633020ec,Further code cleanup after PG12 removal,Removed PG12 specific code guarded by the PG13_LT and PG13_GE macros.
60,Konstantina Skovola,2023-08-10 07:50:13-05,2cb42a62f91b0aa81e9411f44936a63f6ad9c8aa,Remove test_status calls from telemetry test,"Due to the postman-echo endpoint redirecting http requests to https, we get an unexpected 301 response in the tests, leading to repeated test failures. This commit removes these function calls."
61,Jan Nidzwetzki,2023-07-12 15:29:25-05,9a2dfbfb83efdd94340c87d90a7893c7e53bd2da,Improved parallel DecompressChunk worker selection,"This PR improves the way the number of parallel workers for the DecompressChunk node are calculated. Since 1a93c2d482b50a43c105427ad99e6ecb58fcac7f, no partial paths for small relations are generated, which could cause a fallback to a sequential plan and a performance regression. This patch ensures that for all relations, a partial path is created again."
62,Konstantina Skovola,2023-08-09 07:26:03-05,44eab9cf9bef34274c88efd37a750eaa74cd8044,Release 2.11.2,This release contains bug fixes since the 2.11.1 release. We recommend that you upgrade at the next available opportunity.  **Features** * #5923 Feature flags for TimescaleDB features  **Bugfixes** * #5680 Fix DISTINCT query with JOIN on multiple segmentby columns * #5774 Fixed two bugs in decompression sorted merge code * #5786 Ensure pg_config --cppflags are passed * #5906 Fix quoting owners in sql scripts. * #5912 Fix crash in 1-step integer policy creation  **Thanks** * @mrksngl for submitting a PR to fix extension upgrade scripts * @ericdevries for reporting an issue with DISTINCT queries using segmentby columns of compressed hypertable
63,Lakshmi Narayanan Sreethar,2023-08-09 04:54:29-05,b96d1709219a9ba6b502970437d42e5dcd70643c,"Revert ""PG16: Use new function to check vacuum permission""",This reverts commit 8b0ab4164 as the commit that introduced the new function has been reverted upstream.  postgres/postgres@95744599
348,Alexander Kuzmenkov,2023-02-01 03:15:39-06,44cd71a602ba96029001de6e97a1b44488730080,Fix the python code style check,"For an unknown reason, pip started to install an older version of prospector which is incompatible with the current pylint. Require the new prospector version explicitly."
64,noctarius aka Christoph Engelbert,2023-08-09 06:28:54-05,b5b46a3e581b222f679c2d4aa15944646d8190d9,Make logrepl markers for (partial) decompressions (#5805),"Added logical replication messages (PG14+) as markers for (partial) decompression events (mutual compression), which makes it possible to differentiate inserts happening as part of the decompression vs actual inserts by the user, and filter the former out of the event stream. While some tools may be interested in all events, synching the pure ""state"" (without internal behavior) is required for others.  As of now this PR is missing tests. I wonder if anyone has a good idea how to create an automatic test for it."
65,Lakshmi Narayanan Sreethar,2023-07-20 08:24:43-05,a9505b40951ff6eaac330df61767dbf4ca01e84c,PG16: Replace pg_class_ownercheck() with object_ownercheck,PG16 replaces pg_foo_ownercheck() functions with a common object_ownercheck() function. Added a new compat function for pg_class_ownercheck() function affected by this change and replaced all its callers.  postgres/postgres@afbfc029
66,Lakshmi Narayanan Sreethar,2023-07-20 08:24:32-05,22ea5771ad1a7928439b74f6cf3cd53872163c02,PG16: Make aclcheck function calls compatible with PG16,PG16 replaced most of the aclcheck functions with a common object_aclcheck function. Updated the various aclcheck calls in the code to use the new function when compiled with PG16.  postgres/postgres@c727f511
67,Lakshmi Narayanan Sreethar,2023-05-22 10:52:27-05,b2b3acf6ac1f580c0c6737386f6360b912eb6c8f,PG16: No need to pass create_new_ph flag to find_placeholder_info,"PG16 also optimized the PlaceFolderInfo lookups to perform in constant time, so there is no need to do an additional cheap/quick test using bms_overlap to see if the PHV might be evaluated in the outer rels.  postgres/postgres@6569ca439 postgres/postgres@b3ff6c742"
68,Lakshmi Narayanan Sreethar,2023-08-02 14:36:38-05,8abe14807229e691f5b75d37a6e465a58c3de886,PG16: stringToQualifiedNameList requires escontext parameter,postgres/postgres@858e776c84f
69,Nikhil Sontakke,2023-08-04 03:38:03-05,592da23633e49dd3ad97922d64cdcfc22d219d90,Fix assert in debug wait points,Need to ensure that we should try to take a lock only if a valid transaction is around. Otherwise assert is hit due to an error within an error.  Fixes #5917
70,Lakshmi Narayanan Sreethar,2023-07-20 09:20:38-05,2eb0a3883b47529dbf182475f6b81e8180d33114,PG16: Handle DefineIndex's new parameter,"PG16 adds a new parameter to DefineIndex, total_parts, that takes in the total number of direct and indirect partitions of the relation. Updated all the callers to pass either the actual number if it is known or -1 if it is unknown at that point.  postgres/postgres@27f5c712"
71,Lakshmi Narayanan Sreethar,2023-08-02 14:24:14-05,3af0d282ea71d9a8f27159a6171e9516e62ec9cb,PG16: ExecInsertIndexTuples requires additional parameter,"PG16 adds a new boolean parameter to the ExecInsertIndexTuples function to denote if the index is a BRIN index, which is then used to determine if the index update can be skipped. The fix also removes the INDEX_ATTR_BITMAP_ALL enum value.  Adapt these changes by updating the compat function to accomodate the new parameter added to the ExecInsertIndexTuples function and using an alternative for the removed INDEX_ATTR_BITMAP_ALL enum value.  postgres/postgres@19d8e23"
72,Mats Kindahl,2023-08-07 02:28:09-05,8a2b6a03e0a7da92e5cdfc0f6802f93c7cc80820,Add weird user names to update test,Since we want to be able to handle update of weird user names we add some to the update tests and create policies on them. This will create jobs with the strange name as owner.
74,Dmitry Simonenko,2023-08-03 06:30:23-05,7aeed663b9c0f337b530fd6cad47704a51a9b2ec,Feature flags for TimescaleDB features,This PR adds several GUCs which allow to enable/disable major timescaledb features:  - enable_hypertable_create - enable_hypertable_compression - enable_cagg_create - enable_policy_create
75,Markus Engel,2023-07-31 04:28:25-05,5cf354e2469ee7e43248bed382a4b49fc7ccfecd,Fix quoting owners in sql scripts.,"When referring to a role from a string type, it must be properly quoted using pg_catalog.quote_ident before it can be casted to regrole. Fixed this, especially in update scripts."
76,Dmitry Simonenko,2023-08-03 06:36:39-05,2863daf3df83c63ee36c0cf7b66c522da5b4e127,Support CREATE INDEX ONLY ON main table,This PR adds support for CREATE INDEX ONLY ON clause which allows to create index only on the main table excluding chunks.  Fix #5908
77,Lakshmi Narayanan Sreethar,2023-07-20 09:28:59-05,52ed394d4d2c13e693ff280c658c56f3eeb46f74,PG16: Remove recursion-marker values in enum AlterTableType,PG16 removed the recursion-marker values used to handle certain subcommands during an ALTER TABLE execution and provides an alternative flag. Removed the references to the recursion-marker values from timescaledb code.  postgres/postgres@840ff5f4
78,Konstantina Skovola,2023-08-01 06:59:07-05,28612ebc3cbb6e2eeb995b25203b12974ef88127,Fix crash in 1-step integer policy creation,"Previously when a retention policy existed on the underlying hypertable, we would get a segmentation fault when trying to add a Cagg refresh policy, due to passing a bool instead of pointer to bool argument to function `ts_jsonb_get_int64_field` in a particular code path. Fixed by passing the expected pointer type.  Fixes #5907"
79,Nikhil Sontakke,2023-08-01 08:03:02-05,b2773aa344c6f8fe8600830bfac417364676582d,Fix crash in COPY from program returning error,Reset the errcallback appropriately so that the ereport in case of a PROGRAM returning error can work correctly.
80,Alexander Kuzmenkov,2023-08-01 03:04:10-05,0d127f6dcc8827e28528854991cc0c22ee204141,Clean up compressed batch handling in DecompressChunk node,"* Remove unneeded data from batch states to use less memory      * keep only the compressed column data because only for them we        have to do something per row, other columns don't change * Adjust batch memory context size so that the bulk decompression   results fit into it. * Determine whether we're going to use bulk decompression for each   column at planning time, not at execution time. * Introduce ""batch queue"" to unify control flow for normal and batch   sorted merge decompression * In batch sorted merge, compare batches on scan slot, not on projected   slot. This avoids keeping the second slot in batches, and projection can be performed after we find the top batch.     * this requires some custom code to build sort infos relative to       scan tuple, not to targetlist. * Return a reference for the current top batch scan tuple as a result of   DecompressChunk exec, don't copy it out. It is guaranteed to live until the next exec, which is the usual lifetime guarantee.  This is needed to prepare for vectorized filters."
81,Alexander Kuzmenkov,2023-07-28 06:35:05-05,d5268c36fbd23fa2a93c0371998286e8688247bb,Fix SQLSmith workflow,The build was failing because it was picking up the wrong version of Postgres. Remove it.
349,Sven Klemm,2023-02-01 01:34:20-06,d8f19e57a04d17593df5f2c694eae8775faddbc7,Bump version of setup-wsl github action,The currently used version pulls in Node.js 12 which is deprecated on github.  https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/
351,Alexander Kuzmenkov,2023-01-31 05:55:55-06,f75a51def79796ff7fef58ec950c859fe4e71618,Run yamllint in CI,Helps find errors in GitHub workflows.
82,Mats Kindahl,2023-07-25 09:32:06-05,ee2ddf889e07ac20cff55013e2bf7020f99e65e1,Check unique indexes when enabling compression,"When enabling compression on a table, there is a check of unique and primary key *constraints*, but no check if there is just a unique index. This means that it is possible to create a compressed table without getting a warning that you should include the columns in the index into the segmentby field, which can lead to suboptimal query times.  This commit adds a check for the unique index as well and ensure that a similar warning is printed as for a unique constraint.  Fixes #5892"
83,Lakshmi Narayanan Sreethar,2023-07-25 05:41:35-05,61c288ec5eb966a9b4d8ed90cd026ffc5e3543c9,Fix broken CI after PG12 removal,The commit cdea343cc updated the gh_matrix_builder.py script but failed to import PG_LATEST variable into the script thus breaking the CI. Import that variable to fix the CI tests.
84,Lakshmi Narayanan Sreethar,2023-07-07 11:20:14-05,e5691bee11e4f41e4beef1eae5782912b83b2c94,Cleanup PG12 specific code from source and test files,"Removed the PG12 specific macros and all the now, dead code. Also updated the testcases which had workarounds in place to make them compatible with PG12."
85,Lakshmi Narayanan Sreethar,2023-07-07 04:23:01-05,ac33d04aa8f792e9076cdbb1162a5a367e713bf1,Remove update files for PG12,Removed the update files that were used only for PG12.
86,Lakshmi Narayanan Sreethar,2023-07-07 03:44:43-05,c3a9f90fdd972b41ad63c97a2dba352a27073361,Merge PG12 specific testfiles,Merged testfiles that were split out due to their output differing only in PG12.
88,Lakshmi Narayanan Sreethar,2023-07-06 12:07:44-05,81b520d3b5132cb483128837a0e5dc89b98ff308,Remove support for PG12,Remove support for compiling TimescaleDB code against PG12. PG12 specific macros and testfiles will be removed in a followup patch.
90,Mats Kindahl,2023-07-13 07:32:55-05,906bd38573a4752f6f3ec94e925683b5444f924c,Add job exit status and runtime to log,"When a job finishes execution, either because of an error or a success, this commit will print out the execution time of the job in the log together with a message about what job that finished.  For continuous aggregate refreshes, the number of rows deleted from and inserted into the materialization table will be printed."
91,Jan Nidzwetzki,2023-07-12 15:29:25-05,36e71000132cf3a5430849c11ff99e910ef81207,Fix duplicates on partially compressed chunk reads,"When the uncompressed part of a partially compressed chunk is read by a non-partial path and the compressed part by a partial path, the append node on top could process the uncompressed part multiple times because the path was declared as a partial path and the append node assumed it could be executed in all workers in parallel without producing duplicates.  This PR fixes the declaration of the path."
92,Rafia Sabih,2023-07-05 07:33:20-05,1bd527375d0dbe3735cc6efb297d09ff57824fe0,Rectify interval calculation,"For continuous aggregates with variable bucket size, the interval was wrongly manipulated in the process. Now it is corrected by creating a copy of interval structure for validation purposes and keeping the original structure untouched.  Fixes #5734"
93,noctarius aka Christoph Engelbert,2023-07-12 13:22:14-05,4c3d64aa988ad667ff737f20a807058d6fb754cc,Support CAGG names in chunk_detailed_size (#5839),"This patch adds support to pass continuous aggregate names to `chunk_detailed_size` to align it to the behavior of other functions such as `show_chunks`, `drop_chunks`, `hypertable_size`."
94,noctarius aka Christoph Engelbert,2023-07-12 13:21:27-05,963d4eefbff3c4eedb851efd5a51418f14ce1820,Make `set_chunk_time_interval` CAGGs aware (#5852),"This patch adds support to pass continuous aggregate names to the `set_chunk_time_interval` function to align it with functions, such as `show_chunks`, `drop_chunks`, and others.  It reuses the previously existing function to find a hypertable or resolve a continuous aggregate to its underlying hypertable found in chunk.c. It, however, moves the function to hypertable.c and exports it from here. There is some discussion if this functionality should stay in chunk.c, though, it feels wrong in that file now that it is exported."
95,noctarius aka Christoph Engelbert,2023-07-12 07:53:40-05,88aaf23ae37fe7f47252b87325eb570aa417c607,Allow Replica Identity (Alter Table) on CAGGs (#5868),"This commit is a follow up of #5515, which added support for ALTER TABLE ... REPLICA IDENTITY (FULL | INDEX) on hypertables.  This commit allows the execution against materialized hypertables to enable update / delete operations on continuous aggregates when logical replication in enabled for them."
96,Alexander Kuzmenkov,2023-07-06 11:14:08-05,eaa1206b7f01672b95ea45486bcb7602499ffd25,Improvements for bulk decompression,* Restore default batch context size to fix a performance regression on   sorted batch merge plans. * Support reverse direction. * Improve gorilla decompression by computing prefix sums of tag bitmaps   during decompression.
97,Alexander Kuzmenkov,2023-07-06 08:56:28-05,7657efe019bb020af095cd9ef3d577cb7bddd7d0,Cache the libfuzzer corpus between CI runs,This might help us find something interesting. Also add deltadelta/int8 fuzzing and make other minor improvements.
98,Jan Nidzwetzki,2023-07-04 08:50:17-05,490bc916afac6182a0537dba6dcf8c07b2735ff1,Warn if result of ts_set_flags_32 is not used,"The ts_set_flags_32 function takes a bitmap and flags and returns an updated bitmap. However, if the returned value is not used, the function call has no effect. An unused result may indicate the improper use of this function.  This patch adds the qualifier pg_nodiscard to the function which triggers a warning if the returned value is not used."
99,Konstantina Skovola,2023-06-07 07:13:09-05,06d20b1829e7a1afe392e50db17f92370ab0a9f8,Enable altering job schedule type through `alter_job`,"In #4664 we introduced fixed schedules for jobs. This was done by introducing additional parameters fixed_schedule, initial_start and timezone for our add_job and add_policy APIs. These fields were not updatable by alter_job so it was not possible to switch from one type of schedule to another without dropping and recreating existing jobs and policies. This patch adds the missing parameters to alter_job to enable switching from one type of schedule to another.  Fixes #5681"
100,Jan Nidzwetzki,2023-06-30 03:31:03-05,b9a58dd5c49da409669caa47da3db8edbcbcea44,Exclude workflow changes from being backported,"The backport script for the PRs does not have the permission to backport PRs which include workflow changes. So, these PRs are excluded from being automatically backported.  Failed CI run:  https://github.com/timescale/timescaledb/actions/runs/5387338161/    jobs/9780701395  > refusing to allow a GitHub App to create or update workflow > `.github/workflows/xxx.yaml` without `workflows` permission)"
101,Jan Nidzwetzki,2023-06-30 02:45:42-05,9bbf5218890fc8c4435b8cd1cc06ca310b243da7,Remove Ubuntu Kinetic check on ARM64,We stopped to build packages for Ubuntu Kinetic on ARM64 due to the limited support of PostgreSQL versions and the EOL of Kinetic in a few weeks. This patch removes the check for up-to-date packages for this version.
102,Jan Nidzwetzki,2023-06-30 01:22:44-05,a7be1cc06a7f0324f5ac841d3898659352aaa96c,Fixed the ordering of merge_changelogs.sh script,"The CHANGELOG.MD file contains the sections features, bugfixes, and thanks. This patch adjusts the script merge_changelogs.sh to produce the sections in the same order."
103,Jan Nidzwetzki,2023-06-29 04:11:15-05,8a581010957968cc569f5fea43fd6ab7dd3b80a6,Post-release fixes for 2.11.1,Bumping the previous version and adding tests for 2.11.1.
104,Jan Nidzwetzki,2023-06-27 05:36:29-05,8ae2da6260c7de68808db918371f8aacafce8332,Release 2.11.1,This release contains bug fixes since the 2.11.0 release. We recommend that you upgrade at the next available opportunity.  **Features** * #5679 Teach loader to load OSM extension  **Bugfixes** * #5705 Scheduler accidentally getting killed when calling `delete_job` * #5742 Fix Result node handling with ConstraintAwareAppend on   compressed chunks * #5750 Ensure tlist is present in decompress chunk plan * #5754 Fixed handling of NULL values in bookend_sfunc * #5798 Fixed batch look ahead in compressed sorted merge * #5804 Mark cagg_watermark function as PARALLEL RESTRICTED * #5807 Copy job config JSONB structure into current MemoryContext * #5824 Improve continuous aggregate query chunk exclusion  **Thanks** * @JamieD9 for reporting an issue with a wrong result ordering * @xvaara for reporting an issue with Result node handling in   ConstraintAwareAppend
105,Sven Klemm,2023-06-27 06:11:24-05,118526e6aed1afa5e559ff43d16835807572e1eb,Improve continuous aggregate query chunk exclusion,This patch changes the time_bucket exclusion in cagg queries to distinguish between < and <=. Previously those were treated the same leading to failure to exclude chunks when the constraints where exactly at the bucket boundary.
106,Erik Nordström,2023-03-31 07:22:35-05,e2e7e5f286b2282d17440961a1efa043ba054824,Make hypertables support replica identity,"Add support for setting replica identity on hypertables via ALTER TABLE. The replica identity is used in logical replication to identify rows that have changed.  Currently, replica identity can only be altered on hypertables via the root; changing it directly on chunks will raise an error."
107,Jan Nidzwetzki,2023-06-16 15:50:17-05,33a3e10f486e82df0db369c5ec2b80702576d9ec,Fixed batch look ahead in compressed sorted merge,"In decompress_sorted_merge_get_next_tuple it is determine how many batches need to be opened currently to perform a sorted merge. This is done by checking if the first tuple from the last opened batch is larger than the last returned tuple.  If a filter removes the first tuple, the first into the heap inserted tuple from this batch can no longer be used to perform the check. This patch fixes the wrong batch look ahead.  Fixes: #5797"
108,Sven Klemm,2023-06-22 03:19:05-05,da20d071cf88699cafb03f732b3e851bdc286759,Copy job config JSONB structure into current MemoryContext,The job config jsonb can be a nested structure of elements that all need to reside in the same memory context as the other job values. To ensure this we copy the structure on assignment.
109,Jan Nidzwetzki,2023-06-25 14:55:24-05,f1726790224e5444a2a2cb8c66155e4fb0b54e95,Added perltidy make target,"This patch introduces the make target 'perltidy' to format Perl files with perltidy. In addition, calling perltidy is added to 'make format'."
110,Ante Kresic,2023-06-20 08:41:30-05,fb0df1ae4e65a815c61533380f2a5ebdfc5fe1ca,Insert into indexes during chunk compression,"If there any indexes on the compressed chunk, insert into them while inserting the heap data rather than reindexing the relation at the end. This reduces the amount of locking on the compressed chunk indexes which created issues when merging chunks and should help with the future updates of compressed data."
111,Zoltan Haindrich,2023-05-19 07:04:25-05,81d4eb5cfb2c0e4949cbd109e7331539a41db152,Add Ensure-s to reduce crashes in unexpected cases,It could happen that the Chunk is dropped in the middle of processing another command. The test bgw_db_scheduler_fixed can crash due to that reason. By making sure that the system will error out instead of failing in an assertion could help avoid the situation in which postmaster drop out all clients in these cases.
112,Jan Nidzwetzki,2023-06-20 05:58:31-05,81e2f35d4b8d52ed3381cff846556611a8974cf9,Mark cagg_watermark as PARALLEL RESTRICTED,"This patch marks the function cagg_watermark as PARALLEL RESTRICTED. It partially reverts the change of c0f2ed18095f21ac737f96fe93e4035dbfeeaf2c. The reason is as follows: for transaction isolation levels < REPEATABLE READ it can not be ensured that parallel worker reads the same watermark (e.g., using read committed isolation level: worker A reads the watermark, the CAGG is refreshed and the watermark changes, worker B reads the newer watermark). The different views on the CAGG can cause unexpected results and crashes (e.g., the chunk exclusion excludes different chunks in worker A and in worker B).  In addition, a correct snapshot is used when the watermark is read from the CAGG and a TAP test is added, which detects inconsistent watermark reads.  Co-authored-by: Fabrízio de Royes Mello <fabriziomello@gmail.com> Co-authored-by: Zoltan Haindrich <zoltan@timescale.com>"
113,Konstantina Skovola,2023-06-23 07:35:15-05,a22e732c02474ce6b3ee29c4e81049c60daddc90,Fix flaky test bgw_db_scheduler_fixed,"The flakiness was due to two inserts falling into the same chunk instead of distinct ones, so inserted data further apart to ensure they fall in different chunks."
114,Zoltan Haindrich,2023-06-14 10:16:21-05,d223000036db55bdc8ef5e576a906f334272de37,Chunk_create must add existing table or fail,Earlier this function have completed successfully if the requested range already existed - regardless an existing table was supplied or not.
115,Zoltan Haindrich,2023-06-13 07:10:07-05,b2132f00a7b2b5a52af8f577d75fe54fab232443,Make validate_chunk_status accept Chunk as argument,This makes the calls to this method more straightforward and could help to do better checks inside the method.
116,Lakshmi Narayanan Sreethar,2023-05-18 06:08:53-05,8b0ab416437df5d6adf2ce58126dd2754c8ff48b,PG16: Use new function to check vacuum permission,postgres/postgres@b5d63824
117,Lakshmi Narayanan Sreethar,2023-05-17 13:37:00-05,d96e72af607f797b56fd3e0d8e51f6b229b9cebf,PG16: Rename RelFileNode references to RelFileNumber or RelFileLocator,postgres/postgres@b0a55e4
118,Lakshmi Narayanan Sreethar,2023-05-12 14:06:53-05,933285e64675febc5608add9cc1ad2d700518593,PG16: Remove MemoryContextContains usage,Remove the usage of MemoryContextContains as it has been removed in PG16.  postgres/postgres@9543eff
119,Konstantina Skovola,2023-06-01 04:30:34-05,1eb7e38d2df37da7674b2207155484838671d040,Enable ChunkAppend for space partitioned partial chunks,This is a follow-up patch for timescale#5599 which handles space partitioned hypertables.
120,Bharathy,2023-06-15 08:32:23-05,c48f905f780025bf3c92de0d0dd161108a4116a1,Index scan support for UPDATE/DELETE.,"During UPDATE/DELETE on compressed hypertables, we do a sequential scan which can be improved by supporting index scans.  In this patch for a given UPDATE/DELETE query, if there are any WHERE conditions specified using SEGMENT BY columns, we use index scan to fetch all matching rows. Fetched rows will be decompressed and moved to uncompressed chunk and a regular UPDATE/DELETE is performed on the uncompressed chunk."
121,Jan Nidzwetzki,2023-06-13 01:25:45-05,77318dced8a47aae4b2baddba71c829105e8319d,Fix broken download links,"The download links for several platforms are broken. This patch removes the links for the individual platforms and adds a link that points to the self-hosted install docs instead (as proposed by the docs team, see the discussion in #5762).  Fixes: #5762"
122,Jan Nidzwetzki,2023-06-15 05:44:53-05,f05b7f8105a6d7ef05abdfb0b21cb48824135c72,Fixed the naming of the Windows GitHub action,"The ignored workflow for windows-build-and-test does not set the name of the actions properly. Therefore, these actions use the default naming. For example, 'Regression Windows / build (15, windows-2022, Debug)'. However, our CI expects names like 'PG15 Debug windows-2022' in the required checks. This PR corrects the name of the jobs."
123,Valery Meleshkin,2023-06-14 10:55:51-05,4273a27461fab3d23d19bc81b18209e83bb34662,Ensure pg_config --cppflags are passed,"CMAKE_CPP_FLAGS is not a thing at all. Furthermore, CMAKE_CXX_FLAGS is not passed to a C compiler. pg_config uses CPPGLAGS for all includes, and needs to be passed into CMAKE_C_FLAGS as well."
124,Sotiris Stamokostas,2023-06-14 09:57:32-05,14d08576fb7d12b2d5edf4bfa3ea2722d79f7764,Allow flaky-test label,With this PR we allow issues with flaky-test label to be added to our bugs board.
125,Sven Klemm,2023-06-02 02:26:39-05,e302aa2ae97bbd682b1d5d1324e57a77131401d0,Fix handling of Result nodes below Sort nodes in ConstraintAwareAppend,With PG 15 Result nodes can appear between Sort nodes and DecompressChunk when postgres tries to adjust the targetlist.
126,Jan Nidzwetzki,2023-06-12 08:52:06-05,9c7ae3e8a983ff1a19645c3d2dc0508ae8c69550,Fixed two bugs in decompression sorted merge code,"SQLSmith found two bugs in the compression sorted merge code.  * The unused_batch_states are not initialized properly. Therefore,   non-existing unused batch states can be part of the BMS. This patch   fixes the initialization.  * For performance reasons, We reuse the same TupleDesc across all   TupleTableSlots. PostgreSQL sometimes uses TupleDesc data structures   with active reference counting. The way we use the TupleDesc   structures collides with the reference counting of PostgreSQL. This   patch introduces a private TupleDesc copy without reference counting."
127,Lakshmi Narayanan Sreethar,2023-05-17 09:07:42-05,4dce87a1c4a53d080676bfff042e24aa5b822cec,PG16: Refactor handling of PGDLLEXPORT macro definition,"PG16 defines the PGDLLEXPORT macro as a proper visibility attribute. In the previous versions from PG12 to PG16, the PGDLLEXPORT was always defined as an empty macro. Considering all this, the code has been now updated to skip defining PGDLLEXPORT if it has been already defined properly. If not, the macro is redefined without any additional checks.  postgres/postgres@089480c"
128,Lakshmi Narayanan Sreethar,2023-05-17 11:19:00-05,0f1fde8d31dd44180820409ea2ca334b9188a01e,Mark PG16 as a supported version,Note that this change in combination with -DEXPERIMENTAL=ON cmake flag will just allow us to compile timescaledb code with PG16 and this doesn't mean PG16 is supported by the extension.
129,Sotiris Stamokostas,2023-06-12 03:50:21-05,7df16ee560ec79fd0c4b1590189755bad25585f7,Renamed need-more-info label,We plan to rename the need-more-info label to waiting-for-author. This PR performs the needed adjustments in our GitHub actions.
130,Sotiris Stamokostas,2023-06-09 07:26:18-05,8b10a6795c4d696b6c59ab546d733decf2a68352,Compression test changes for PG14.0,We have changed the compression test by disabling parallel append in some test cases because the regression test was falling only in PG14.0 but not in PG14.8 or any other PostgreSQL version
131,Alexander Kuzmenkov,2023-06-07 08:55:53-05,f26e656c0f9ad7eb27c5de2232fef0f9154d80d5,Bulk decompression of compressed batches,"Add a function to decompress a compressed batch entirely in one go, and use it in some query plans. As a result of decompression, produce ArrowArrays. They will be the base for the subsequent vectorized computation of aggregates.  As a side effect, some heavy queries to compressed hypertables speed up by about 15"
134,Zoltan Haindrich,2023-06-06 10:11:40-05,769646bdb61d74f5cb026b598de24072cc6c4d8c,Fix issues with scripts/test_update_smoke.sh,"The test was failing on first run by leaving a database behind as a sideeffect. Between two steps the extension was dropped; without a proper cleanup. A non-existent sql function was called during cleanup.  This patch also removes the ""debug mode"" and every execution will leave the logs/etc in the /tmp directory for further inspection."
135,Jan Nidzwetzki,2023-06-07 02:05:58-05,b8e674c137f54e293e868ce747f19983a72e0e60,Fixed handling of NULL values in bookend_sfunc,"In the function bookend_sfunc values are compared. If the first processed value is a NULL value, it was copied into the state of the sfunc. A following comparison between the NULL value of the state and a non-NULL value could lead to a crash.  This patch improves the handling of NULL values in bookend_sfunc."
136,Jan Nidzwetzki,2023-06-06 04:59:39-05,f2eac72e2bb320a4866766f79b181fe78856c8a1,Ensure tlist is present in decompress chunk plan,"In PostgreSQL < 15, CustomScan nodes are projection capable. The planner invokes create_plan_recurse with the flag CP_IGNORE_TLIST. So, the target list of a CustomScan node can be NIL. However, we rely on the target list to derive information for sorting.  This patch ensures that the target list is always populated before the sort functions are called.  Fixes: #5738"
137,Zoltan Haindrich,2023-06-01 05:56:34-05,ac7090653eb020c2392bc29b653339b581d56bcf,Ensure PR number is referenced in the .unreleased files,Adds a simple check to ensure that the PR number is present at least once in the added changelog file. Also fixes an earlier PR which introduced a typo.
139,Sotiris Stamokostas,2023-05-03 14:20:44-05,1a93c2d482b50a43c105427ad99e6ecb58fcac7f,Improve parallel workers for decompression,"So far, we have set the number of desired workers for decompression to 1. If a query touches only one chunk, we end up with one worker in a parallel plan. Only if the query touches multiple chunks PostgreSQL spins up multiple workers. These workers could then be used to process the data of one chunk.  This patch removes our custom worker calculation and relies on PostgreSQL logic to calculate the desired parallelity.  Co-authored-by: Jan Kristof Nidzwetzki <jan@timescale.com>"
140,Jan Nidzwetzki,2023-04-06 06:12:13-05,10cab43e6c3348a1353a682a01bdf8a70173aca8,Enable compressed merge append for partial chunks,This patch enables the compressed merge optimization (see #5530) also for partially compressed chunks.
141,Dipesh Pandit,2023-06-01 09:04:47-05,c507f31069fccd3755a4138b907cbe997d34d15f,Internal Server Error when loading Explorer tab (#5723),"Internal Server Error when loading Explorer tab (SDC #995)          This is with reference to a weird scenarios where chunk table entry exist in     timescaledb catalog but it does not exist in PG catalog. The stale entry blocks     executing hypertable_size function on the hypertable.          The changes in this patch are related to improvements suggested for     hypertable_size function which involves:     1. Locking the hypertable in ACCESS SHARE mode in function hypertable_size to     avoid risk of chunks being dropped by another concurrent process.     2. Joining the hypertable and inherited chunk tables with ""pg_class"" to make     sure that a stale table without an entry is pg_catalog is not included as part     of hypertable size calculation.     3. An additional filter (schema_name) is required on pg_class to avoid     calculating size of multiple hypertables with same in different schema.          NOTE: With this change calling hypertable_size function will require select     privilege on the table.  Disable-check: force-changelog-file"
142,Konstantina Skovola,2023-04-21 03:51:44-05,ecc1b7b11a2aa9d483b14e240128632047af1b46,Enable ChunkAppend for partial chunks,"This patch enables ChunkAppend optimization for partially compressed chunks on hypertables without space partitioning, allowing for more efficient processing of LIMIT order by queries. A follow-up patch is required to handle space partitioned hypertables."
144,Bharathy,2023-05-26 23:30:37-05,b38c920266b33c5b83c85279c9b36f4131b9ea83,MERGE support on hypertables,"This patch does following:  1. Planner changes to create ChunkDispatch node when MERGE command    has INSERT action. 2. Changes to map partition attributes from a tuple returned from    child node of ChunkDispatch against physical targetlist, so that    ChunkDispatch node can read the correct value from partition column. 3. Fixed issues with MERGE on compressed hypertable. 4. Added more testcases. 5. MERGE in distributed hypertables is not supported. 6. Since there is no Custom Scan (HypertableModify) node for MERGE    with UPDATE/DELETE on compressed hypertables, we don't support this.  Fixes #5139"
147,Alexander Kuzmenkov,2023-05-24 14:57:54-05,29154b29d11901ee4b98312ca6245e87216a5877,Ignore the telemetry test for the time begin,It's the fifths run and it still fails in some jobs.
148,Alexander Kuzmenkov,2023-05-24 03:55:35-05,6589f43160980475c04fcc2a6e4e2b985a111357,Compression fuzzing in CI,"This serves as a way to exercise the decompression fuzzing code, which will be useful when we need to change the decompression functions. Also this way we'll have a check in CI that uses libfuzzer, and it will be easier to apply it to other areas of code in the future."
149,Fabrízio de Royes Mello,2023-05-22 14:35:45-05,54d4c3de6bbb52421b15f6035db662f683059c86,Introduce utility function ts_get_relation_relid,"In several places of our code base we use a combination of `get_namespace_oid` and `get_relname_relid` to return the Oid of a schema qualified relation, so refactored the code to encapsulate this behavior in a single function."
150,Eric Gillespie,2023-03-28 13:01:56-05,f2743648dff26f61f305aa8579bcac806e61a2a2,Update emacs configuration,"Use postgres for sql-mode, which makes it easier to connect to local postgresql with C-c C-z and also sets up postgresql extensions for font-lock.  Copy the 4-space tab-width setting to diff-mode so that, for example, diffs of lines containing 18 TABs don't display with 144-space indent."
151,Sven Klemm,2023-05-23 03:55:52-05,f14ff40d6305f139aaabc3e22cbc2fb5fca55bf2,Bump PG version used in CI,"Use PG 12.15, 13.11, 14.8 and 15.3 in CI."
152,Sven Klemm,2023-05-22 04:34:06-05,a31c9b9f8cdfe8643499b710dc983e5c5d6457e4,Increase number of sqlsmith loops in nightly CI,To improve coverage with sqlsmith we run it for longer in the scheduled nightly run.
153,Konstantina Skovola,2023-05-22 03:03:01-05,dd9f01ae9b5b7a4cf3f71ea2b78d69f511e58608,Fix occasional scheduler quit with `delete_job`,"Previously it was possible to send pg_cancel to the scheduler instead of the background worker for the job. That was because we attempted to take an advisory lock on the job id, which was assumed to be held by the background worker for the job. However, it is possible either for the job's worker or for the scheduler to be the one holding the lock. The scheduler takes this lock after the job's worker has exited, to update the job's scheduled status. This patch adds a check to determine if the background worker holding the lock is the scheduler. If it's the scheduler, wait for the lock to be released without canceling the worker.  Fixes #5711, #5224"
154,Rafia Sabih,2023-04-27 08:01:38-05,d9849325d0d0f81a13db1e41aa56f8b567945e72,Improve test suite,Add more regression tests for Continuous aggregates with joins.
155,Maheedhar PV,2023-05-18 07:30:07-05,38ee7f49b0cddc5f7fe697321135b28680471ff7,Every PR to have its own changelog,"The changes in this commit 1. workflow action to check if the PR has its own changelog file in "".unreleased/"" folder. 2. script to merge the individual changelog entries that can be copied into the CHANGELOG.md file. 3. script to check the format of the lines in the change log file. 4. script to delete the individual changelogs, post release."
156,Mats Kindahl,2023-05-17 06:07:02-05,2bd18b8e44e6bc6d2ef57aebc2a3e22ba91059b3,Use environment variables for workflows,Remove GitHub variables from locations where they are expanded in place.  See https://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions#using-an-intermediate-environment-variable
158,Konstantina Skovola,2023-05-12 09:47:06-05,19dd7bbd7a09de25af7c233a5923ac7eaef809de,Fix DISTINCT query with JOIN on multiple segmentby columns,"Previously when adding equivalence class members for the compressed chunk's variables, we would only consider Vars. This led us to ignore cases where the Var was wrapped in a RelabelType, returning inaccurate results.  Fixed the issue by accepting Vars with RelabelType for segmentby equivalence class.  Fixes #5585"
159,Alexander Kuzmenkov,2023-05-16 11:33:48-05,fb65086b5542a871dc3d9757724e886dca904ef6,Add a ubsan suppression for overflow in histogram(),"It is in postgres code, and doesn't lead to bugs."
160,Alexander Kuzmenkov,2023-05-16 07:43:18-05,8ff0648fd0768b9229853f933056855420ad82ee,Fix ubsan failure in gorilla decompression,Also add more tests
161,Alexander Kuzmenkov,2023-05-15 10:52:37-05,936d751037381cbbb3a59ac9da36cdaa0dd4904a,Add AddressSanitizer instrumentation for memory contexts,"Use manual poison/unpoison at the existing Valgrind hooks, so that AddressSanitizer sees palloc/pfree as well, not only the underlying mallocs which are called much less often.  Fix some out-of-bound reads found with this instrumentation."
163,Alexander Kuzmenkov,2023-05-15 10:28:35-05,030bfe867df02dffa4f5c0fc8a0909075def6f4a,Fix errors in decompression found by fuzzing,"For deltadelta and gorilla codecs, add various length and consistency checks that prevent segfaults on incorrect data."
164,Alexander Kuzmenkov,2023-05-15 10:23:27-05,a7321199a4184aaa39c9c5b56b412497d62da1dc,Enable branch-level code coverage,Helps to check the test coverage for various complex conditions in the decompression code.
167,Mats Kindahl,2023-04-28 03:38:43-05,3947c011244050878ebb091b560e19ea4c9d8378,Support sending telemetry event reports,Add table `_timescaledb_catalog.telemetry_event` table containing events that should be sent out with telemetry reports. The table will be truncated after reporting being generated.
168,Bharathy,2023-05-11 07:25:27-05,2d71a5bca9a21490819d239aeee8b2758a54ec1d,Fix leak during concurrent UPDATE/DELETE,"When updating and deleting the same tuple while both transactions are running at the same time, we end up with reference leak. This is because one of the query in a transaction fails and we take error path. However we fail to close the table.  This patch fixes the above mentioned problem by closing the required tables.  Fixes #5674"
169,Mats Kindahl,2023-05-09 11:58:12-05,656daf45f6596e5d663082a8673e705359c01534,Fix subtransaction resource owner,"When executing a subtransaction using `BeginInternalSubTransaction` the memory context switches from the current context to `CurTransactionContext` and when the transaction is aborted or committed using `ReleaseCurrentSubTransaction` or `RollbackAndReleaseCurrentSubTransaction` respectively, it will not restore to the previous memory context or resource owner but rather use `TopTransactionContext`. Because of this, both the memory context and the resource owner will be wrong when executing `calculate_next_start_on_failure`, which causes `run_job` to generate an error when used with the telemetry job.  This commit fixes this by saving both the resource owner and the memory context before starting the internal subtransaction and restoring it after finishing the internal subtransaction.  Since the `ts_bgw_job_run_and_set_next_start` was incorrectly reading the wrong result from the telemetry job, this commit fixes this as well. Note that `ts_bgw_job_run_and_set_next_start` is only used when running the telemetry job, so it does not cause issues for other jobs."
170,Erik Nordström,2023-05-03 07:06:25-05,abb6762450fb90aae5536641fef85cfd3c75b510,Reduce memory usage for distributed analyze,Use a per-tuple memory context when receiving chunk statistics from data nodes. Otherwise memory usage is proportional to the number of chunks and columns.
171,Erik Nordström,2023-05-03 07:05:47-05,96d2acea30e6cc27ab624e5a155f6b57767ac411,Cleanup PGresults on transaction end,Fix a regression due to a previous change in c571d54c. That change unintentionally removed the cleanup of PGresults at the end of transactions. Add back this functionality in order to reduce memory usage.
172,Fabrízio de Royes Mello,2023-05-05 10:14:04-05,f250eaa631920c209673fd0babfe29af9b9778dd,Remove FK from continuous_agg_migrate_plan,During the `cagg_migrate` execution if the user set the `drop_old` parameter to `true` the routine will drop the old Continuous Aggregate leading to an inconsistent state because the catalog code don't handle this table as a normal catalog table so the records are not removed when dropping a Continuous Aggregate. The same problem will happen if you manually drop the old Continuous Aggregate after the migration.  Fixed it by removing the useless Foreign Key and also adding another column named `user_view_definition` to the main plan table just to store the original user view definition for troubleshooting purposes.  Fixed #5662
173,Ante Kresic,2023-05-09 07:45:15-05,ab224789922f0e3b88bf57f5b43f0a5c258243cc,Fix DML decompression issues with bitmap heap scan,"Bitmap heap scans are specific in that they store scan state during node initialization. This means they would not pick up on any data that might have been decompressed during a DML command from the compressed chunk. To avoid this, we update the snapshot on the node scan state and issue a rescan to update the internal state."
174,shhnwz,2023-05-10 00:47:38-05,bd36afe2f3a5bc437d58a88a85b19b2253c0a926,Fixed Coverity Scan Warnings,Unused Value reported during coverity scan link: https://scan4.scan.coverity.com/reports.htm#v56957/p12995
175,Ante Kresic,2023-04-27 04:59:01-05,8e69a9989f9e9893807e7ea2403ab1c267895396,Ignore multinode tests from PR CI runs,"dist_move_chunk, dist_param, dist_insert and remote_txn create a lot of friction due to their flakiness. Ignoring them until we can fix them."
177,Ante Kresic,2023-05-04 04:42:13-05,6782beb1504bc421f973269f029c9854e17c7650,Fix index scan handling in DML decompression,We need to use the correct qualifiers for index scans since the generic scan qualifiers are not populated in this case.
178,Dmitry Simonenko,2023-05-04 06:54:27-05,8ca17e704c8f9320f360bab7eaf3622391d686bd,Fix ALTER TABLE SET with normal tables,Running ALTER TABLE SET with multiple SET clauses on a regular PostgreSQL table produces irrelevant error when timescaledb extension is installed.  Fix #5641
179,Sven Klemm,2023-05-03 02:42:38-05,9259311275ded2e562e5222bc0853509dc2dd206,Fix JOIN handling in UPDATE/DELETE on compressed chunks,When JOINs were present during UPDATE/DELETE on compressed chunks the code would decompress other hypertables that were not the target of the UPDATE/DELETE operations and in the case of self-JOINs potentially decompress chunks not required to be decompressed.
180,Bharathy,2023-05-03 12:07:08-05,769f9fe609df950a82d6697a08f33675da5c8f28,Fix segfault when deleting from compressed chunk,"During UPDATE/DELETE on compressed hypertables, we iterate over plan tree to collect all scan nodes. For each scan nodes there can be filter conditions.  Prior to this patch we collect only first filter condition and use for first chunk which may be wrong. In this patch as and when we encounter a target scan node, we immediatly process those chunks.  Fixes #5640"
181,Fabrízio de Royes Mello,2023-04-28 07:16:33-05,90f585ed7fbe51568749c96dc650f6666a4a20af,Fix CoverityScan deference after null check,Don't need to check NULL for `direct_query->jointree` because we don't allow queries without FROM clause in Continuous Aggregate definition.  CoverityScan link: https://scan4.scan.coverity.com/reports.htm#v54116/p12995/fileInstanceId=131745632&defectInstanceId=14569562&mergedDefectId=384045
182,Konstantina Skovola,2023-04-21 07:29:13-05,6e65172cd821c5509da3b7f41904a5b2f1916d16,Fix tablespace for compressed hypertable and corresponding toast,"If a hypertable uses a non default tablespace, the compressed hypertable and its corresponding toast table and index is still created in the default tablespace. This PR fixes this unexpected behavior and creates the compressed hypertable and its toast table and index in the same tablespace as the hypertable.  Fixes #5520"
183,Jan Nidzwetzki,2023-04-06 06:12:13-05,df32ad4b7920970a75a2b83dd3587fdda07b8302,Optimize compressed chunk resorting,"This patch adds an optimization to the DecompressChunk node. If the query 'order by' and the compression 'order by' are compatible (query 'order by' is equal or a prefix of compression 'order by'), the compressed batches of the segments are decompressed in parallel and merged using a binary heep. This preserves the ordering and the sorting of the result can be prevented. Especially LIMIT queries benefit from this optimization because only the first tuples of some batches have to be decompressed. Previously, all segments were completely decompressed and sorted.  Fixes: #4223  Co-authored-by: Sotiris Stamokostas <sotiris@timescale.com>"
184,Fabrízio de Royes Mello,2023-04-26 18:18:19-05,cc9c3b343113fc9a43f095720c1d2f2596a23b03,Post-release 2.10.3,Adjust the upgrade/downgrade scripts and add the tests.
185,Nikhil Sontakke,2023-04-25 09:57:31-05,ed8ca318c056ee8b01197a5740a25a358a034894,Quote username identifier appropriately,Need to use quote_ident() on the user roles. Otherwise the extension scripts will fail.  Co-authored-by: Mats Kindahl <mats@timescale.com>
187,Zoltan Haindrich,2023-04-14 09:24:42-05,1d092560f45bbec887a8cb098f338a789250196d,Fix on-insert decompression after schema changes,On compressed hypertables 3 schema levels are in use simultaneously  * main - hypertable level  * chunk - inheritance level  * compressed chunk  In the build_scankeys method all of them appear - as slot have their fields represented as a for a row of the main hypertable.  Accessing the slot by the attribut numbers of the chunks may lead to indexing mismatches if there are differences between the schemes.  Fixes: #5577
188,Mats Kindahl,2023-04-25 07:28:10-05,be2879438490d5f3c4aeafd446b6c60afcefd6a5,Enable run_job() for telemetry job,"Since telemetry job has a special code path to be able to be used both from Apache code and from TSL code, trying to execute the telemetry job with run_job() will fail.  This code will allow run_job() to be used with the telemetry job to trigger a send of telemetry data. You have to belong to the group that owns the telemetry job (or be the owner of the telemetry job) to be able to use it.  Closes #5605"
192,Fabrízio de Royes Mello,2023-04-26 12:37:25-05,002b6e879abaeecbde295775d7c46292bcb535bf,Fix CI ignored regression workflows,"We defined some paths to ignore regression test workflows when, for example, we change the CHANGELOG.md and others. But in fact it was not happening because we didn't define a proper name in the fake regression workflow that fake to the CI that some required status passed.  Fixed it defining a proper regression name like we do for the regular regression workflow."
193,Fabrízio de Royes Mello,2023-04-26 07:02:57-05,3c8d7cef77d34d88ace3cd0a58fbb7640965d5c0,Fix cagg_repair for the old CAgg format,In commit 4a6650d1 we fixed the cagg_repair running for broken Continuous Aggregates with JOINs but we accidentally removed the code path for running against the old format (finalized=false) leading us to a dead code pointed out by CoverityScan: https://scan4.scan.coverity.com/reports.htm#v54116/p12995/fileInstanceId=131706317&defectInstanceId=14569420&mergedDefectId=384044  Fixed it by restoring the old code path for running the cagg_repair for Continuous Aggregates in the old format (finalized=false).
194,Mats Kindahl,2023-04-25 11:01:48-05,d3730a4f6ac89612d749d10843def181f7830635,Add permission checks to run_job(),"There were no permission checks when calling run_job(), so it was possible to execute any job regardless of who owned it. This commit adds such checks."
195,Fabrízio de Royes Mello,2023-04-21 14:34:49-05,4a6650d170cc07dbec0afe01bb3685a2cf9d9a37,Fix broken CAgg with JOIN repair function,The internal `cagg_rebuild_view_definition` function was trying to cast a pointer to `RangeTblRef` but it actually is a `RangeTblEntry`.  Fixed it by using the already existing `direct_query` data struct to check if there are JOINs in the CAgg to be repaired.
196,Ante Kresic,2023-04-23 23:29:11-05,910663d0be132aa05100233ebf7e97c66da7297f,Reduce decompression during UPDATE/DELETE,"When updating or deleting tuples from a compressed chunk, we first need to decompress the matching tuples then proceed with the operation. This optimization reduces the amount of data decompressed by using compressed metadata to decompress only the affected segments."
199,Jan Nidzwetzki,2023-04-24 05:26:28-05,2f194e6109d186a4c61352afaa40381db617e437,Make compression metadata column names reusable,Move the creation of metadata column names for min/max values to separate functions to make the code reusable.
200,Jan Nidzwetzki,2023-04-21 08:00:10-05,c54d8bd9464cd2cf410f37968edeb1c438950547,Add missing order by to compression_ddl tests,"Some queries in compression_ddl had no order by. Therefore the output order was not defined, which led to flaky tests."
202,Sven Klemm,2023-04-18 04:59:11-05,744b44cc52f1c2a3ae329657cdfdf29868df1015,Fix parameterization in DecompressChunk path generation,All children of an append path are required to have the same parameterization so we have to reparameterize when the selected path does not have the right parameterization.
203,Ante Kresic,2023-04-20 05:54:28-05,23b3f8d7a6c6f9e5549fcf18cbf70937f02bcc57,Block unique idx creation on compressed hypertable,"This block was removed by accident, in order to support this we need to ensure the uniqueness in the compressed data which is something we should do in the future thus removing this block."
204,Alexander Kuzmenkov,2023-04-20 00:40:34-05,12f3131f9e784a03136d046f9b2fd840145ee6f0,Post-release 2.10.2,Adjust the upgrade/downgrade scripts and add the tests.
205,Zoltan Haindrich,2023-04-14 09:24:42-05,a0df8c8e6df75b952fef9f371d689f2f064397e5,Fix on-insert decompression for unique constraints,Inserting multiple rows into a compressed chunk could have bypassed constraint check in case the table had segment_by columns.  Decompression is narrowed to only consider candidates by the actual segment_by value. Because of caching - decompression was skipped for follow-up rows of the same Chunk.  Fixes #5553
206,Ante Kresic,2023-04-18 07:52:33-05,a49fdbcffba03e4af99d6f578e9f457c24aa8fad,Reduce decompression during constraint checking,"When inserting into a compressed chunk with constraints present, we need to decompress relevant tuples in order to do speculative inserting. Usually we used segment by column values to limit the amount of compressed segments to decompress. This change expands on that by also using segment metadata to further filter compressed rows that need to be decompressed."
303,Fabrízio de Royes Mello,2023-02-24 06:58:28-06,152ef02d74fbe376e307399564bcb48b23a18150,Fix uninitialized bucket_info variable,The `bucket_info` variable is initialized by `caggtimebucketinfo_init` function called inside the following branch:  `if (rte->relkind == RELKIND_RELATION || rte->relkind == RELKIND_VIEW)`  If for some reason we don't enter in this branch then the `bucket_info` will not be initialized leading to an uninitialized variable when returning `bucket_info` at the end of the `cagg_validate_query` function.  Fixed it by initializing with zeros the `bucket_info` variable when declaring it.  Found by coverity scan.
492,Alexander Kuzmenkov,2022-11-09 10:03:49-06,feb09c54e9da991baec6ecb11d9bd661b20a121b,Rebuild cached PG daily and on config changes,Otherwise it's easy to break these builds and not notice it until much later.
207,Alexander Kuzmenkov,2023-04-14 04:52:23-05,28d9db1af95f6a1ee9302edab79cb730d32099d2,Changelog for 2.10.2,## 2.10.2 (2023-04-20)  **Bugfixes** * #5410 Fix file trailer handling in the COPY fetcher * #5446 Add checks for malloc failure in libpq calls * #5233 Out of on_proc_exit slots on guc license change * #5428 Use consistent snapshots when scanning metadata * #5499 Do not segfault on large histogram() parameters * #5470 Ensure superuser perms during copy/move chunk * #5500 Fix when no FROM clause in continuous aggregate definition * #5433 Fix join rte in CAggs with joins * #5556 Fix duplicated entries on timescaledb_experimental.policies view * #5462 Fix segfault after column drop on compressed table * #5543 Copy scheduled_jobs list before sorting it * #5497 Allow named time_bucket arguments in Cagg definition * #5544 Fix refresh from beginning of Continuous Aggregate with variable time bucket * #5558 Use regrole for job owner * #5542 Enable indexscan on uncompressed part of partially compressed chunks  **Thanks** * @nikolaps for reporting an issue with the COPY fetcher * @S-imo-n for reporting the issue on Background Worker Scheduler crash * @geezhu for reporting issue on segfault in historgram() * @mwahlhuetter for reporting the issue with joins in CAggs * @mwahlhuetter for reporting issue with duplicated entries on timescaledb_experimental.policies view * @H25E for reporting error refreshing from beginning of a Continuous Aggregate with variable time bucket
208,Konstantina Skovola,2023-04-11 04:33:11-05,5633960f8b0e4675e342b1721bde954fdbe938e7,Enable indexscan on uncompressed part of partially compressed chunks,"This was previously disabled as no data resided on the uncompressed chunk once it was compressed, but this is not the case anymore with partially compressed chunks, so we enable indexscan for the uncompressed chunk again.  Fixes #5432  Co-authored-by: Ante Kresic <ante.kresic@gmail.com>"
209,shhnwz,2023-04-12 00:29:26-05,ca472ebb0d3e290d029bd9beeda1647547302920,Fixed transparent decompress chunk,Transparent decompress chunk was added into to ignore list due to the side effect of #5118. This issue is to fix the flaky nature of the test.
210,Mats Kindahl,2023-04-13 06:28:25-05,9a64385f34db737cc174be5ee8ed560bd82b1663,Use regrole for job owner,"Instead of using a user name to register the owner of a job, we use regrole. This allows renames to work properly since the underlying OID does not change when the owner name changes.  We add a check when calling `DROP ROLE` that there is no job with that owner and generate an error if there is."
211,Alexander Kuzmenkov,2023-04-17 10:23:02-05,20db884bd72b1798844176d34bb106b211f4e30e,Increase remote tuple and startup costs,"Our cost model should be self-consistent, and the relative values for the remote tuple and startup costs should reflect their real cost, relative to costs of other operations like CPU tuple cost.  For example, now remote costs are set even lower than the parallel tuple and startup cost. Contrary to that, their real world cost is going to be an order of magnitude higher or more, because parallel tuples are sent through shared memory, and remote tuples are sent over the network.  Increasing these costs leads to query plan improvements, e.g. we start to favor the GROUP BY pushdown in some cases."
212,Fabrízio de Royes Mello,2023-04-17 09:57:40-05,b16bf3b100c5a9ce693ef5fe49208421445e78c5,Fix post repair tests,Commit 3f9cb3c2 introduced new repair tests for broken Continuous Aggregates with JOIN clause but in the post.repair.sql we not properly calling the post.repair.cagg_joins.sql because a wrong usage of psql `if` statement.
213,Alexander Kuzmenkov,2023-04-14 04:18:15-05,d32224a914877f8f9342cca63859ca1d76d9bddb,Add labels to backported PRs,Makes it easier to check what was done when looking at the release milestone.
214,Sven Klemm,2023-04-14 05:22:54-05,90e54def8a55e833f7e6bd5408d585ce244c8bd0,Improve interpolate error message on datatype mismatch,Include information about the expected and the returned datatype in the error details of interpolate.
215,Fabrízio de Royes Mello,2023-04-04 13:39:38-05,a3d778f7a0313fca4ad86f64f9f32158011c8a0e,Add CI check for missing gitignore entries,Whenever we create a template sql file (*.sql.in) we should add the respective .gitignore entry for the generated test files.  So added a CI check to check for missing gitignore entries for generated test files.
216,Lakshmi Narayanan Sreethar,2023-04-11 06:14:13-05,a383c8dd4ff6cc932dfafe00b7a1a44d9efb6740,Copy scheduled_jobs list before sorting it,"The start_scheduled_jobs function mistakenly sorts the scheduled_jobs list in-place. As a result, when the ts_update_scheduled_jobs_list function compares the updated list of scheduled jobs with the existing scheduled jobs list, it is comparing a list that is sorted by job_id to one that is sorted by next_start time. Fix that by properly copying the scheduled_jobs list into a new list and use that for sorting.  Fixes #5537"
217,Lakshmi Narayanan Sreethar,2023-04-13 09:50:34-05,b10139ba4899d88053c98130881b8fe84cc7ccf6,Update bgw_custom testcase,Added a few cleanup steps and updated a test logic to make the testcase runs more stable.
218,Maheedhar PV,2023-04-13 11:22:20-05,b136bb554baf08bfc7f648a9d5f8ec59402230a3,Run dist_move_chunk as a solo test in PG15.2,Running the test dist_move_chunk in parallel can cause a timeouts(deadlock) in other tests running in parallel on PG15.2. Force the test to run solo.
219,Maheedhar PV,2023-04-12 09:56:48-05,ca240c33ca2d6f445984f4f43c38d571c1bfac83,Build failure on windows,"The cmake call to ""find_package(OpenSSL)"" on windows may set the variable OPENSSL_LIBRARIES to a non-standard list which might include non path items such as ""debug"", ""optimized"". These non-standard elements in the list would cause the link failure with error ""LNK1104"".  Fix: Check and retain only valid paths in OPENSSL_LIBRARIES list.  closes#407"
220,Ante Kresic,2023-03-30 05:46:44-05,464d20fb4103ea99ac048c116f0b386594dff143,Propagate vacuum/analyze to compressed chunks,"With recent changes, we enabled analyze on uncompressed chunk tables for compressed chunks. This change includes analyzing the compressed chunks table when analyzing the hypertable and its chunks, enabling us to remove the generating stats when compressing chunks."
221,Rafia Sabih,2023-03-14 10:37:02-05,3f9cb3c27ac828901842c70e17451beec754bbb8,Pass join related structs to the cagg rte,"In case of joins in the continuous aggregates, pass the required structs to the new rte created. These values are required by the planner to finally query the materialized view.  Fixes #5433"
222,Fabrízio de Royes Mello,2023-04-12 10:17:41-05,09565acae497274dd48ef8231eda2845aabab1e7,Fix timescaledb_experimental.policies duplicates,Commit 16fdb6ca5e introduced `timescaledb_experimental.policies` view to expose the Continuous Aggregate policies but the current JOINS over our catalog are not accurate.  Fixed it by properly JOIN the underlying catalog tables to expose the correct information without duplicates about the Continuous Aggregate policies.  Fixes #5492
223,Fabrízio de Royes Mello,2023-04-11 14:58:13-05,f6c8468ee630bdad03cdb433ddb6be8daa1709c0,Fix timestamp out of range refreshing CAgg,"When refreshing from the beginning (window_start=NULL) of a Continuous Aggregate with variable time bucket we were getting a `timestamp out of range` error.  Fixed it by setting `-Infinity` when passing `window_start=NULL` when refreshing a Continuous Aggregate with variable time bucket.  Fixes #5474, #5534"
224,Mats Kindahl,2023-04-12 05:52:41-05,3cc8a4ca34ec76f4de66349af3194851acb7c228,Fix error message for continuous aggregates,"Several error messages for continuous aggregates are not following the error message style guidelines at https://www.postgresql.org/docs/current/error-style-guide.html  In particular, they do not write the hints and detailed messages as full sentences."
225,Sven Klemm,2023-04-12 03:56:35-05,f0623a8c3891f5a2c28c5da8d3abfa545e5f59d1,Skip Ordered Append when only 1 child node is present,"This is mostly a cosmetic change. When only 1 child is present there is no need for ordered append. In this situation we might still benefit from a ChunkAppend node here due to runtime chunk exclusion when we have non-immutable constraints, so we still add the ChunkAppend node in that situation even with only 1 child."
227,Sven Klemm,2023-04-06 07:50:27-05,2d7eb18f249e898e63fc90bf02012af9ce2ccded,Drop unused SQL functions,"This patch drops the following internal SQL functions which were unused:   _timescaledb_internal.is_main_table(regclass);   _timescaledb_internal.is_main_table(text, text);   _timescaledb_internal.hypertable_from_main_table(regclass);   _timescaledb_internal.main_table_from_hypertable(integer);   _timescaledb_internal.time_literal_sql(bigint, regtype);"
228,Ante Kresic,2023-04-06 07:00:49-05,84b6783a199c37bff6b36a3c0940915e5ca569ce,Fix chunk status when inserting into chunks,"While executing compression operations in parallel with inserting into chunks (both operations which can potentially change the chunk status), we could get into situations where the chunk status would end up inconsistent. This change re-reads the chunk status after locking the chunk to make sure it can decompress data when handling ON CONFLICT inserts correctly."
229,Ante Kresic,2023-03-31 04:36:17-05,54074f1fd4ab7acb3e4d2e2becd82b844b570e67,Add more compression DML isolation tests,Verify that insertion into compressed chunks does not block each other if the chunks is already partially compressed. Also check that using the RETURNING clause works the same.
230,Ante Kresic,2023-03-14 06:44:44-05,dc5bf3b32ee39e121ca9a8888124460b8f1eee1e,Test compression DML with physical layout changes,These tests try to verify that changing physical layout of chunks (either compressed or uncompressed) should yield consistent results. They also verify index mapping on compressed chunks is handled correctly.
231,Fabrízio de Royes Mello,2023-04-10 12:33:48-05,9a466ca185ba3df611cbdb460b7b4aa906fcdf2f,Silence WARNING after extension update,Commit 8afdddc2da added the first step for deprecating the old format of Continuous Aggregate but just for PostgreSQL 15 and later versions.  During the extension update we emit a message about the deprecation but this has being emited even if the user is using PostgreSQL versions before 15.  Fixed it by emiting the WARNING just when PostgreSQL version is greater or equal to 15.
232,Sven Klemm,2023-04-06 06:00:00-05,04f43335dea11e9c467ee558ad8edfc00c1a45ed,Move aggregate support function into _timescaledb_functions,"This patch moves the support functions for histogram, first and last into the _timescaledb_functions schema. Since we alter the schema of the existing functions in upgrade scripts and do not change the aggregates this should work completely transparently for any user objects using those aggregates."
233,Konstantina Skovola,2023-04-06 01:00:55-05,3814a3f351a0691681e3123631a99e3e5b45aae6,Properly format license error hint,Commit 57fde383b3dddd0b52263218e65a0135981c2d34 changed the messaging but did not format the error hint correctly. This patch fixes the error hint.  Fixes #5490
234,Alexander Kuzmenkov,2023-04-06 09:10:15-05,8c77be6c68b3f53143eb0faa8b49bbf15674e284,Look up compressed column metadata only at planning time,"Now we look them up again at execution time, which adds up for tables with a large number of chunks.  This gives about 15"
236,Konstantina Skovola,2023-04-04 09:25:19-05,df70f3e050e817460e8a089a3a8fc90828d5d6c5,Remove unused variable in tsl_get_compressed_chunk_index_for_recompression,Commit 72c0f5b25e569015aacb98cc1be3169a1720116d introduced an unused variable. This patch removes it.
237,Zoltan Haindrich,2023-03-17 11:32:33-05,975e9ca1662ac4dd050b6fb296e51d507a64a773,Fix segfault after column drop on compressed table,"Decompression produces records which have all the decompressed data set, but it also retains the fields which are used internally during decompression. These didn't cause any problem - unless an operation is being done with the whole row - in which case all the fields which have ended up being non-null can be a potential segfault source.  Fixes #5458 #5411"
238,Sven Klemm,2023-03-31 01:22:57-05,feef9206facc5c5f506661de4a81d96ef059b095,Add _timescaledb_functions schema,Currently internal user objects like chunks and our functions live in the same schema making locking down that schema hard. This patch adds a new schema _timescaledb_functions that is meant to be the schema used for timescaledb internal functions to allow separation of code and chunks or other user objects.
240,Bharathy,2023-04-05 06:17:52-05,1fb058b1992bd89b6d69e910b42d5cae5ee46212,Support UPDATE/DELETE on compressed hypertables.,"This patch does following:  1. Executor changes to parse qual ExprState to check if SEGMENTBY    column is specified in WHERE clause. 2. Based on step 1, we build scan keys. 3. Executor changes to do heapscan on compressed chunk based on    scan keys and move only those rows which match the WHERE clause    to staging area aka uncompressed chunk. 4. Mark affected chunk as partially compressed. 5. Perform regular UPDATE/DELETE operations on staging area. 6. Since there is no Custom Scan (HypertableModify) node for    UPDATE/DELETE operations on PG versions < 14, we don't support this    feature on PG12 and PG13."
241,Sven Klemm,2023-04-03 06:09:06-05,c2941a3f9a95417c77b15db900b61bffc89cf36c,Fix windows package test,Use the windows packages from the github release for package testing.
242,Erik Nordström,2023-03-08 09:58:05-06,2e6c6b5c58c5e139f8a34a0de30dd2590908f654,Refactor and optimize distributed COPY,"Refactor the code path that handles remote distributed COPY. The main changes include:  * Use a hash table to lookup data node connections instead of a list. * Refactor the per-data node buffer code that accumulates rows into   bigger CopyData messages. * Reduce the default number of rows in a CopyData message to 100. This   seems to improve throughput, probably striking a better balance   between message overhead and latency. * The number of rows to send in each CopyData message can now be   changed via a new foreign data wrapper option."
243,Ildar Musin,2023-04-03 09:08:15-05,c6b9f5097886d0d33bb88f2aee1d427b0c69b969,Fix OSM chunks exclusion from append paths,OSM chunks have their own fdw_private which conflicts with checks in the MergeAppend code path causing segfaults. This commit fixes this by returning early when there is an OSM chunk in the MergeAppendPath.
244,Nikhil Sontakke,2023-03-27 03:10:52-05,517dee9f6bf9c1a94b7dd05f1ea0057e4f27f5b6,Add test for superuser chunk copy/move,Add isolation test case to check that the chunk object created during chunk copy/move operation on the destination datanode always has superuser credentials till the end of the operation.
245,Rafia Sabih,2023-03-28 05:17:06-05,ff5959f8f9c1d40dfe8b36a18cbd28d78ac77dca,Handle when FROM clause is missing in continuous aggregate definition,It now errors out for such a case.  Fixes #5500
246,Konstantina Skovola,2023-03-28 08:27:00-05,cb81c331ae4a9fa233106439ef5f73c02b702065,Allow named time_bucket arguments in Cagg definition,Fixes #5450
247,Rafia Sabih,2023-02-08 04:54:28-06,98218c1d079231a9aa469b37ddd0ed39e77c2adb,Enable joins for heirarchical continuous aggregates,"The joins could be between a continuous aggregate and hypertable, continuous aggregate and a regular Postgres table, and continuous aggregate and a regular Postgres view."
248,Mats Kindahl,2023-03-28 02:27:52-05,777c599a345242e69e0e9d505897992c73bf98fc,Do not segfault on large histogram() parameters,There is a bug in `width_bucket()` causing an overflow and subsequent NaN value as a result of dividing with `+inf`. The NaN value is interpreted as an integer and hence generates an index out of range for the buckets.  This commit fixes this by generating an error rather than segfaulting for bucket indexes that are out of range.
249,Konstantina Skovola,2023-03-27 03:19:40-05,22841abdf0d2c7b853386d8594d2a317df5f1e07,Update community license related errors,Update the error message printed when attempting to use a community license feature with apache license installed.  Fixes #5438
250,Erik Nordström,2023-03-16 10:45:28-05,a51d21efbe3ec0a16c986d2e349c7a29e06b2b0c,Fix issue creating dimensional constraints,"During chunk creation, the chunk's dimensional CHECK constraints are created via an ""upcall"" to PL/pgSQL code. However, creating dimensional constraints in PL/pgSQL code sometimes fails, especially during high-concurrency inserts, because PL/pgSQL code scans metadata using a snapshot that might not see the same metadata as the C code. As a result, chunk creation sometimes fail during constraint creation.  To fix this issue, implement dimensional CHECK-constraint creation in C code. Other constraints (FK, PK, etc.) are still created via an upcall, but should probably also be rewritten in C. However, since these constraints don't depend on recently updated metadata, this is left to a future change.  Fixes #5456"
251,Konstantina Skovola,2023-02-13 05:02:51-06,72c0f5b25e569015aacb98cc1be3169a1720116d,Rewrite recompress_chunk in C for segmentwise processing,"This patch introduces a C-function to perform the recompression at a finer granularity instead of decompressing and subsequently compressing the entire chunk.  This improves performance for the following reasons: - it needs to sort less data at a time and - it avoids recreating the decompressed chunk and the heap inserts associated with that by decompressing each segment into a tuplesort instead.  If no segmentby is specified when enabling compression or if an index does not exist on the compressed chunk then the operation is performed as before, decompressing and subsequently compressing the entire chunk."
252,Nikhil Sontakke,2023-03-21 06:00:21-05,7e43f45ccb8b30800db5f19af6ab024d37efea2b,Ensure superuser perms during copy/move chunk,"There is a security loophole in current core Postgres, due to which it's possible for a non-superuser to gain superuser access by attaching dependencies like expression indexes, triggers, etc. before logical replication commences.  To avoid this, we now ensure that the chunk objects that get created for the subscription are done so as a superuser. This avoids malicious dependencies by regular users."
253,Fabrízio de Royes Mello,2023-03-17 15:47:34-05,38fcd1b76b14ddd73f64853a1bf6efca5e17fb4c,Improve Realtime Continuous Aggregate performance,"When calling the `cagg_watermark` function to get the watermark of a Continuous Aggregate we execute a `SELECT MAX(time_dimension)` query in the underlying materialization hypertable.  The problem is that a `SELECT MAX(time_dimention)` query can be expensive because it will scan all hypertable chunks increasing the planning time for a Realtime Continuous Aggregates.  Improved it by creating a new catalog table to serve as a cache table to store the current Continous Aggregate watermark in the following situations: - Create CAgg: store the minimum value of hypertable time dimension   data type; - Refresh CAgg: store the last value of the time dimension materialized   in the underlying materialization hypertable (or the minimum value of   materialization hypertable time dimension data type if there's no   data materialized); - Drop CAgg Chunks: the same as refresh cagg.  Closes #4699, #5307"
254,shhnwz,2022-12-22 23:37:51-06,699fcf48aa71189e581c27777715f89824b56268,Stats improvement for Uncompressed Chunks,During the compression autovacuum use to be disabled for uncompressed chunk and enable after decompression. This leads to postgres maintainence issue. Let's not disable autovacuum for uncompressed chunk anymore. Let postgres take care of the stats in its natural way.  Fixes #309
255,Alexander Kuzmenkov,2023-03-22 03:43:22-05,5c07a57a0246cf81eb55d5f563825cd20512c7cf,Simplify control flow in decompress_chunk_exec,"No functional changes, mostly just reshuffles the code to prepare for batch decompression.  Also removes unneeded repeated column value stores and ExecStoreTuple, to save 3-5"
257,Erik Nordström,2023-03-10 06:15:02-06,63b416b6b0065e6e178913a916a2f366f4f0d1a4,Use consistent snapshots when scanning metadata,"Invalidate the catalog snapshot in the scanner to ensure that any lookups into `pg_catalog` uses a snapshot that is consistent with the snapshot used to scan TimescaleDB metadata.  This fixes an issue where a chunk could be looked up without having a proper relid filled in, causing an assertion failure (`ASSERT_IS_VALID_CHUNK`). When a chunk is scanned and found (in `chunk_tuple_found()`), the Oid of the chunk table is filled in using `get_relname_relid()`, which could return InvalidOid due to use of a different snapshot when scanning `pg_class`. Calling `InvalidateCatalogSnapshot()` before starting the metadata scan in `Scanner` ensures the pg_catalog snapshot used is refreshed.  Due to the difficulty of reproducing this MVCC issue, no regression or isolation test is provided, but it is easy to hit this bug when doing highly concurrent COPY:s into a distributed hypertable."
258,Fabrízio de Royes Mello,2023-03-19 16:30:48-05,7d6cf90ee783656b8230e6ed8b35b47126729423,Add missing gitignore entry,Pull request #4827 introduced a new template SQL test file but missed to add the properly `.gitignore` entry to ignore generated test files.
259,Bharathy,2023-03-20 11:29:38-05,cc51e20e87f3ce9426b88790149cd7321cc4853e,Add support for ON CONFLICT DO UPDATE for compressed hypertables,This patch fixes execution of INSERT with ON CONFLICT DO UPDATE by removing error and allowing UPDATE do happen on the given compressed hypertable.
260,Konstantina Skovola,2023-03-14 07:23:09-05,8cccc375fbf3bd8e0025f596583d3f09db489e9d,Add license information to extension description,Fixes #5436
263,Mats Kindahl,2023-03-15 08:20:41-05,67ff84e8f2e380b8466152b8054ca4d30a17a3ed,Add check for malloc failure in libpq calls,"The functions `PQconndefaults` and `PQmakeEmptyPGresult` calls `malloc` and can return NULL if it fails to allocate memory for the defaults and the empty result. It is checked with an `Assert`, but this will be removed in production builds.  Replace the `Assert` with an checks to generate an error in production builds rather than trying to de-reference the pointer and cause a crash."
264,Zoltan Haindrich,2023-03-13 09:04:00-05,790b322b242351b151bfbd46706d42dc61e05595,Fix DEFAULT value handling in decompress_chunk,The sql function decompress_chunk did not filled in default values during its operation.  Fixes #5412
265,Alexander Kuzmenkov,2023-03-15 08:27:32-05,827684f3e2718c09e93914d60b88e79488374b33,Use prepared statements for parameterized data node scans,"This allows us to avoid replanning the inner query on each new loop, speeding up the joins."
266,Sven Klemm,2023-03-14 14:21:35-05,03a799b874a1a5d6accb40a2b49ed53a77b48e0a,Mention that new status values need handling in downgrade script,When adding new status values we must make sure to add special handling for these values to the downgrade script as previous versions will not know how to deal with those.
267,Dmitry Simonenko,2023-03-13 10:15:31-05,f8022eb33253dbe91bc643c317b787e03be37e1d,Add additional tests for compression with HA,Make sure inserts into compressed chunks work when a DN is down  Fix #5039
268,Sven Klemm,2023-01-29 14:53:50-06,65562f02e88e814b4823be40fd27bee245b130ae,Support unique constraints on compressed chunks,This patch allows unique constraints on compressed chunks. When trying to INSERT into compressed chunks with unique constraints any potentially conflicting compressed batches will be decompressed to let postgres do constraint checking on the INSERT. With this patch only INSERT ON CONFLICT DO NOTHING will be supported. For decompression only segment by information is considered to determine conflicting batches. This will be enhanced in a follow-up patch to also include orderby metadata to require decompressing less batches.
269,Sven Klemm,2023-02-05 12:23:14-06,c02cb76b38ee3ecbf33a8fe0d6a5864454ee2b67,Don't reindex relation during decompress_chunk,Reindexing a relation requires AccessExclusiveLock which prevents queries on that chunk. This patch changes decompress_chunk to update the index during decompression instead of reindexing. This patch does not change the required locks as there are locking adjustments needed in other places to make it safe to weaken that lock.
270,Sven Klemm,2023-03-12 15:12:59-05,20ea406616d557bb93de5c93710656bce0a5de8f,Add utility function to map attribute numbers,This patch adds a function ts_map_attno that can be used to map the attribute number from one relation to another by column name.
271,Jan Nidzwetzki,2023-03-10 07:34:56-06,356a20777c6f1aab234c5604f0f97de8077b4890,Handle user-defined FDW options properly,"This patch changes the way user-defined FDW options (e.g., startup costs, per-tuple costs) are handled. So far, these values were retrieved in apply_fdw_and_server_options() but reset to default values afterward."
272,Maheedhar PV,2023-03-12 18:48:18-05,5e0391392aca031c601f79e885452bb82ad909d1,Out of on_proc_exit slots on guc license change,"Problem:  When the guc timescaledb.license = 'timescale' is set in the conf file and a SIGHUP is sent to postgress process and a reload of the tsl module is triggered.  This reload happens in 2 phases 1. tsl_module_load is called which will load the module only if not already loaded and 2.The ts_module_init is called for every ts_license_guc_assign_hook irrespective of if it is new load.This ts_module_init initialization function also registers a on_proc_exit function to be called on exit.  The list of on_proc_exit methods are maintained in a fixed array on_proc_exit_list of size MAX_ON_EXITS (20) which gets filled up on repeated SIGHUPs and hence an error.  Fix:  The fix is to make the ts_module_init() register the on_proc_exit callback, only in case the module is reloaded and not in every init call.  Closes #5233"
273,Alexander Kuzmenkov,2023-03-09 04:34:28-06,e92d5ba748de9ba7f26221ebb9717c7db3fcca5d,Add more tests for compression,"Unit tests for different data sequences, and SQL test for float4."
274,Jan Nidzwetzki,2023-03-09 07:19:19-06,f5db023152afed21ac5ea413b5e5ae12185a23be,Track file trailer only in debug builds,The commit 96574a7 changes the handling of the file_trailer_received flag. It is now only used in asserts and not in any other kind of logic. This patch encapsulates the file_trailer_received in a USE_ASSERT_CHECKING macro.
275,Sven Klemm,2023-03-08 16:03:51-06,217ba014a7fc8e864a1dbd1b4af8634db67ed507,Use version checks to decide about RelationGetSmgr backporting,Use explicit version checks to decide whether to define backported RelationGetSmgr function or rely on the function being available. This simplifies the cmake code a bit and make the backporting similar to how we handle this for other functions.
276,Jan Nidzwetzki,2023-03-07 06:32:02-06,7b8177aa74de886b2e88a0fad41bc8ab9feb1e87,Fix file trailer handling in the COPY fetcher,"The copy fetcher fetches tuples in batches. When the last element in the batch is the file trailer, the trailer was not handled correctly. The existing logic did not perform a PQgetCopyData in that case. Therefore the state of the fetcher was not set to EOF and the copy operation was not correctly finished at this point.  Fixes: #5323"
277,Sven Klemm,2023-03-08 06:27:13-06,a854b2760f66cc640bf2fe71683b0e4595dfbdde,Simplify ts_indexing_relation_has_primary_or_unique_index,Rely on postgres functionality for index column tracking instead of rolling our own.
278,Bharathy,2023-03-08 06:46:29-06,f54dd7b05df420df1fd3318bd3f768305c619c20,Fix SEGMENTBY columns predicates to be pushed down,WHERE clause with SEGMENTBY column of type text/bytea non-equality operators are not pushed down to Seq Scan node of compressed chunk. This patch fixes this issue.  Fixes #5286
279,Erik Nordström,2023-02-24 05:38:16-06,c76a0cff688afe970d256c31862e63207d29b2ba,Add parallel support for partialize_agg(),"Make `partialize_agg()` support parallel query execution. To make this work, the finalize node need combine the individual partials from each parallel worker, but the final step that turns the resulting partial into the finished aggregate should not happen. Thus, in the case of distributed hypertables, each data node can run a parallel query to compute a partial, and the access node can later combine and finalize these partials into the final aggregate. Esssentially, there will be one combine step (minus final) on each data node, and then another one plus final on the access node.  To implement this, the finalize aggregate plan is simply modified to elide the final step, and to reserialize the partial. It is only possible to do this at the plan stage; if done at the path stage, the PostgreSQL planner will hit assertions that assume that the node has certain values (e.g., it doesn't expect combine Paths to skip the final step)."
280,Bharathy,2023-03-07 08:41:42-06,c13ed17fbce56129136239ad369ac5e5f1502821,Fix DELETE command tag,DELETE on hypertables always reports 0 as affected rows. This patch fixes this issue.
281,Sven Klemm,2023-03-07 04:29:52-06,f680b995299c2900a1883f4a94128b89d5b61db5,Fix assertion in calculate_chunk_interval for negative target size,When called with negative chunk_target_size_bytes calculate_chunk_interval will throw an assertion. This patch adds error handling for this condition. Found by sqlsmith.
282,Sven Klemm,2023-03-07 02:42:59-06,00321dba410dbfd3984c45401941d1f5989e05de,2.10.1 Post-release adjustments,Add 2.10.1 to update test scripts and adjust the downgrade versioning.
283,Konstantina Skovola,2023-02-10 02:21:47-06,5a3cacd06f95270569082cf14ba55b9092426c17,Fix sub-second intervals in hierarchical caggs,"Previously we used date_part(""epoch"", interval) and integer division internally to determine whether the top cagg's interval is a multiple of its parent's. This led to precision loss and wrong results in the case of intervals with sub-second components.  Fixed by using the `ts_interval_value_to_internal` function to convert intervals to appropriate integer representation for division.  Fixes #5277"
284,Fabrízio de Royes Mello,2023-03-06 12:51:03-06,00b566dfe478c11134bcf1e7bcf38943e7fafe8f,Remove unused functions,We don't use `ts_catalog_delete[_only]` functions anywhere and instead we rely on `ts_catalog_delete_tid[_only]` functions so removing it from our code base.
285,Sven Klemm,2023-03-03 01:44:49-06,d386aa1def48bb2a3321beffe3bb565548666765,Release 2.10.1,This release contains bug fixes since the 2.10.0 release. We recommend that you upgrade at the next available opportunity.  **Bugfixes** * #5159 Support Continuous Aggregates names in hypertable_(detailed_)size * #5226 Fix concurrent locking with chunk_data_node table * #5317 Fix some incorrect memory handling * #5336 Use NameData and namestrcpy for names * #5343 Set PortalContext when starting job * #5360 Fix uninitialized bucket_info variable * #5362 Make copy fetcher more async * #5364 Fix num_chunks inconsistency in hypertables view * #5367 Fix column name handling in old-style continuous aggregates * #5378 Fix multinode DML HA performance regression * #5384 Fix Hierarchical Continuous Aggregates chunk_interval_size  **Thanks** * @justinozavala for reporting an issue with PL/Python procedures in the background worker * @Medvecrab for discovering an issue with copying NameData when forming heap tuples. * @pushpeepkmonroe for discovering an issue in upgrading old-style   continuous aggregates with renamed columns * @pushpeepkmonroe for discovering an issue in upgrading old-style continuous aggregates with renamed columns
286,Sven Klemm,2023-03-04 03:18:34-06,5cd2c038796fb302190b080c90e5acddbef4b8d1,Simplify windows-build-and-test-ignored.yaml,Remove code not needed for the skip workflow of the windows test.
287,Sven Klemm,2023-03-04 02:06:28-06,8a2f1f916aeee0dabc0eb653f2483ba5e8d43f74,Fix windows package test,Chocolatey has all the postgres versions we need available so we can reenable previously disabled tests. But the recent packages seem to have different versioning schema without a suffix.
288,Dmitry Simonenko,2023-03-06 09:42:36-06,830c37b5b0d2ab920e12e288000a4cdb0f2d5896,Fix concurrent locking with chunk_data_node table,"Concurrent insert into dist hypertable after a data node marked as unavailable would produce 'tuple concurrently deleted` error.  The problem occurs because of missing tuple level locking during scan and concurrent delete from chunk_data_node table afterwards, which should be treated as `SELECT … FOR UPDATE` case instead.  Based on the fix by @erimatnor.  Fix #5153"
289,Ildar Musin,2023-03-02 10:13:18-06,4c0075010dbf57d6d300feda74bfa60cb76355ba,Add hooks for hypertable drops,To properly clean up the OSM catalog we need a way to reliably track hypertable deletion (including internal hypertables for CAGGS).
291,Fabrízio de Royes Mello,2023-03-02 13:09:49-06,32046832d3af53d2432bb1b2c7c9bdf5f88fcfb8,Fix Hierarchical CAgg chunk_interval_size,When a Continuous Aggregate is created the `chunk_interval_size` is defined my the `chunk_interval_size` of the original hypertable multiplied by a fixed factor of 10.  The problem is currently when we create a Hierarchical Continuous Aggregate the same factor is applied and it lead to an exponential `chunk_interval_size`.  Fixed it by just copying the `chunk_interval_size` from the base Continuous Aggregate for an Hierachical Continuous Aggreagate.  Fixes #5382
292,Nikhil Sontakke,2023-03-02 01:54:31-06,1423b55d1888b73c40d3d326fa63f8647fd8a413,Fix perf regression due to DML HA,"We added checks via #4846 to handle DML HA when replication factor is greater than 1 and a datanode is down. Since each insert can go to a different chunk with a different set of datanodes, we added checks on every insert to check if DNs are unavailable. This increased CPU consumption on the AN leading to a performance regression for RF > 1 code paths.  This patch fixes this regression. We now track if any DN is marked as unavailable at the start of the transaction and use that information to reduce unnecessary checks for each inserted row."
293,Mats Kindahl,2023-02-28 05:04:17-06,a6ff7ba6cc15b280a275e5acd315741ec9c86acc,Rename columns in old-style continuous aggregates,"For continuous aggregates with the old-style partial aggregates renaming columns that are not in the group-by clause will generate an error when upgrading to a later version. The reason is that it is implicitly assumed that the name of the column is the same as for the direct view. This holds true for new-style continous aggregates, but is not always true for old-style continuous aggregates. In particular, columns that are not part of the `GROUP BY` clause can have an internally generated name.  This commit fixes that by extracting the name of the column from the partial view and use that when renaming the partial view column and the materialized table column."
294,Sven Klemm,2023-03-03 04:22:06-06,e2e7ae304521b74ac6b3f157a207da047d44ab06,Don't run sanitizer test on individual PRs,Sanitizer tests take a long time to run so we don't want to run them on individual PRs but instead run them nightly and on commits to master.
295,Sven Klemm,2023-03-03 01:11:49-06,9574dd8e748486d9c365c199c94b4b535a7dd250,Adjust ARM64 package test,We only package timescaledb for ubuntu kinetic on PG14 because there are no prebuilt postgres packages available for the other postgres versions.
296,Pallavi Sontakke,2023-03-03 04:22:34-06,6be14423d5c51282d5c0d6f9c0b57a4e7123a4b7,Flag test space_constraint.sql.in for release run (#5380),"It was incorrectly flagged as requiring a debug build.  Disable-check: force-changelog-changed"
297,Erik Nordström,2023-02-24 11:25:45-06,386d31bc6e51913a02b0f0a0f964b36068bb9925,Make copy fetcher more async,"Make the copy fetcher more asynchronous by separating the sending of the request for data from the receiving of the response. By doing that, the async append node can send the request to each data node before it starts reading the first response. This can massively improve the performance because the response isn't returned until the remote node has finished executing the query and is ready to return the first tuple."
298,Sotiris Stamokostas,2023-03-02 01:54:55-06,750e69ede1a3161b5a650a7a0dfe1dd9cf1fe1b0,Renamed size_utils.sql,Renamed: tsl/test/sql/size_utils.sql tsl/test/expected/size_utils.out To: tsl/test/sql/size_utils_tsl.sql tsl/test/expected/size_utils_tsl.out because conflicting with test/sql/size_utils.sql
299,Jan Nidzwetzki,2023-02-23 13:23:32-06,7887576afaa113bc6e45f5007a6c6bca21b2108d,Handle MERGE command in reference join pushdown,"At the moment, the MERGE command is not supported on distributed hypertables. This patch ensures that the join pushdown code ignores the invocation by the MERGE command."
300,shhnwz,2023-02-26 10:48:02-06,e6f6eb3ab8c95b2531401324b97d345ec6fb8ff2,Fix for inconsistent num_chunks,Different num_chunks values reported by timescaledb_information.hypertables and timescaledb_information.chunks. View definition of hypertables was not filtering dropped and osm_chunks.  Fixes #5338
301,gayyappan,2022-10-28 15:32:43-05,2f7e0433a99d48455e65bcc7e6f63f547b243c7f,Create index fails if hypertable has foreign table chunk,We cannot create indexes on foreign tables. This PR modifies process_index_chunk to skip OSM chunks
302,Bharathy,2023-02-26 21:53:07-06,56ce7907d91ba8b5ec80ff2d79c32ceaaae65eaa,Backport MERGE command specific postgresql code,This patch backports following:  1. Refactor ExecInsert/Delete/Update    Backported commit 25e777cf8e547d7423d2e1e9da71f98b9414d59e 2. Backport all MERGE related interfaces and its implementations.    Backported commit 7103ebb7aae8ab8076b7e85f335ceb8fe799097c
309,Jan Nidzwetzki,2023-02-16 08:38:58-06,330bb8f4af761f3277b4b62269dca9c50e882bac,Added coccinelle rule to find strlcpy on NameData,NameData is a fixed-size type of 64 bytes. Using strlcpy to copy data into a NameData struct can cause problems because any data that follows the initial null-terminated string will also be part of the data.
311,Oleg Tselebrovskiy,2023-02-14 00:17:04-06,0746517c77d4412909776e009143fbaed5c71f0e,Fix some incorrect memory handling,"While running TimescaleDB under valgrind I've found two cases of incorrect memory handling.  Case 1: When creating timescaledb extension, during the insertion of metadata there is some junk in memory that is not zeroed before writing there. Changes in metadata.c fix this.  Case 2: When executing GRANT smth ON ALL TABLES IN SCHEMA some_schema and deconstructing this statement into granting to individual tables, process of copying names of those tables is wrong. Currently, you aren't copying the data itself, but an address to data on a page in some buffer. There's a problem - when the page in this buffer changes, copied address would lead to wrong data. Changes in process_utility.c fix this by allocating memory and then copying needed relname there.  Fixes #5311"
312,Jan Nidzwetzki,2023-02-20 02:23:41-06,7e43c702ba2663d56feba1043431d2232d8437c8,Increase timeout for PostgreSQL in upgrade tests,"The upgrade and downgrade tests are running PostgreSQL in Docker containers. The function 'wait_for_pg' is used to determine if PostgreSQL is ready to accept connections. In contrast to the upgrade tests, the downgrade tests use more relaxed timeout values. The upgrade tests sometimes fail because PostgreSQL cannot accept connections within the configured time range. This patch applies the more relaxed timeout values also to the upgrade script."
313,Mats Kindahl,2023-02-17 06:16:52-06,0cbd7407a66c84365f8969d1a197572a57325442,Get PortalContext when starting job,"When executing functions, SPI assumes that `TopTransactionContext` is used for atomic execution contexts and `PortalContext` is used for non-atomic contexts. Since jobs need to be able to commit and start transactions, they are executing in a non-atomic context hence `PortalContext` will be used, but `PortalContext` is not set when starting the job. This is not a problem for PL/PgSQL executor, but for other executors (such as PL/Python) it would be.  This commit fixes the issue by setting the `PortalContext` variable to the portal context created for the portal and restores it (to NULL) after execution.  Fixes #5326"
314,Jan Nidzwetzki,2023-02-17 09:24:45-06,83fc20f1954c5ff0d22f790cd8ee11309724445a,Change bgw_main field length to BGW_MAXLEN,"The content of bgw_main is copied into BackgroundWorker. bgw_function_name, which has a length of BGW_MAXLEN. This patch ensures that the size of these fields has the same length."
315,Maheedhar PV,2023-02-19 23:36:05-06,91b4a66eb9ae5feea57b94707b4a8e44b6505e9d,Release 2.10.0 (#5324),"This release contains new features and bug fixes since the 2.9.3 release.  This release is high priority for upgrade. We strongly recommend that you upgrade as soon as possible.  **Features** * #5241 Allow RETURNING clause when inserting into compressed chunks * #5245 Manage life-cycle of connections via memory contexts * #5246 Make connection establishment interruptible * #5253 Make data node command execution interruptible * #5243 Enable real-time aggregation for continuous aggregates with joins * #5262 Extend enabling compression on a continuous aggregrate with 'compress_segmentby' and 'compress_orderby' parameters  **Bugfixes** * #4926 Fix corruption when inserting into compressed chunks * #5218 Add role-level security to job error log * #5214 Fix use of prepared statement in async module * #5290 Compression can't be enabled on continuous aggregates when segmentby/orderby columns need quotation * #5239 Fix next_start calculation for fixed schedules"
316,Sven Klemm,2023-02-13 01:35:41-06,ce85546b5633ba2654fc3c26c658a9381e7ce73d,Fix ABI test,Pin the OpenSSL version we compile against to OpenSSL 1.1 in backwards ABI test because OpenSLL 3 is not available across all supported versions.
317,Fabrízio de Royes Mello,2023-02-17 16:07:52-06,c7f46393e7406791f49edababe9f058995b152a0,Change usage of term nested to hierarchical,"To don't make developers confused the right name for Continuous Aggregates on top of another Continuous Aggregates is `Hierarchical Continuous Aggregates`, so changed the usage of term `nested` for `hierarchical`."
318,Nikhil Sontakke,2023-02-02 02:38:32-06,d50de8a72d1c294323ac4f0599ca1e5e1b89546f,Fix uninitialized `bucket_info.htpartcolno` warning,Found by coverity.
319,Mats Kindahl,2023-02-15 08:32:12-06,38b71d0e703caa568a38a854e041a916c5601057,Use NameData and namestrcpy for names,"Using `strlcpy` to copy variables holding PostgreSQL names can cause issues since names are fixed-size types of length 64. This means that any data that follows the initial null-terminated string will also be part of the data.  Instead of using `const char*` for PostgreSQL names, use `NameData` type for PostgreSQL names and use `namestrcpy` to copy them rather than `strlcpy`."
320,Jacob Champion,2023-02-06 11:05:33-06,09636092714bbff8689d8d37c3965b99bee0db26,Align GUC initializations with boot values,"As of upstream a73952b795 (16devel), custom GUCs have their initialized values checked against the configured boot value. If they differ, an assertion is tripped. So I've gone through the list and aligned the two sides, adding defaults where they did not previously exist for consistency.  To allow for dynamic boot values, the new assertion logic allows the initialized value to be zero, in which case any boot value will be accepted. ts_guc_max_open_chunks_per_insert relies on the value of work_mem, so I've removed its (explicit) initializer entirely."
321,Jacob Champion,2023-02-01 18:41:16-06,917f7804a73ce93bbd704061011516a14123af0e,Remove obsolete usage of AssertArg(),Upstream HEAD (16devel) removed this in commit b1099eca8f. Assert() is identical.
322,Jacob Champion,2023-02-01 19:00:19-06,f21bf6faacb3c4f9cb2a2af3f76ea3db80cdb949,Fix misuse of CStringGetDatum,"OidFunctionCall1() returns a Datum, not const char *."
323,Jacob Champion,2023-01-27 17:14:17-06,20e468f40c98b45c29452a0a8c490fabf820670d,Fix use of TextDatumGetCString(),"TextDatumGetCString() was made typesafe in upstream HEAD (16devel), so now the compiler catches this. As Tom puts it in ac50f84866:      ""TextDatumGetCString(PG_GETARG_TEXT_P(x))"" is formally wrong: a text*     is not a Datum.  Although this coding will accidentally fail to fail on     all known platforms, it risks leaking memory if a detoast step is needed,     unlike ""TextDatumGetCString(PG_GETARG_DATUM(x))"" which is what's used     elsewhere."
324,Alexander Kuzmenkov,2023-02-09 02:55:07-06,fd66f5936abb6f9271a713e15002be3c4854fb39,Warn about mismatched chunk cache sizes,"Just noticed abysmal INSERT performance when experimenting with one of our customers' data set, and turns out my cache sizes were misconfigured, leading to constant hypertable chunk cache thrashing. Show a warning to detect this misconfiguration. Also use more generous defaults, we're not supposed to run on a microwave (unlike Postgres)."
326,Zoltan Haindrich,2023-02-10 07:22:09-06,9d3866a50efecbed3604622b7b3b981bea0200c3,Accept all compression options on caggs,"Enable to properly handle 'compress_segmentby' and 'compress_orderby' compression options on continous aggregates.  ALTER MATERIALIZED VIEW test_table_cagg SET (   timescaledb.compress = true,   timescaledb.compress_segmentby = 'device_id' );  Fixes #5161"
327,Alexander Kuzmenkov,2023-02-13 01:29:58-06,d00c1f37213dced5c443baf6fdfc1d298281779d,Fix some errors in processing of code coverage data,"We were using mismatched compiler and gcov, which led to either segfaults or errors like ""GCOV_TAG_COUNTER_ARCS mismatch"". Add some cmake code that tries to find the gcov that matches the compiler.  This should hopefully fix some of the mysterious missing coverage problems that we've been experiencing for a while."
330,Alexander Kuzmenkov,2023-02-10 10:02:46-06,484a4ea3fc5acb58468f19d445b38c7aaf38fb7b,Fix our codecov repository yaml,Codecov keeps complaining that it's invalid.
332,Konstantina Skovola,2023-01-27 09:06:59-06,348796f9d9dcfc01716e78886ac1cb3de8623f8f,Fix next_start calculation for fixed schedules,"This patch fixes several issues with next_start calculation.  - Previously, the offset was added twice in some cases. This is fixed by this patch.  - Additionally, schedule intervals with month components were not handled correctly. Internally, time_bucket with origin is used to calculate the next start. However, in the case of month intervals, the timestamp calculated for a bucket is always aligned on the first day of the month, regardless of origin. Therefore, previously the result was aligned with origin by adding the difference between origin and its respective time bucket. This difference was computed as a fixed length interval in terms of days and time. That computation led to incorrect computation of next start occasionally, for example when a job should be executed on the last day of a month. That is fixed by adding an appropriate interval of months to initial_start and letting Postgres handle this computation properly.  Fixes #5216"
333,Sven Klemm,2023-02-07 03:57:36-06,756ef68d0af0b3a9a5e7616b1bb7e1ac993c6f98,Fix compression_hypertable ordering reliance,The hypertable_compression test had on implicit reliance on the ordering of tuples when querying the materialized results. This patch makes the ordering explicit in this test.
334,Alexander Kuzmenkov,2023-02-02 06:28:25-06,caf79e0f5e3b789f70361f4fd3093afcea3a1da0,"When backporting, fetch the main branch with large depth",Apparently in some cases we're getting a shallow repo in the Github Actions.
335,Alexander Kuzmenkov,2023-02-08 05:08:35-06,063a9dae295672fbbfa4087cf524cd35705afe17,Improve cost model for data node scans,"1) Simplify the path generation for the parameterized data node scans. 1) Adjust the data node scan cost if it's an index scan, instead of always    treating it as a sequential scan. 1) Hard-code the grouping estimation for distributed hypertable, instead    of using the totally bogus per-column ndistinct value. 1) Add the GUC to disable parameterized data node scan. 1) Add more tests."
336,Zoltan Haindrich,2023-02-07 03:53:09-06,cad2440b58926817912809ef48626bb1b5ca77cd,Compression can't be enabled on caggs,The continuous aggregate creation failed in case segmentby/orderby columns needed quotation.
337,Lakshmi Narayanan Sreethar,2023-02-07 01:54:09-06,1eb8aa3f14d1f5c2198b44150944295fbc253c93,Post-release fixes for 2.9.3,Bumping the previous version and adding tests for 2.9.3.
339,Sven Klemm,2023-02-05 06:13:08-06,8132908c97ae469ace776b6cf55bbe3dd8bf8971,Refactor chunk decompression functions,Restructure the code inside decompress_chunk slightly to make core loop reusable by other functions.
340,Lakshmi Narayanan Sreethar,2023-02-02 23:35:05-06,fb3ad7d6c6dd52813c703f11de6692708b883b5a,Release 2.9.3,This release contains bug fixes since the 2.9.2 release. This release is high priority for upgrade. We strongly recommend that you upgrade as soon as possible.  **Bugfixes** * #4804 Skip bucketing when start or end of refresh job is null * #5108 Fix column ordering in compressed table index not following the order of a multi-column segment by definition * #5187 Don't enable clang-tidy by default * #5255 Fix year not being considered as a multiple of day/month in hierarchical continuous aggregates * #5259 Lock down search_path in SPI calls
341,Erik Nordström,2023-02-03 05:14:13-06,206056ca12c611b2038b35bfa0eac43372a563c2,Fix dist_hypertable test,A previous change accidentally broke the dist_hypertable test so that it prematurely exited. This change restores the test so that it executes properly.
342,Erik Nordström,2023-02-02 09:50:19-06,b81033b835bd22802cf59ae5dde5c9bc137d19f5,Make data node command execution interruptible,"The function to execute remote commands on data nodes used a blocking libpq API that doesn't integrate with PostgreSQL interrupt handling, making it impossible for a user or statement timeout to cancel a remote command.  Refactor the remote command execution function to use a non-blocking API and integrate with PostgreSQL signal handling via WaitEventSets.  Partial fix for #4958.  Refactor remote command execution function"
343,Alexander Kuzmenkov,2023-02-01 07:08:22-06,c4d8f3530787ee21f8a80176d0b11b18ba585d9e,Improve automated backports,"A follow-up for the review comments in the previous PR.  1. Create one backport PR per one source PR, even with multiple commits. 1. Add a comment to the source PR if we fail to backport it for some    reason."
344,Konstantina Skovola,2023-01-31 02:38:58-06,6bc89802161656b00928dbf5fffc08a0e2fdfffb,Fix year not multiple of day/month in nested CAgg,"Previously all intervals were converted to seconds using ""epoch"" with date_part. However, this treats a year as 365.25 days to account for leap years, leading to the unexpected situation that a year is not a multiple of a day or a month.  Fixed by treating month-only intervals as multiples of 30 days.  Fixes #5231"
345,Alexander Kuzmenkov,2023-02-01 10:38:12-06,9133319081aef92705f1405087822fc281d215d4,Fix pylint again,Apparently a new version is out and it complains about Exception being too general.
346,Alexander Kuzmenkov,2023-01-26 08:58:22-06,739fd00bb9184832d6527e253cc44ca1a47585ca,Add a workflow for automated backporting,"1. Find the latest release branch 1. For each commit in main and not in release branch (compared by    title), find the corresponding PR. 1. If the PR fixes an issue labeled ""bug"", and neither the PR nor the    issue are labeled ""no-backport"", cherry-pick the commits from the PR onto the release branch, and create a PR with these changes."
347,Sven Klemm,2023-02-01 00:59:14-06,2a47462fbc930ac6230a83d07100c8a21d040b70,Remove SECURITY DEFINER from get_telemetry_report,We should not broadly make functions security definer as that increases the attack surface for our extension. Especially for our telemetry we should strive to only run it with the minimum required privileges.
352,Fabrízio de Royes Mello,2023-01-30 14:11:22-06,c0f2ed18095f21ac737f96fe93e4035dbfeeaf2c,Mark cagg_watermark parallel safe,"The `cagg_watermark` function perform just read-only operations so is safe to make it parallel safe to take advantage of the Postgres parallel query.  Since 2.7 when we introduced the new Continuous Aggregate format we don't use partials anymore and those aggregate functions `partialize_agg` and `finalize_agg` are not parallel safe, so make no sense don't take advantage of Postgres parallel query for realtime Continuous Aggregates."
354,Erik Nordström,2023-01-24 11:18:22-06,d489ed6f3225931d400ac7b664dbb003e76a7f0b,Fix use of prepared statement in async module,"Broken code caused the async connection module to never send queries using prepared statements. Instead, queries were sent using the parameterized query statement instead.  Fix this so that prepared statements are used when created."
355,Lakshmi Narayanan Sreethar,2023-01-30 00:49:03-06,1a3e7ad7d19f2530f25bd30752957c04ac8fa82e,Run dist_move_chunk as a solo test in PG15,"When run in a parallel group, the dist_move_chunk test can get into a deadlock with another test running a 'DROP DATABASE' command. So, mark it as a solo test to disallow it from running in a parallel group.  Closes #4972"
356,Lakshmi Narayanan Sreethar,2023-01-30 00:16:11-06,03b740cd709b5e1887352f3ca1d879773ed8a923,Enable telemetry_stats testcase,The telemetry_stats testcase was accidentally disabled by PR #5162.
357,Erik Nordström,2023-01-30 04:57:05-06,5d12a3883d80f026252eaac8416143de6cd5653d,Make connection establishment interruptible,"Refactor the data node connection establishment so that it is interruptible, e.g., by ctrl-c or `statement_timeout`.  Previously, the connection establishment used blocking libpq calls. By instead using asynchronous connection APIs and integrating with PostgreSQL interrupt handling, the connection establishment can be canceled by an interrupt caused by a statement timeout or a user.  Fixes #2757"
358,Sven Klemm,2023-01-30 06:36:55-06,01ea255f2f3226178b42cff597c427ec66c62442,Rename variable in ts_chunk_dispatch_get_chunk_insert_state,The variable new_chunk was misleading since the chunk could be either a new chunk or an existing chunk.
359,Sven Klemm,2023-01-30 03:31:53-06,b229b3aefd01049ee6fd046f762ad7a9fcb2a6e2,Small decompress_chunk refactor,Refactor the decompression code to move the decompressor initialization into a separate function.
360,Erik Nordström,2023-01-30 04:14:51-06,cce0e18c3665fec327797b2d469eafb411ee3050,Manage life-cycle of connections via memory contexts,"Tie the life cycle of a data node connection to the memory context it is created on. Previously, a data node connection was automatically closed at the end of a transaction, although often a connection needs to live beyond a single transaction. For example, a connection cache is maintained for data node connections, and, for such cases, a flag was set on a connection to avoid closing it automatically.  Instead of tying connections to transactions, they are now life-cycle managed via memory contexts. This simplifies the handling of connections and avoids having to create exceptions to closing connections at transaction end."
361,Sven Klemm,2023-01-30 05:37:33-06,872128438f16cccb60e570a49f9ab51791964a91,Update checkout version in changelog-check,Update checkout to not pull in Node.js 12 which is deprecated on GitHub.  https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/.
362,Mats Kindahl,2023-01-25 01:38:56-06,5661ff15236ea6bba8125160eba71538910f1cb9,Add role-level security to job error log,"Since the job error log can contain information from many different sources and also from many different jobs it is important to ensure that visibility of the job error log entries is restricted to job owners.  This commit extend the view `timescaledb_information.job_errors` with role-based checks so that a user can only see entries for jobs that she has permission to view and restrict the permissions to `_timescaledb_internal.job_errors` so that users only can view the job error log through the view. A special case is added so that the superuser and the database owner can see all log entries, even if there is no associated job id with the log entry.  Closes #5217"
363,Alexander Kuzmenkov,2023-01-25 07:45:00-06,c89fb2550dce9c35c7b89cfa33c8c813a579f610,Run sqlsmith on all commits in main,This would help to gather more statistics. We have some rare assertion failures that happen with SQLSmith like #4185
364,Alexander Kuzmenkov,2023-01-27 08:46:09-06,21a3f8206c0de98932867096637c7d1e3d04d925,Run python linter and formatter in CI,Helps find some errors and cosmetic problems.
365,Sven Klemm,2023-01-29 17:15:51-06,334864127db53edd711d1053a26aaeeb346f6d33,Stop blocking RETURNING for compressed chunks,Recent refactorings in the INSERT into compressed chunk code path allowed us to support this feature but the check to prevent users from using this feature was not removed as part of that patch. This patch removes the blocker code and adds a minimal test case.
366,Rafia Sabih,2022-10-07 09:25:46-05,043092a97fc7348f2ddfa08edd758723de93993c,Fix timestamp out of range,"When start or end for a refresh job is null, then it gives an error while bucketing because start and end are already min and max timestamp value before bucketing. Hence, skip bucketing for this case.  Partly fixes #4117"
367,Bharathy,2023-01-24 08:46:12-06,684637a172b3a05a7500425cd8ce82cf7aad6092,Post-release fixes for 2.9.2,Bumping the previous version and adding tests for 2.9.2
368,Alexander Kuzmenkov,2023-01-20 06:52:00-06,552950d221b332c6e0cbc2682e39022f2b972f32,Save SQLSmith results to the CI DB,Mostly we are interested in stacktraces and failed queries here.
369,Rafia Sabih,2022-10-24 06:05:55-05,a67b90e977194f3e55c93ed6b3f5d2a671d503c1,Allow joins in continuous aggregates,Enable the support of having join in the query used for creating the continuous aggregates. It has follwoing restrictions- 1. Join can involve only one hypertable and one normal table 2. Join should be a inner join 3. Join condition can only be equality
370,Jan Nidzwetzki,2023-01-18 07:47:48-06,9ae3ae33b758fb05ee07c90078d0a20790e196fa,Add scan plan logic for remote joins,This patch adds the missing functionality to create scan plans for remote joins. Most of the code is a backport from PG Upstream.
371,Bharathy,2023-01-23 03:35:30-06,f211294c613a40a56604a7be6ac0d1663ea49958,Release 2.9.2,This release contains bug fixes since the 2.9.1 release. We recommend that you upgrade at the next available opportunity.  **Bugfixes** * #5114 Fix issue with deleting data node and dropping the database on multi-node * #5133 Fix creating a CAgg on a CAgg where the time column is in a different order of the original hypertable * #5152 Fix adding column with NULL constraint to compressed hypertable * #5170 Fix CAgg on CAgg variable bucket size validation * #5180 Fix default data node availability status on multi-node * #5181 Fix ChunkAppend and ConstraintAwareAppend with TidRangeScan child subplan * #5193 Fix repartition behavior when attaching data node on multi-node
372,Alexander Kuzmenkov,2023-01-16 09:04:55-06,d2254cb5c5a8e267abcd8503c4b3f17596d1004c,Don't enable clang-tidy by default,"This is aimed at developers. If we enable it by default, it confuses our users and slows down the build for them."
373,Konstantina Skovola,2023-01-20 01:33:30-06,014b7b9664f828c30b98dd36a563ac6190f08337,Move .gitattributes to root and remove build-13,Commit effc8efe148c4ec0048bd7c1dfe0ca01df2afdc9 accidentally placed .gitattributes inside a build-13 directory instead of the root of the project. This commit removes build-13 and fixes the structure.
374,Jan Nidzwetzki,2023-01-17 03:36:44-06,28dbeaa2ca98c6f86c4b8cee6241841df80855cd,Add cost estimation for remote joins,This patch adds the missing functionality to estimate the costs of remote joins. Most of the code is a backport from PG Upstream.
376,Erik Nordström,2023-01-17 07:10:01-06,167625984094c4a62770521d60feeaa1b7acc8d6,Fix repartition behavior when attaching data node,"When attaching a data node and specifying `repartition=>false`, the current number of partitions should remain instead of recalculating the partitioning based on the number of data nodes.  Fixes #5157"
378,Bharathy,2023-01-17 23:02:56-06,9a2cbe30a102a9d027f832b6c288b0b9adddf62a,"Fix ChunkAppend, ConstraintAwareAppend child subplan","When TidRangeScan is child of ChunkAppend or ConstraintAwareAppend node, an error is reported as ""invalid child of chunk append: Node (26)"". This patch fixes the issue by recognising TidRangeScan as a valid child.  Fixes: #4872"
380,Alexander Kuzmenkov,2023-01-17 03:36:41-06,6aa3d6e2fc7d68b4ef4946ad2b8b186202c946ae,Don't run the APT package test on PRs,We run them nightly on main.
381,Jan Nidzwetzki,2023-01-17 02:07:43-06,1d3d81491b9116dc00d609ba9e900c1f91064eca,Update issue workflow actions and used token,This patch updates the used issue workflow actions and ensures that the 'secrets.ORG_AUTOMATION_TOKEN' is used by all actions.
382,Dmitry Simonenko,2023-01-16 07:20:25-06,5c897ff75dd014d00484507ee96eb2730fda596c,Fix default data node availability status,"Function alter_data_node() return uninitialized value for ""available"" option when it is not presented in the option list.  Fix #5154"
383,Sven Klemm,2023-01-16 03:15:15-06,dbe89644b5abf9d5a15f43d8d260b676ea8af0a1,Remove no longer used compression code,The recent refactoring of INSERT into compression chunk made this code obsolete but forgot to remove it in that patch.
384,Fabrízio de Royes Mello,2023-01-10 12:19:18-06,73df496c75c206745d44bab2088bf4c6c661ec51,Fix CAgg on CAgg variable bucket size validation,"Previous attempt to fix it (PR #5130) was not entirely correct because the bucket width calculation for interval width was wrong.  Fixed it by properly calculate the bucket width for intervals using the Postgres internal function `interval_part` to extract the epoch of the interval and execute the validations. For integer widths use the already calculated bucket width.  Fixes #5158, #5168"
385,Mats Kindahl,2023-01-13 04:33:38-06,ca9d508edeada0deba58beda5f1735631af2b21b,Add missing ignore files,The workflow ignore files for 32-bit Linux builds and Windows was missing from #5156 so these are added here.
386,Erik Nordström,2022-12-22 03:24:42-06,1e7b9bc558b1e6f9d036cf10baabc214f8eeb8d6,Fix issue with deleting data node and dropping database,"When deleting a data node with the option `drop_database=>true`, the database is deleted even if the command fails.  Fix this behavior by dropping the remote database at the end of the delete data node operation so that other checks fail first."
387,Fabrízio de Royes Mello,2023-01-11 14:29:32-06,4118a72575d4a1d493ec0884e659e6a7437fd3a0,Remove parallel safe from partialize_agg,Previous PR #4307 mark `partialize_agg` and `finalize_agg` as parallel safe but this change is leading to incorrect results in some cases.  Those functions are supposed work in parallel but seems is not the case and it is not evident yet the root cause and how to properly use it in parallel queries so we decided to revert this change and provide correct results to users.  Fixes #4922
388,Mats Kindahl,2023-01-09 06:53:58-06,f36db10826f53b48f6cacd5083342aa79fc8df87,Do not run regress workflows on benign changes,"If only documentation is changed, the full regression check workflow will still be executed, so this commit will instead skip running the regression workflows if there are only changes to files that will not affect the success of the workflow."
389,Jan Nidzwetzki,2023-01-03 08:59:02-06,396bc6def7c50523367f5ca12f688cd6d508d321,Add join functionality to the deparser code,This patch adds the missing functionality to handle joins to the deparser. Most of the code is a backport from PG Upstream.
390,Sven Klemm,2023-01-10 05:14:41-06,cfd34f2752213de6cf7b52b0e5ffffb573883ed8,Restructure chunk_dispatch,This patch adjusts the code layout for chunk_dispatch to be similar to the other custom nodes. All the files related to chunk_dispatch are moved into dedicated nodes/chunk_dispatch directory.
391,Lakshmi Narayanan Sreethar,2023-01-10 03:52:48-06,06eca172bda323779ba2105814507494e4d81303,Fix telemetry_stats test failure in PG15,The telemetry_stats testcase uses random() with seed(1) to generate the column values on which the hypertable is partitioned. The Postgres commit postgres/postgres@3804539e48 updates the random() implementation to use a better algorithim causing the test to generate a different set of rows in PG15. Due to this the test failed in PG15 as the distrubution stats of the tuples have now changed. Fixed that by creating separate test outputs for PG15 and other releases.  Fixes #5037
392,Lakshmi Narayanan Sreethar,2023-01-10 02:10:22-06,7d3d260afb1f47ff0e7be5a49bfeb715198a1e79,Skip auto assigning reviewers for draft PRs,Reviewers should be assigned only when the PR is ready for review.
394,Fabrízio de Royes Mello,2022-12-26 15:03:07-06,cd48553de5464505df6006b987cf7970bb48fe76,Fix CAgg on CAgg variable bucket size validation,During the creation of a CAgg on top of another CAgg we check if the bucket width is multiple of the parent and for this arithmetic we made an assumption that picking just the `month` part of the `interval` for variable bucket size was enought.  This assumption was wrong for bucket sizes that doesn't have the month part (i.e: using timezones) leading to division by zero error because in that case the `month` part of an `interval` is equal to 0 (zero).  Fixed it by properly calculating the bucket width for variable sized buckets.  Fixes #5126
395,Fabrízio de Royes Mello,2022-12-28 12:56:49-06,73c97524a0130e728eda2d58f82cba05ddad854a,Fix CAgg on CAgg different column order,Creating a CAgg on a CAgg where the time column is in a different order of the original hypertable was raising the following exception:  `ERROR:  time bucket function must reference a hypertable dimension column`  The problem was during the validation we're initializing internal data structure with the wrong hypertable metadata.  Fixes #5131
396,Fabrízio de Royes Mello,2023-01-05 13:02:11-06,41d6a1f142a80fbb781c77feafba7d75e6707f25,Fix adding column with NULL constraint,"Adding new column with NULL constraint to a compressed hypertable is raising an error but it make no sense because NULL constraints in Postgres does nothing, I mean it is useless and exist just for compatibility with other database systems: https://www.postgresql.org/docs/current/ddl-constraints.html#id-1.5.4.6.6  Fixed it by ignoring the NULL constraints when we check for `ALTER TABLE .. ADD COLUMN ..` to a compressed hypertable.  Fixes #5151"
397,Jan Nidzwetzki,2022-12-01 02:57:09-06,1751efbaea57be75d813fbf93a0c9b4ec8a7d994,Improve the PR workflow actions,"This patch includes two changes to the PR handling workflow:  (1) It changes the trigger for the workflow to pull_request_target. So,     PRs can now also be assigned to reviewers if the PR is opened from     external sources.  (2) A workflow has been added that automatically assigns PRs to the     author."
398,Alexander Kuzmenkov,2023-01-03 05:41:14-06,2c70dc622e4ca36deb3b1afa3ae51ebe96f4be5d,Run all the tests on every commit in main,"If we test every commit in master, we can allow GitHub to merge the PRs automatically without requiring a manual rebase on the current master. These rebases are a real time sink."
399,Sven Klemm,2023-01-02 02:33:24-06,64e8e31c6e2a33e5c17bafab7ded90dc23588c32,Improve ASSERT_IS_VALID_CHUNK macro,Currently when ASSERT_IS_VALID_CHUNK fails it is impossible to tell which of the conditions fails without opening the coredump in debugger as all the conditions are ANDed in a single Assert. This patch splits the conditions into individual Asserts so you can immediately see from stacktrace which condition failed.
400,Bharathy,2023-01-02 04:00:22-06,3a8d294d5862a9c2c3244b0754affab4472e5756,SELECT from partial compressed chunks crashes,"SELECT from partially compressed chunk crashes due to reference to NULL pointer. When generating paths for DecompressChunk, uncompressed_partial_path is null which is not checked, thus causing a crash. This patch checks for NULL before calling create_append_path().  Fixes #5134"
403,Sven Klemm,2022-12-23 00:14:25-06,93667df7d883af7f64bea60719902d705333253c,Release 2.9.1,This release contains bug fixes since the 2.9.0 release. This release is high priority for upgrade. We strongly recommend that you upgrade as soon as possible.  **Bugfixes** * #5072 Fix CAgg on CAgg bucket size validation * #5101 Fix enabling compression on caggs with renamed columns * #5106 Fix building against PG15 on Windows * #5117 Fix postgres server restart on background worker exit * #5121 Fix privileges for job_errors in update script
405,Konstantina Skovola,2022-12-22 13:23:51-06,cdf8676689549a3240126c603283b6012add3f13,Fix postgres server restart on background worker exit,Fixed by removing the croak signal handler that was introduced to enable the silent exit of the telemetry job.
407,Sven Klemm,2022-12-22 05:03:35-06,b1314e63f2ff6151ab5becfb105afa3682286a4d,Fix RPM package test for PG15 on centos 7,Installing PG15 on Centos 7 requires the EPEL repository to satisfy the dependencies.
408,Sven Klemm,2022-11-05 05:13:25-05,4527f51e7c0e5e923fcf37e4a15d5ee424b4f83b,Refactor INSERT into compressed chunks,This patch changes INSERTs into compressed chunks to no longer be immediately compressed but stored in the uncompressed chunk instead and later merged with the compressed chunk by a separate job.  This greatly simplifies the INSERT-codepath as we no longer have to rewrite the target of INSERTs and on-the-fly compress leading to a roughly 2x improvement on INSERT rate into compressed chunk. Additionally this improves TRIGGER-support for INSERTs into compressed chunks.  This is a necessary refactoring to allow UPSERT/UPDATE/DELETE on compressed chunks in follow-patches.
410,Sven Klemm,2022-12-20 04:38:48-06,1d5167233650c7fda45009bd09e3bc0948b8e473,Skip package downgrade test for PG15,Since we currently only have 1 released version that supports PG15 we cannot test downgrade with PG15.
411,Lakshmi Narayanan Sreethar,2022-12-07 01:52:42-06,a4806827dca4c87c44e85e29704584144bd2912e,Enable PG15 in various CI checks,This commit enables PG15 in the following workflows:  - Regression Linux  - ABI test  - Memory tests  - Coverity  - SQLSmith  - Additional cron tests  Co-authored-by: Bharathy Satish <bharathy@timescale.com>
412,Fabrízio de Royes Mello,2022-12-09 13:01:50-06,024b1e1f30db0c58b49eae04ff0b50055b191734,Fix CAgg on CAgg bucket size validation,The bucket size of a Continuous Aggregate should be greater or equal to the parent Continuous Aggregate because there are many cases where you actually want to roll up on another dimension.
413,Bharathy,2022-12-20 08:20:39-06,c5e496a554e9f4d04578f39669108554c22c918d,Fix column ordering in compressed table index.,"When defining compression segment by parameter using multiple columns, the parameter ordering is not respected for index creation.  This patch fixes the issue by maintaining the same order in which user has defined columns in segment by clause.  Fixes #5104"
414,Sven Klemm,2022-12-19 16:51:57-06,7d1b74a8c6b0e07c7cd156943f48fc9be16876dc,Use rand() instead of random(),Use rand() instead of random() cause the latter is not available on Windows and postgres stopped backporting it with PG15. Ideally we switch to the crypto functions added in PG15 but that requires a bit more work and this is the minimal change required to get it to build against PG15 on Windows.
416,Sven Klemm,2022-12-15 09:42:46-06,08bb21f7e69b636a736635e081bf16b1083eacb1,2.9.0 Post-release adjustments,Add 2.9.0 to update test scripts and adjust downgrade scripts for 2.9.0. Additionally adjust CHANGELOG to sync with the actual release CHANGELOG and add PG15 to CI.
417,Alexander Kuzmenkov,2022-12-01 11:18:34-06,27310470bee4790c66d2c75b9e0b58e14145fed4,Allow AsyncAppend under IncrementalSort,We forgot to add a case for it.
418,Sven Klemm,2022-12-17 01:57:21-06,c0e9bb4a30b9d1b15d057992e803f975f2c22358,Fix enabling compression on caggs with renamed columns,On caggs with realtime aggregation changing the column name does not update all the column aliases inside the view metadata. This patch changes the code that creates the compression configuration for caggs to get the column name from the materialization hypertable instead of the view internals.  Fixes #5100
419,Fabrízio de Royes Mello,2022-12-02 08:39:00-06,317f8f1a9964df67960394145c83be1e1f9d1460,Refactor CAggs on CAggs regression tests,When CAggs on CAggs was introduced in commit 3749953 the regression tests was splited into 6 (six) different test suites.  Simplyfied it grouping tests and reduced to just 2 (two) different test suites. It save resources and time because each suite test spawn it own Postgres instance.
420,Fabrízio de Royes Mello,2022-12-16 11:07:09-06,854b67774904e1d244aef2dcd49e533e33d52167,Fix broken postgres install on Windows CI,The last minor versions for PG14 (14.6) and PG15 (15.1) were unlisted by chocolatey maintainers due to some issues.  Fixed it by hardcoding the 14.5 until the packages become available again.
421,Fabrízio de Royes Mello,2022-12-05 11:31:30-06,4694c7d7986f9c6f2d61201c94273e980f45f224,Refactor CAgg migrate regression tests,When CAggs migration was introduced in commit e34218ce the regression tests was splited into 6 (six) different test suites.  Simplyfied it grouping tests and reduced to just 2 (two) different test suites. It save resources and time because each suite test spawn it own Postgres instance.
422,Mats Kindahl,2022-12-05 06:26:51-06,e5843dddd3e97fb1fafc82336335abff0696dca1,Link development and design documentation,"The tree contains a lot of design and architecture documents, but they are not linked together, so this commits adds a few additional README and build a basic structure for the documentation."
423,shhnwz,2022-10-13 10:08:05-05,601b37daa84c33917157d9a57a07ca3b5832b630,Index support for compress chunk,"It allows to override tuplesort with indexscan if compression setting keys matches with Index keys. Moreover this feature has Enable/Disable Toggle. To Disable from the client use the following command, SET timescaledb.enable_compression_indexscan = 'OFF'"
424,Ante Kresic,2022-12-15 02:59:42-06,cbf51803dd38b1e05a066ce4636ec97fb1057464,Fix index att number calculation,Attribute offset was used by mistake where attribute number was needed causing wrong values to be fetched when scanning compressed chunk index.
425,Ante Kresic,2022-12-14 11:53:00-06,3f9e3379a93ee2657d48c26c3d8a1dbe530b441f,Allow BitmapHeapScans on non-parameterized paths,"Planner can decide to use BitmapHeapScans exclusively when scanning compressed chunks. Since we filter out such scans due to previously reported issues, this can lead to no query plan devised when querying compressed chunks. This change allows Bitmap scans on compressed chunks only when it is not parameterized.  Fixes #5090"
493,Mats Kindahl,2022-11-14 05:38:45-06,141e114ccb5207bd5ba5f99ee7d46d1736bad899,Fix race in bgw_db_scheduler_fixed,"When deleting a job in the test, the job does not necessarily terminate immediately, so wait for a log entries from the job before checking the jobs table.  Fixed #4859"
426,Lakshmi Narayanan Sreethar,2022-12-13 11:39:24-06,3b3846b0ffc2b9d4ab4de505ba50b15044d40a62,Fix assertion failure in cursor_fetcher_rewind,"The cursor_fetcher_rewind method assumes that the data node cursor is rewind either after eof or when there is an associated request. But the rewind can also occur once the server has generated required number of rows by joining the relation being scanned with another regular relation. In this case, the fetch would not have reached eof and there will be no associated requests as the rows would have been already loaded into the cursor causing the assertion in cursor_fetcher_rewind to fail. Fixed that by removing the Assert and updating cursor_fetcher_rewind to discard the response only if there is an associated request.  Fixes #5053"
427,Jan Nidzwetzki,2022-12-14 03:00:10-06,940626b1d4d6458cfa10559fd56fe57f2117288b,Fix Git permission issue during CI build,"The new permissions checks to fix CVE-2022-29187 in Git caused some issues in our CI pipeline. This patch adds the checkout directory to Git's ""safe.directory"" setting."
428,Mats Kindahl,2022-12-13 02:41:56-06,558688c86f2e02cc8b5721e58294dbe552364772,Reset baserel cache on invalid hypertable cache,"When popping the hypertable cache stack, it might happen that the hypertable cache was invalidated between the push and the pop. In that case, the baserel cache can contain invalid entries pointing to the now popped hypertable cache, so we reset the baserel cache.  Fixes #4795"
431,Bharathy,2022-12-12 05:07:27-06,dd65a6b43676459668f100ffdf58ab112ff911d9,Fix segfault after second ANALYZE,"Issue occurs in extended query protocol mode only where every query goes through PREPARE and EXECUTE phase. First time ANALYZE is executed, a list of relations to be vaccumed is extracted and saved in a list. This list is referenced in parsetree node. Once execution of ANALYZE is complete, this list is cleaned up, however reference to the same is not cleaned up in parsetree node. When second time ANALYZE is executed, segfault happens as we access an invalid memory location.  Fixed the issue by restoring the actual value in parsetree node once ANALYZE completes its execution.  Fixes #4857"
432,Jan Nidzwetzki,2022-12-08 08:56:29-06,d92739099b7135712f15d6e0c5e9240d5b430109,Reduce test group size in sanitizer runs,"When the sanitizer is active, the tests require a lot of memory. If they are run in large parallel groups, out-of-memory situations can occur. This patch reduces the size of parallel groups to 5 when the sanitizer is active."
433,Alexander Kuzmenkov,2022-12-08 08:40:35-06,a01e483bf3552ead468bf6156d734a17f0007d4c,More gdb output in CI,Print locals and arguments.
434,Jan Nidzwetzki,2022-12-07 13:59:00-06,c76dfa0acbd19b33c8ef43e84c88a6f69e6ae8ff,Improve Sanitizer checks,This patch contains two changes to the Sanitizer checks:  (1) All logfiles of the Sanitizer will be uploaded to the     CI database.  (2) The Sanitizer checks are executed on every PR.
435,Jan Nidzwetzki,2022-12-07 15:37:19-06,323d41b53b2880a7b47b776eb882330790bf530d,Ensure dist_hypertable is executed as solo test,"The `dist_hypertable` test needs a lot of memory, especially when the sanitizer is enabled. This patch runs this test as a `SOLO_TEST`. This ensures that PostgreSQL does not run into an out-of-memory situation."
436,Jan Nidzwetzki,2022-12-07 10:52:53-06,5fd9170b0a4697fc55ffd32275d2348d42b26d6a,Correct sanitizer log directory,"So far, we have treated the 'log_path' setting of the sanitizer like a file. In fact, this value is used as a prefix for the created log file. Since we expected the exact file name when uploading the sanitizer output, this file was not found and we lost the messages of the sanitizer. This PR changes the behavior. We now treat the setting as a prefix and upload all files created in a new sanitizer output folder."
437,Bharathy,2022-12-05 22:20:31-06,bfed42c2d371322b7f5bcddfbf43d09042296379,Fix remote_txn on PG15,"In remote_txn, testcases which kill remote processes on data nodes, tend to rollback transactions and as part of the process, WARNINGS/ERROR are reported to client. Client however reports WARNINGS/ERROR in different order intermittently. This is an issue specific to psql utility. Placing psql in gdb and trying to diagnise the problem does not reproduce the issue.  This patch fixes the tests by not reporting WARNINGS.  Fixes #4837"
438,Erik Nordström,2022-11-15 04:52:34-06,fd42fe76fa37d1dce9e1aa3d6fe6709c797310c5,Read until EOF in COPY fetcher,"Ensure the COPY fetcher implementation reads data until EOF with `PQgetCopyData()`. Also ensure the malloc'ed copy data is freed with `PQfreemem()` if an error is thrown in the processing loop.  Previously, the COPY fetcher didn't read until EOF, and instead assumed EOF when the COPY file trailer is received. Since EOF wasn't reached, it required terminating the COPY with an extra call to the (deprecated) `PQendcopy()` function.  Still, there are cases when a COPY needs to be prematurely terminated, for example, when querying with a LIMIT clause. Therefore, distinguish between ""normal"" end (when receiving EOF) and forceful end (cancel the ongoing query)."
439,Sachin,2022-11-30 06:32:58-06,cd4509c2a35d86f6b5d4a2dc8701ef116693cf9a,Release 2.9.0,This release adds major new features since the 2.8.1 release. We deem it moderate priority for upgrading.  This release includes these noteworthy features: * Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate) * Improve `time_bucket_gapfill` function allowing specifying timezone to bucket * Use `alter_data_node()` to change the data node configuration. This function introduces the option to configure the availability of the data node.  This release also includes several bug fixes.  **Features** * #4476 Batch rows on access node for distributed COPY * #4567 Exponentially backoff when out of background workers * #4650 Show warnings when not following best practices * #4664 Introduce fixed schedules for background jobs * #4668 Hierarchical Continuous Aggregates * #4670 Add timezone support to time_bucket_gapfill * #4678 Add interface for troubleshooting job failures * #4718 Add ability to merge chunks while compressing * #4786 Extend the now() optimization to also apply to CURRENT_TIMESTAMP * #4820 Support parameterized data node scans in joins * #4830 Add function to change configuration of a data nodes * #4966 Handle DML activity when datanode is not available * #4971 Add function to drop stale chunks on a datanode  **Bugfixes** * #4663 Don't error when compression metadata is missing * #4673 Fix now() constification for VIEWs * #4681 Fix compression_chunk_size primary key * #4696 Report warning when enabling compression on hypertable * #4745 Fix FK constraint violation error while insert into hypertable which references partitioned table * #4756 Improve compression job IO performance * #4770 Continue compressing other chunks after an error * #4794 Fix degraded performance seen on timescaledb_internal.hypertable_local_size() function * #4807 Fix segmentation fault during INSERT into compressed hypertable * #4822 Fix missing segmentby compression option in CAGGs * #4823 Fix a crash that could occur when using nested user-defined functions with hypertables * #4840 Fix performance regressions in the copy code * #4860 Block multi-statement DDL command in one query * #4898 Fix cagg migration failure when trying to resume * #4904 Remove BitmapScan support in DecompressChunk * #4906 Fix a performance regression in the query planner by speeding up frozen chunk state checks * #4910 Fix a typo in process_compressed_data_out * #4918 Cagg migration orphans cagg policy * #4941 Restrict usage of the old format (pre 2.7) of continuous aggregates in PostgreSQL 15. * #4955 Fix cagg migration for hypertables using timestamp without timezone * #4968 Check for interrupts in gapfill main loop * #4988 Fix cagg migration crash when refreshing the newly created cagg  **Thanks** * @jflambert for reporting a crash with nested user-defined functions. * @jvanns for reporting hypertable FK reference to vanilla PostgreSQL partitioned table doesn't seem to work * @kou for fixing a typo in process_compressed_data_out * @xvaara for helping reproduce a bug with bitmap scans in transparent decompression * @byazici for reporting a problem with segmentby on compressed caggs * @tobiasdirksen for requesting Continuous aggregate on top of another continuous aggregate * @xima for reporting a bug in Cagg migration
440,Sachin,2022-12-01 08:37:07-06,29f35da905e99dace4b5cf87b2653a5b5c7e25ee,Fix Github CI failures,"Not specifying alpine version causes libssl version to change, which in turn cause error in downgrade tests as well as ABI tests. This commit also fixes shellcheck failures. Some failing windows tests are addd to ignore list.  Co-authored-by: Lakshmi Narayanan Sreethar <lakshmi@timescale.com> Co-authored-by: Alexander Kuzmenkov <akuzmenkov@timescale.com> Signed-off-by: Sachin <sachin@timescale.com>"
441,Sven Klemm,2022-11-29 03:41:47-06,1a806e2fde2e6148cd7ffd02752ed59478d426cd,Check for presence of RelationGetSmgr,RelationGetSmgr was backported by upstream to the STABLE branches but is not yet available in any released version so we cannot use pg version to determine presence of RelationGetSmgr.
442,Mats Kindahl,2022-11-24 04:03:26-06,09c0ba713691f69cdc96c992c44f1654181e9838,Do not spam log with telemetry problems,"The telemetry process runs on a regular basis and usually does not make a lot of noise, but in a few particular cases, it writes entries to the log unnecessarily.  If the telemetry server cannot be contacted, it will print a warning in the log that the server cannot be contacted. Since it is nothing wrong with the system and the telemetry process will try to re-connect at a later time, it is unnecessary to print as a warning.  If the telemetry response is malformed, a warning is printed. This is also unnecessary since there is nothing wrong with the system, there is nothing the user can do about it, and this warning can be largely ignored.  If the hard-coded telemetry scheme is incorrect, a warning will be printed. This should not normally happen, and if it happens on a running server, there is nothing that can be done to eliminate the error message and the message is unnecessary.  When the telemetry job exits, a standard termination message is printed in the log. Although harmless, it is mostly confusing and provide no value to the user.  If the telemetry process is attempting to connect, or is connected, to the telemetry server, the telemetry server will wait until the connection gets a timeout before shutting down. This is unnecessary since there is no critical problem in aborting the connection and doing a direct shutdown.  This commit turns those warnings into notices and installs a signal handler so that the telemetry job exits silently and abort any outstanding connections.  Fixes #4028"
443,Sven Klemm,2022-11-26 04:18:27-06,558da2c5c698aa4237f3ebc35dd12e7c511c2a90,Use RelationGetSmgr instead of rd_smgr,rd_smgr should not be accessed directly but RelationGetSmgr should be used instead. Accessing it directly can lead to segfaults when parallel relcache flushes are happening.  https://github.com/postgres/postgres/commit/f10f0ae420ee62400876ab34dca2c09c20dcd030
444,Sven Klemm,2022-11-26 17:00:37-06,2d0087a0e7c2be92cd5f4a1e47098d73c5ecde30,Fix segfault in cagg creation,When trying to create cagg on top of any relation that is a neither a hypertable nor a continuous aggregate the command would segfault. This patch changes the code to handle this case gracefully and error out when trying to create a cagg on top of a relation that is not supported. Found by coverity.
445,Fabrízio de Royes Mello,2022-11-24 10:19:36-06,35c91204987ccb0161d745af1a39b7eb91bc65a5,Add Hierarchical Continuous Aggregates validations,Commit 3749953e introduce Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate) but it lacks of some basic validations.  Validations added during the creation of a Hierarchical Continuous Aggregate:  * Forbid create a continuous aggregate with fixed-width bucket on top of   a continuous aggregate with variable-width bucket.  * Forbid incompatible bucket widths:   - should not be equal;   - bucket width of the new continuous aggregate should be greater than     the source continuous aggregate;   - bucket width of the new continuous aggregate should be multiple of     the source continuous aggregate.
446,Sven Klemm,2022-11-25 01:27:45-06,83b13cf6f73a74656dde9cc6ec6cf76740cddd3c,Use packaged postgres for sqlsmith and coverity CI,The sqlsmith and coverity workflows used the cache postgres build but could not produce a build by themselves and therefore relied on other workflows to produce the cached binaries. This patch changes those workflows to use normal postgres packages instead of custom built postgres to remove that dependency.
447,Sven Klemm,2022-11-21 10:36:55-06,3b94b996f248291caa37de206ef65dd4cc3a56c2,Use custom node to block frozen chunk modifications,This patch changes the code that blocks frozen chunk modifications to no longer use triggers but to use custom node instead. Frozen chunks is a timescaledb internal object and should therefore not be protected by TRIGGER which is external and creates several hazards. TRIGGERs created to protect internal state contend with user-created triggers. The trigger created to protect frozen chunks does not work well with our restoring GUC which we use when restoring logical dumps. Thirdly triggers are not functional for any internal operations but are only working in code paths that explicitly added trigger support.
448,Mats Kindahl,2022-11-25 09:10:40-06,ce778faa117f503231b3cd62d94d6718670ac257,Updating scheduled run,Updating scheduled run to avoid original creator from being notified.
449,Konstantina Skovola,2022-11-16 05:42:12-06,4a30e5969bf310402f468f2792c48370b144dc3f,Fix flaky bgw_db_scheduler_fixed test,"Apply date_trunc to last_successful_finish.  Commit 20cdd9ca3ed0c2d62779c4fc61d278a489b4460a mostly fixed the flakiness, but date_trunc was not applied to the last_successful_finish so we still got some flaky runs."
450,Nikhil Sontakke,2022-11-11 07:44:21-06,c92e29ba3a64d99173afa97ac6d8910dae49760c,Fix DML HA in multi-node,"If a datanode goes down for whatever reason then DML activity to chunks residing on (or targeted to) that DN will start erroring out. We now handle this by marking the target chunk as ""stale"" for this DN by changing the metadata on the access node. This allows us to continue to do DML to replicas of the same chunk data on other DNs in the setup. This obviously will only work for chunks which have ""replication_factor"" > 1. Note that for chunks which do not have undergo any change will continue to carry the appropriate DN related metadata on the AN.  This means that such ""stale"" chunks will become underreplicated and need to be re-balanced by using the copy_chunk functionality by a micro service or some such process.  Fixes #4846"
451,Dmitry Simonenko,2022-11-24 10:33:58-06,26e3be1452e47be35795982626c4f549d45158d1,Test dist caggs with an unavailable data node,Add additional test cases to ensure caggs functionality on distributed hypertable during data node being unavailable.  Fix #4978
452,Dmitry Simonenko,2022-11-24 07:13:17-06,826dcd2721d0f406672da3aca466a130a230e498,Ensure nodes availability using dist restore point,Make sure that a data node list does not have unavailable data nodes when using create_distributed_restore_point() API.  Fix #4979
454,Dmitry Simonenko,2022-11-23 08:54:59-06,5813173e07af7d798e6d29ec10123e361c13ded3,Introduce drop_stale_chunks() function,This function drops chunks on a specified data node if those chunks are not known by the access node.  Call drop_stale_chunks() automatically when data node becomes available again.  Fix #4848
455,Alexander Kuzmenkov,2022-11-22 08:56:11-06,bdae647f0a73a3941541dbf3b2a0cda0b6f9ecfa,Add i386 check results to database,Also add some more gdb commands to give us more context.
456,Alexander Kuzmenkov,2022-11-22 08:54:30-06,26db8666376541f8929a0bfa2cc637092fe1ed99,Fix GITHUB_OUTPUT on Windows,"We have to add it to WSLENV and translate it as a path, so that it properly passes the WSL <-> native process boundary."
457,Konstantina Skovola,2022-11-22 07:23:24-06,40297f1897ab73d5b9b7d960b5ae83338f14e1bf,Fix TRUNCATE on hierarchical caggs,"When truncating a cagg that had another cagg defined on top of it, the nested cagg would not get invalidated accordingly. That was because we were not adding a corresponding entry in the hypertable invalidation log for the materialization hypertable of the base cagg. This commit adds an invalidation entry in the table so that subsequent refreshes see and properly process this invalidation.  Co-authored-by: name <fabriziomello@gmail.com>"
459,Fabrízio de Royes Mello,2022-11-16 11:59:56-06,e84a6e2e6523de90b7b41a9bcf80120b85844894,Remove the refresh step from CAgg migration,We're facing some weird `portal snapshot` issues running the `refresh_continuous_aggregate` procedure called from other procedures.  Fixed it by ignoring the Refresh Continuous Aggregate step from the `cagg_migrate` and warning users to run it manually after the execution.  Fixes #4913
460,Lakshmi Narayanan Sreethar,2022-11-21 03:17:42-06,7bc6e56cb7aa24e4142ddb97b51f58754bb46c58,Fix plan_hashagg test failure in PG15,Updated the expected output of plan_hashagg to reflect changes introduced by postgres/postgres@4b160492.
461,Sven Klemm,2022-11-22 04:37:45-06,639a5018a3bad4499bce0e1215c24270514234d5,Change time of scheduled CI run,Since we now use the date as a part of the cache key to ensure no stale cache entries hiding build failures we need to make sure we have a cache entry present before workflows that depend on cache are run.
462,Konstantina Skovola,2022-11-21 10:43:43-06,48d9733fda44b869d95c3d3158f8423ab87b64ad,Add telemetry for caggs on top of caggs,Commit #4668 introduced hierarchical caggs. This patch adds a field `num_caggs_nested` to the telemetry report to include the number of caggs defined on top of other caggs.
463,Jan Nidzwetzki,2022-11-22 01:53:46-06,fd84bf42a5c691d6e9f3e928f95830b81a85f17a,Use Ensure in get_or_add_baserel_from_cache,"This patch changes an Assert in get_or_add_baserel_from_cache to an Ensure. Therefore, this check is also performed in release builds. This is done to detect metadata corruptions at an early stage."
469,Fabrízio de Royes Mello,2022-10-05 16:45:40-05,3749953e9704e45df8f621607989ada0714ce28d,Hierarchical Continuous Aggregates,"Enable users create Hierarchical Continuous Aggregates (aka Continuous Aggregates on top of another Continuous Aggregates).  With this PR users can create levels of aggregation granularity in Continuous Aggregates making the refresh process even faster.  A problem with this feature can be in upper levels we can end up with the ""average of averages"". But to get the ""real average"" we can rely on ""stats_aggs"" TimescaleDB Toolkit function that calculate and store the partials that can be finalized with other toolkit functions like ""average"" and ""sum"".  Closes #1400"
470,Jan Nidzwetzki,2022-11-02 05:32:48-05,fd11479700a1d91f78b20ba57cfddd43836fb72a,Speed up get_or_add_baserel_from_cache operation,"Commit 9f4dcea30135d1e36d1c452d631fc8b8743b3995 introduces the get_or_add_baserel_from_cache function. It contains a performance regression, since an expensive metadata scan (ts_chunk_get_hypertable_id_by_relid) is performed even when it could be avoided."
471,Jan Nidzwetzki,2022-10-28 09:09:52-05,380464df9bb35784853f186b9e268cd9d50e442b,Perform frozen chunk status check via trigger,"The commit 9f4dcea30135d1e36d1c452d631fc8b8743b3995 introduces frozen chunks. Checking whether a chunk is frozen or not has been done so far in the query planner. If it is not possible to determine which chunks are affected by a query in the planner (e.g., due to a cast in the WHERE condition), all chunks are checked. This leads (1) to an increased planning time and (2) to the situation that a single frozen chunk could reject queries, even if the frozen chunk is not addressed by the query."
472,Lakshmi Narayanan Sreethar,2022-11-17 06:43:41-06,7c32ceb0737b0d959eac0d866447918c4453844a,Fix perl test import in PG15,Removed an invalid import from 007_healthcheck.pl test. Also enabled all the perl tests and a couple of others in PG15.
473,gayyappan,2022-11-14 08:56:24-06,b9ca06d6e3d6a0d8ca924917fddb3e78e6f14a64,Move freeze/unfreeze chunk to tsl,Move code for freeze and unfreeze chunk to tsl directory.
474,Bharathy,2022-11-17 07:14:09-06,bfa641a81c2db82366baf7848c0723ee6eae7f90,INSERT .. SELECT on distributed hypertable fails on PG15,INSERT .. SELECT query containing distributed hypertables generates plan with DataNodeCopy node which is not supported. Issue is in function tsl_create_distributed_insert_path() where we decide if we should generate DataNodeCopy or DataNodeDispatch node based on the kind of query. In PG15 for INSERT .. SELECT query timescaledb planner generates DataNodeCopy as rte->subquery is set to NULL. This is because of a commit in PG15 where rte->subquery is set to NULL as part of a fix.  This patch checks if SELECT subquery has distributed hypertables or not by looking into root->parse->jointree which represents subquery.  Fixes #4983
475,Sachin,2022-11-07 02:50:22-06,1e3200be7db529560d12a0a2323ed962e8614301,USE C function for time_bucket() offset,Instead of using SQL UDF for handling offset parameter added ts_timestamp/tz/date_offset_bucket() which will handle offset
476,Lakshmi Narayanan Sreethar,2022-11-09 14:32:18-06,839e42dd0c1d132a230760681e135706d3b7c971,Use async API to drop database from delete_data_node,PG15 introduced a ProcSignalBarrier mechanism in drop database implementation to force all backends to close the file handles for dropped tables. The backend that is executing the drop database command will emit a new process signal barrier and wait for other backends to accept it. But the backend which is executing the delete_data_node function will not be able to process the above mentioned signal as it will be stuck waiting for the drop database query to return. Thus the two backends end up waiting for each other causing a deadlock.  Fixed it by using the async API to execute the drop database command from delete_data_node instead of the blocking remote_connection_cmdf_ok call.  Fixes #4838
477,Alexander Kuzmenkov,2022-11-16 03:32:26-06,1b65297ff7ed21ae2b338a88cf16d151903178f4,Fix memory leak with INSERT into compressed hypertable,We used to allocate some temporary data in the ExecutorContext.
479,Alexander Kuzmenkov,2022-11-15 08:03:05-06,676d1fb1f1f82f34ae5cdad96022796682215f27,Fix const null clauses in runtime chunk exclusion,"The code we inherited from postgres expects that if we have a const null or false clause, it's going to be the single one, but that's not true for runtime chunk exclusion because we don't try to fold such restrictinfos after evaluating the mutable functions. Fix it to also work for multiple restrictinfos."
480,Mats Kindahl,2022-11-02 16:29:11-05,f3a3da780440ec204a4c979bc663aa298c80c0d3,Take advisory lock for job tuple,"Job ids are locked using an advisory lock rather than a row lock on the jobs table, but this lock is not taken in the job API functions (`alter_job`, `delete_job`, etc.), which appears to cause a race condition resulting in addition of multiple rows with the same job id.  This commit adds an advisory `RowExclusiveLock` on the job id while altering it to match the advisory locks taken while performing other modifications.  Closes #4863"
481,Ante Kresic,2022-11-10 04:56:01-06,51e5f319184050c36599a0517fdf4b2a6a23fd62,Update compress chunk interval on compressed data,Compress chunk interval is set using an ALTER TABLE statement. This change makes it so you can update the compress chunk interval while keeping the rest of the compression settings intact. Updating it will only affect chunks that are compressed and rolled up after the change.
482,Sven Klemm,2022-11-11 15:19:01-06,8b6eb9024f6995e4615098e9c166aabb4c77f2e5,Check for interrupts in gapfill main loop,Add CHECK_FOR_INTERRUPTS() macro to gapfill main loop.
483,Sven Klemm,2022-11-10 12:13:26-06,87756bcff955cb3916c88ef19c3992d5c25183a7,Bump postgres versions used in CI,"Use PG 12.13, 13.9 and 14.6 in our CI"
485,Lakshmi Narayanan Sreethar,2022-11-14 08:00:33-06,33531212b233027d10ba7887ef9754dad14c35b2,Disable dist_move_chunk test in PG15,The dist_move_chunk causes the CI to hang when compiled and run with PG15 as explained in #4972.  Also fixed schema permission issues in data_node and dist_param tests.
486,Bharathy,2022-11-14 18:48:15-06,8afdddc2da6020e56ba9128d8f36d3cb5b7775f6,Deprecate continuous aggregates with old format,"This patch will report a warning when upgrading to new timescaledb extension, if their exists any caggs with partial aggregates only on release builds. Also restrict users from creating cagss with old format on timescaledb with PG15."
487,Mats Kindahl,2022-11-11 03:34:59-06,b085833fdac4d6ffeafef320227e5e8c53f23a1b,Print errors in release builds for jobs,Old assertions checking integrety of metadata for jobs will print error message in release builds instead of continuing executing with bad metadata.
488,Alexander Kuzmenkov,2022-11-14 08:09:54-06,121631c70fdf08c285ac9201d0767ed99e4891c8,Support parameterized data node scans in joins,"This allows us to perform a nested loop join of a small outer local table to an inner distributed hypertable, without downloading the entire hypertable to the access node."
489,Alexander Kuzmenkov,2022-11-14 08:21:10-06,9964ba8ba6440a7df9f7799f880fb144db1858a6,Remove accidental debug output,Was added in # 4890
490,Alexander Kuzmenkov,2022-11-14 06:59:59-06,0d30155b26da1711f7f0b824f1fa7a7302ebf268,Upload test results into a database,This will help us find the flaky tests or the rare failures.
496,Fabrízio de Royes Mello,2022-11-10 06:56:09-06,6ae192631edb3c5a852b0ac2d0441e5b3e2b8a12,Fix CAgg migration with timestamp without timezone,It was a leftover from the original implementation where we didn't add tests for time dimension using `timestamp without timezone`.  Fixed it by dealing with this datatype and added regression tests.  Fixes #4956
497,Alexander Kuzmenkov,2022-11-09 10:06:32-06,252cefb509153fadcb32741a27ec3fa977487049,Upload test results into the database,This will help us find the flaky tests or the rare failures.
498,Erik Nordström,2022-10-14 04:09:53-05,f13214891cf47fed0db6183ce6b56d9273865de3,Add function to alter data nodes,"Add a new function, `alter_data_node()`, which can be used to change the data node's configuration originally set up via `add_data_node()` on the access node.  The new functions introduces a new option ""available"" that allows configuring the availability of the data node. Setting `available=>false` means that the node should no longer be used for reads and writes. Only read ""failover"" is implemented as part of this change, however.  To fail over reads, the alter data node function finds all the chunks for which the unavailable data node is the ""primary"" query target and ""fails over"" to a chunk replica on another data node instead. If some chunks do not have a replica to fail over to, a warning will be raised.  When a data node is available again, the function can be used to switch back to using the data node for queries.  Closes #2104"
499,Sven Klemm,2022-11-05 10:39:06-05,fe6731cead8625ed71c3feb9ed5a17679bffe837,Fix compress_segmentby in isolation tests,compress_segmentby should never be on a column with random() values as that will result in very inefficient compression as the batches will only have 1 tuple each.
500,Jan Nidzwetzki,2022-11-10 07:13:47-06,4f9eef3211166b22424beb51a87dfe7c329b60c7,Print correct variables in downgrade test script,The downgrade script has printed a message in which the same variable is used for the upgrade and the downgrade version. This patch corrects the output and uses the correct variables.
501,Alexander Kuzmenkov,2022-11-09 09:59:15-06,6ad28248f364d2b78ecb9495c37bc13a99969eb0,Change the flaky check to use output files,"This is simpler, accounts for both sql and isolation tests, changes in included files, and versioned tests."
502,Sven Klemm,2022-11-09 02:29:36-06,e4ba2bcf560568ae68f3775c058f0a8d7f7c0501,Remove debian 9 from packages tests.,Debian 9 is EOL since July 2022 so we won't build packages for it anymore and can remove it from CI.
503,Mats Kindahl,2022-11-08 06:59:18-06,1f807153085b37c4c48f955ddbcc575bec10c6d2,Check for trailing whitespace,"As a result of editing, trailing whitespace is often resulting and since some editors automatically remove trailing whitespace this creates diffs with more changed lines than necessary.  Add a check that files do not have trailing whitespace and fail if there are."
504,Sven Klemm,2022-11-01 04:25:46-05,9744b4f3bc9859bed7a434143b46c9ae298999bf,Remove BitmapScan support in DecompressChunk,We don't want to support BitmapScans below DecompressChunk as this adds additional complexity to support and there is little benefit in doing so. This fixes a bug that can happen when we have a parameterized BitmapScan that is parameterized on a compressed column and will lead to an execution failure with an error regarding incorrect attribute types in the expression.
505,Fabrízio de Royes Mello,2022-11-07 14:06:13-06,bfef3173bc89b8470fbc423ceb5a49ee138c82e3,Refactor CAgg migration code to use job API,The current implementation update the jobs table directly and to make it consistent with other parts of the code we changed it to use the `alter_job` API instead to enable and disable the jobs during the migration. This refactoring is related to #4863.
506,Bharathy,2022-11-07 10:12:22-06,2a64450651a62114b2de3811f772eac44232ee9c,Add new tests to gitignore list,"Since new tests specific to PG15 were added, these tests which generated .sql files needs to be added to .gitnore"
507,Bharathy,2022-11-07 08:35:30-06,3a9688cc97bb726d31b5c381ece9e1a5da756cc9,Extra Result node on top of CustomScan on PG15,"On PG15 CustomScan by default is not projection capable, thus wraps this node in Result node. THis change in PG15 causes tests result files which have EXPLAIN output to fail. This patch fixes the plan outputs.  Fixes #4833"
508,Mats Kindahl,2022-11-04 09:01:57-05,b95576550cae05c0db1b520b5f0ae1668312e949,Add printout for multiple jobs with same job_id,"We have a rare condition where a debug build asserts on more than one job with the same job id. Since it is hard to create a reproduction, this commit adds a printout for those conditions and print out all the jobs with that job id in the postgres log.  Part of #4863"
509,Sven Klemm,2022-11-04 14:44:53-05,3059290beaa4e722e77dada386a9dd29d59c51df,Add new chunk state CHUNK_STATUS_COMPRESSED_PARTIAL,A chunk is in this state when it is compressed but also has uncompressed data in the uncompressed chunk. Individual tuples can only ever exist in either area. This is preparation patch to add support for uncompressed staging area for DML operations.
510,Sven Klemm,2022-11-04 14:35:49-05,5b0bff384bd85db1dd727050e1782ac618a4b553,Improve InvalidOid coccinelle check,The initial version of the check did not include a detailed message about the code failure in the CI output and did not check for expressions with operands in wrong order.
512,Bharathy,2022-11-06 21:39:04-06,12745c880668825c8e3b966c43e3d4cdcb4d2900,Fix error: variable not found in subplan target list on PG15,"On PG15 new flag CUSTOMPATH_SUPPORT_PROJECTION is introduced. This flag tells if a planner node is projection capable or not. CustomScan created in TimescaleDB by default is not projection capable, this causes CustomScan node to be wrapped around Result node. Update query on a hypertable has a logic which is based on assumption that ""ModifyTable"" plan nodes lefttree should be CustomScan node. With PG15 this assumption is broken which causes ""ERROR:  variable not found in subplan target list"".  Fixes #4834"
513,Fabrízio de Royes Mello,2022-11-03 13:10:34-05,6c73b61b998293f750ed3392f8afa35b97a63b66,Fix orphan jobs after CAgg migration,When using `override => true` the migration procedure rename the current cagg using the suffix `_old` and rename the new created with suffix `_new` to the original name.  The problem is the `copy polices` step was executed after the `override` step and then we didn't found the new cagg name because it was renamed to the the original name leading the policy orphan (without connection with the materialization hypertable).  Fixed it by reordering the steps executin the `copy policies` before the `override` step. Also made some ajustments to properly copy all `bgw_job` columns even if this catalog table was changed.  Fixes #4885
514,Alexander Kuzmenkov,2022-11-02 02:12:37-05,1847f64a2f766c5ff2936fa39c568fb4f82ae702,Fix sanitizer builds with -Wclobbered,They use GCC 10 which has some other set of false positives.
515,Konstantina Skovola,2022-10-14 02:43:18-05,c54cf3ea56de9e05852ea54f90c55c5a221fa1bb,Add job execution statistics to telemetry,"This patch adds two new fields to the telemetry report, `stats_by_job_type` and `errors_by_sqlerrcode`. Both report results grouped by job type (different types of policies or user defined action). The patch also adds a new field to the `bgw_job_stats` table, `total_duration_errors` to separate the duration of the failed runs from the duration of successful ones."
516,Fabrízio de Royes Mello,2022-11-01 17:07:51-05,f1535660b04251cfb6cdfd4435cfa5185420f1f7,Honor usage of OidIsValid() macro,Postgres source code define the macro `OidIsValid()` to check if the Oid is valid or not (comparing against the `InvalidOid` type). See `src/include/c.h` in Postgres source three.  Changed all direct comparisons against `InvalidOid` for the `OidIsValid` call and add a coccinelle check to make sure the future changes will use it correctly.
517,Fabrízio de Royes Mello,2022-10-28 13:29:10-05,7dd45cf348576f3afec90fe2ee282e97dda0ed26,Fix failure resuming a CAgg migration,"Trying to resume a failed Continuous Aggregate raise an exception that the migration plan already exists, but this is wrong and the expected behaviour should be resume the migration and continue from the last failed step."
518,Alexander Kuzmenkov,2022-11-03 04:41:00-05,08791cad4316b7cb567176fb1ea46f28a6f66fd8,Disable llvm on macos,I accidentally re-enabled it when adding the flaky check.
519,Ante Kresic,2022-09-16 08:45:07-05,2475c1b92ff9e566cb3858924ca153aa14ab4a25,Roll up uncompressed chunks into compressed ones,"This change introduces a new option to the compression procedure which decouples the uncompressed chunk interval from the compressed chunk interval. It does this by allowing multiple uncompressed chunks into one compressed chunk as part of the compression procedure. The main use-case is to allow much smaller uncompressed chunks than compressed ones. This has several advantages: - Reduce the size of btrees on uncompressed data (thus allowing faster inserts because those indexes are memory-resident). - Decrease disk-space usage for uncompressed data. - Reduce number of chunks over historical data.  From a UX point of view, we simple add a compression with clause option `compress_chunk_time_interval`. The user should set that according to their needs for constraint exclusion over historical data. Ideally, it should be a multiple of the uncompressed chunk interval and so we throw a warning if it is not."
521,Bharathy,2022-11-01 09:09:38-05,c06b647680dabac74cd027735ad7176f855c24a0,pg_dump on PG15 does not log messages with log level set to PG_LOG_INFO.,"Version 15 pg_dump program does not log any messages with log level < PG_LOG_WARNING to stdout. Whereas this check is not present in version 14, thus we see corresponding tests fail with missing log information. This patch fixes by supressing those log information, so that the tests pass on all versions of postgresql.  Fixes #4832"
522,Alexander Kuzmenkov,2022-10-27 09:04:27-05,840f144e09640fcf981706eab30c387ce59a1fd1,Enable and fix -Wclobbered,The one in job_stat.c could probably lead to errors.
523,Alexander Kuzmenkov,2022-10-27 06:59:31-05,1cc8c15cad938b432995217bcdaada5030a34b9f,Do not clobber the baserel cache on UDF error,The baserel cache should only be allocated and freed by the top-level query.
524,Sven Klemm,2022-11-01 06:22:59-05,20cdd9ca3ed0c2d62779c4fc61d278a489b4460a,Fix bgw_db_scheduler_fixed flakyness,Depending on date boundaries the number of chunks produced for the cagg hypertable was not constant resulting on flaky tests on certain days.
525,Sven Klemm,2022-11-01 05:11:48-05,3d30f07bf48d5e0e186535ede438d8e4003a7664,Swap lookup order for clang-format,Look for the binary with exact version before looking for the generic name to prevent failure when clang-format is lower then required version but clang-format-14 exists.
526,Alexander Kuzmenkov,2022-10-31 09:23:14-05,d51fefb74405a519c51dd0f10a89d75c816399c4,Add the Flaky Check,It runs new or changed tests multiple times to find flakiness.
527,gayyappan,2022-10-25 09:43:33-05,e08e0a59db5f8cee24350f79971a0271ee4269d6,Add hook for chunk creation,"After data is tiered using OSM, we cannot insert data into the same range. Need a callback that can be invoked by timescaledb to check for range overlaps before creating a new chunk"
528,Alexander Kuzmenkov,2022-10-28 08:06:46-05,5b2d9d5a106d6dc381d16758780893d5bcbc35dd,Enable -Wnewline-eof,"It enforces the newline at end of file, which is required by the C standard."
529,gayyappan,2022-10-27 17:47:18-05,c48b1231a884bebe061624281f30934037885c9b,Allow foreign tables in hypetable modify path,OSM chunks are foreign tables. Modify assert to allow updates/deletes on hypertables with foreign table chunks.
530,Jan Nidzwetzki,2022-10-27 09:26:22-05,dfbf030af7ceec8bf6b41ae02e87414bc72bd318,Remove no-activity label on issue activity,"So far, only the ""need-more-info"" label was removed when there was activity on an issue. This PR ensures that the ""no-activity"" label is also removed."
531,Alexander Kuzmenkov,2022-10-27 10:00:01-05,85f5efdc8f86630039faeaa3523a2ddbe16f1974,Fix -Wsign-compare with PG 15,"It changed the type of Var.varno from Index to int. I'm starting to wonder if it was a good idea to enable this warning, but maybe we can give it the last try."
532,Alexander Kuzmenkov,2022-10-26 08:25:22-05,d8e892a658d9b3a3071764be745465c331f86d52,Save postgres logs on Windows in CI,We don't save them currently.
533,Fabrízio de Royes Mello,2022-10-26 14:44:59-05,8d1e165d7f2cd660fe5ee82d34463a4f9aa420a6,Refactor Continuous Aggregate catalog code,Get rid of `GETSTRUCT` to fill the form data and use `heap_deform_tuple` instead. This is necessary specially when you have variable lenght fields and/or fields that accept NULL values. This refactoring will be specially usefull in a following PR for Nested Continuous Aggregates where we'll add a new metadata to the catalog that can accept NULL values.  Also refactor the rename view and schema code paths improving the readability and maintainability.
534,Alexander Kuzmenkov,2022-10-27 06:17:26-05,9b157d5438049dc2a267dd3a5ed57ef33b8820f4,Don't use docker for clang-format,It's not something we normally do for the developer tools.
535,Alexander Kuzmenkov,2022-10-27 06:08:09-05,313845a88245312fdb9d19f26c29d2e027808a3e,Enable -Wextra,Our code mostly has warnings about comparison with different signedness.
536,Alexander Kuzmenkov,2022-10-21 08:04:41-05,864da20cee764b33b149444d561af4f60653eb64,Build on Ubuntu 22.04,It has newer GCC which should detect more warnings.
537,Fabrízio de Royes Mello,2022-10-26 09:08:24-05,2e7d7ee960d2a0506ff415e1f4f50b74a51df632,Remove useless message from tsl_cagg_try_repair,The PR #3899 introduced a new function named `tsl_cagg_try_repair` to try to fix buggy Continuous Aggregates that lead to segfault on a select query. It added an INFO message when skipping the check for Continuous Aggregate that don't have partials and it's annoying during the upgrade/downgrade the extension specially if you have many Continuous Aggregate.  Remove this message because it's completely useless.
538,Alexander Kuzmenkov,2022-10-21 13:39:38-05,da9af2c05d1be2b487c2d3f873a0de49bdb2740f,Do not cache the classify_relation result,"It depends on the context, not only on the relation id. The same chunk can be expanded both as a child of hypertable and as an independent table."
539,Dmitry Simonenko,2022-10-26 05:00:21-05,498b8af261f7c5e93852448db55ca418e86ae6e2,Block multi-statement DDL command in one query,Ensure that queries involving several distributed DDL commands in one query string are blocked.  Fix #4818
596,Sven Klemm,2022-09-29 01:59:06-05,1d4b9d6977098e74ad10888c2b44e8faf187458b,Fix join on time column of compressed chunk,Do not allow paths that are parameterized on a compressed column to exist when creating paths for a compressed chunk.
540,Sven Klemm,2022-10-23 09:38:44-05,cef8c462dbfa46e68896976e9bc1e7ecd5e46162,Fix postgres version assert in telemetry,The telemetry code that reads the postgres version was not updated when support for older postgres version was dropped so we introduce a new macro PG_MAJOR_MIN which is the oldest pg major version we support.
541,Jan Nidzwetzki,2022-10-24 05:11:15-05,f05545883bc3bd338936446d7007fc53759a0cf8,Add a Coccinelle test for PG 12.3+ ereport syntax,"This PR adds a Coccinelle test for ereport(..) calls that use the PG 12.3+ syntax (postgres/postgres@a86715451653c730d637847b403b0420923956f7). We had some of these calls in the past, which we had to fix afterward ( see #4733, #4809, #4871). This Coccinelle patch detects such calls and reports them in the CI run."
542,Konstantina Skovola,2022-10-19 00:58:35-05,fabb01fdaf611c2deb66ce53e133a0ca3efb3e0c,Add telemetry for fixed schedule jobs,Previous commit #4664 introduced the ability to execute background jobs on a fixed schedule. This commit updates our telemetry data to include the number of jobs scheduled to execute on a fixed schedule vs the number registered to execute on a drifting schedule.
543,Fabrízio de Royes Mello,2022-10-21 13:38:13-05,06f2e57c50fc54b899565fc156aac7ba72b91e03,Fix ereport call in job schedule for PG 12.0,"Since PG 12.3 the `ereport` syntax changed and the commit 54ed0d introduced and `ereport` call that works just using newer PG versions.  Changed the `ereport` call a PG 12.0 compatible syntax.  CI failure: https://github.com/timescale/timescaledb/actions/runs/3292571583/jobs/5428090238  Related PRs: #4733, #4809"
544,Alexander Kuzmenkov,2022-10-21 09:43:26-05,39c9921947eec033081003d4ee4b74604476a9c6,Fix flaky copy_memory_usage tests,"The changes from e555eea lead to flakiness. They are a leftover of earlier version and probably not needed anymore.  The original version is also still flaky on Windows, so use linear regression to tell if the memory usage is increasing.  Verified to still fail on 2.7.x"
545,Alexander Kuzmenkov,2022-10-18 09:04:37-05,25628e037a225857eee2f17f1d2a3a88c9317727,Use non-refcounted tupdesc for multi-insert buffers,"We don't need reference counting there, and it spends a lot of CPU in ResourceOwner.  This slightly improves COPY performance on some data sets."
546,Mats Kindahl,2022-10-20 06:37:26-05,84b2fef6ef166a5bad2b9d8fafcf0590259f8473,Fix GitHub output action,To avoid untrusted logged data to use `set-state` and `set-output` workflow commands without the intention of the workflow author GitHub have introduced a new set of environment files to manage state and output.  This commit changes the existing uses of `set-output` to use the new environment files instead.  See https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/
547,Erik Nordström,2022-09-01 10:26:41-05,4b05402580b9ae8e843acaf48f1559c7b261e032,Add health check function,"A new health check function _timescaledb_internal.health() returns the health and status of the database instance, including any configured data nodes (in case the instance is an access node).  Since the function returns also the health of the data nodes, it tries hard to avoid throwing errors. An error will fail the whole function and therefore not return any node statuses, although some of the nodes might be healthy.  The health check on the data nodes is a recursive (remote) call to the same function on those nodes. Unfortunately, the check will fail with an error if a connection cannot be established to a node (or an error occurs on the connection), which means the whole function call will fail. This will be addressed in a future change by returning the error in the function result instead."
548,Jan Nidzwetzki,2022-10-21 02:24:17-05,23c01c44e5c259cbc1091a1f3492758a27dcaad3,Remove an unused function in the copy code,Since e555eea9dbc05f4c09cf0d7e23b814054a459d19 the function TSCopyMultiInsertInfoIsEmpty is no longer used. This patch removes the unused code from src/copy.c.
549,Jan Nidzwetzki,2022-10-17 04:57:28-05,e555eea9dbc05f4c09cf0d7e23b814054a459d19,Fix performance regressions in the copy code,"In 8375b9aa536a619a5ac2644e0dae3c25880a4ead, a patch was added to handle chunks closes during an ongoing copy operation. However, this patch introduces a performance regression. All MultiInsertBuffers are deleted after they are flushed. In this PR, the performance regression is fixed. The most commonly used MultiInsertBuffers survive flushing.  The 51259b31c4c62b87228b059af0bbf28caa143eb3 commit changes the way the per-tuple context is used. Since this commit, more objects are stored in this context. The size of the context was used to set the tuple size to PG < 14. The extra objects in the context lead to wrong (very large) results and flushes almost after every tuple read.  The cache synchronization introduced in 296601b1d7aba7f23aea3d47c617e2d6df81de3e is reverted. With the current implementation, `MAX_PARTITION_BUFFERS` survive the flash. If `timescaledb.max_open_chunks_per_insert` is lower than `MAX_PARTITION_BUFFERS` , a buffer flush would be performed after each tuple read."
550,Erik Nordström,2022-10-15 09:20:34-05,40a6c4cf87edb3ca261cebcf4b2c6af11fcc6f0e,Fix unused sort in dimension partition lookup,"Dimension partition lookups use binary search to find the partition to place a chunk in. However, in the code, an array of partitions might not be sorted because the sort happened on a copy of the array instead of the main array. This change fixes the issue to ensure the array is sorted and binary search works properly."
551,Alexander Kuzmenkov,2022-10-18 12:12:35-05,f862212c8ca19b1af56c7608a68f22b7dd0c985e,Add clang-tidy warning readability-inconsistent-declaration-parameter-name,Mostly cosmetic stuff. Matched to definition automatically with --fix-notes.
553,Alexander Kuzmenkov,2022-10-18 11:53:38-05,05ba1cf22f0dc9232069b566dd23c3edb2cbaee4,Add clang-tidy warning readability-suspicious-call-argument,"Helps find accidentally swapped arguments, like in the recent epoll_ctl() error."
554,Mats Kindahl,2022-05-20 13:55:39-05,276d3a331dc4e248056424230ad953041866106d,Add macro to assert or error,"For some unexpected conditions, we have a check and an error that is generated. Since this always generate an error, it is more difficult to find the bug if the error is generated rather than an assert fired generating a core dump. Similarly, some asserts can occur in production builds and will lead to strange situations triggering a crash. For those cases we should instead generate an error.  This commit introduces a macro `Ensure` that will result in an assert in debug builds, but an error message in release build. This macro should only be used for conditions that should not occur during normal runtime, but which can happen is odd corner-cases in release builds and therefore warrants an error message.  It also replaces some existing checks with such errors to demonstrate usage."
555,Nikhil Sontakke,2022-10-04 05:25:54-05,f55aaf06dd2bf8d1d9296b19bb5f7eea541f05af,Add hook for ssl options,External components like timescaledb_cloudutils might want to add additional options or do additional ssl related processing. They can do so by implementing a hook and then assigning it to a timescaledb variable to allow timescale to invoke it as appropriate.
574,Markos Fountoulakis,2022-10-06 08:25:34-05,7600896a66e55fd933e81433f1aceecc7ddfc1c6,Fix libpq disconnect for PG15,Make sure the FATAL error message before the data node disconnects is not lost when using PG15.  https://github.com/postgres/postgres/commit/618c1670
556,Bharathy,2022-10-19 07:05:02-05,d218715d5c064c88f0e8d56070d762a95c3b65ab,Fix tests which fail on PG15,"When TimescaleDB 2.9.0 compiled against PG15, many tests fail due to permissions on default public schema not getting propagated to data nodes, although connected user has required GRANTS on access nodes. This patch fixes failing tests by explicitly granting required permissions for connected user or role after data nodes are added."
557,Alexander Kuzmenkov,2022-10-18 11:26:45-05,080011d767a6e61d09dc83b4d58f8be7dc9ecb31,Speed up the dist_copy tests,"In some cases we can use less chunks, less data, and not truncate tables."
558,Fabrízio de Royes Mello,2022-10-18 15:30:00-05,702ac53c0ac4fe339e87130622ddaaa63366f03f,Bump codecov github action version,Leftover from previous commit 8950ab where we bumped some github action versions to run on Node16 instead of Node12 (in deprecation).
559,Konstantina Skovola,2022-08-25 05:58:08-05,54ed0d5c05df990674a6fc9857e1120cdcc9b338,Introduce fixed schedules for background jobs,"Currently, the next start of a scheduled background job is calculated by adding the `schedule_interval` to its finish time. This does not allow scheduling jobs to execute at fixed times, as the next execution is ""shifted"" by the job duration.  This commit introduces the option to execute a job on a fixed schedule instead. Users are expected to provide an initial_start parameter on which subsequent job executions are aligned. The next start is calculated by computing the next time_bucket of the finish time with initial_start origin. An `initial_start` parameter is added to the compression, retention, reorder and continuous aggregate `add_policy` signatures. By passing that upon policy creation users indicate the policy will execute on a fixed schedule, or drifting schedule if `initial_start` is not provided. To allow users to pick a drifting schedule when registering a UDA, an additional parameter `fixed_schedule` is added to `add_job` to allow users to specify the old behavior by setting it to false.  Additionally, an optional TEXT parameter, `timezone`, is added to both add_job and add_policy signatures, to address the 1-hour shift in execution time caused by DST switches. As internally the next start of a fixed schedule job is calculated using time_bucket, the timezone parameter allows using timezone-aware buckets to calculate the next start."
560,Fabrízio de Royes Mello,2022-10-17 13:22:30-05,8950abe0ee071549fbcd2d52ce544532ef3e7844,Bump github action versions,All github actions that run on Node12 are deprecated so bumped github action versions to run on Node16.  https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/
561,Fabrízio de Royes Mello,2022-10-13 18:05:04-05,043bd55c0bf7ef372da6ad3e7874be030b10781c,Miss segmentby compression option in CAGGs,"Timescale 2.7 released a new version of Continuous Aggregate (#4269) that store the final aggregation state instead of the byte array of the partial aggregate state, offering multiple opportunities of optimizations as well a more compact form.  This new version also removes the unecessary `chunk_id` column from the materialization hypertable and consequently the re-aggregation in the user view. It means the user view that query the materialization hypertable don't have a GROUP BY clause anymore that was problematic for query performance.  Before 2.7 when users turn compression ON we infer compression options `segmentby` and `orderby` based on the GROUP BY clause and time bucket respectively. With the new version without a GROUP BY clause in the user view the inferetion for the 'segmentby' compression option stopped to work.  Fixed it by changing the code to the compression on the new version of Continuous Aggregate (aka finals form) behave the same as the old version.  Fix #4816"
562,Alexander Kuzmenkov,2022-10-17 11:41:20-05,bde337e92d32e7d20f5c5e75fb0ccbc3e50e8262,Fix the flaky pg_dump test,It was frequently failing on Windows. Sort by what is actually printed.
563,Bharathy,2022-10-17 10:23:34-05,0e32656b54ca2221ea1a2da6b941e8055f13c7da,Support for PG15.,"As part of this patch, added and fixed some of the regress checks which fail on PG15."
564,Markos Fountoulakis,2022-10-17 08:06:15-05,d5c25e8914513694bb20dad22146c39c16d908a7,Adjust partition pruning to support PG15,PostgreSQL 15 introduced a Bitmapset for tracking non-pruned partitions for performance purposes. Adjust our code for expanding hypertable chunks to support this.  https://github.com/postgres/postgres/commit/475dbd0b718
565,Alexander Kuzmenkov,2022-10-13 12:25:11-05,066bcbed6d18a8c8a29a96b5ac607d9939ddc860,Rename row-by-row fetcher to COPY fetcher,"This name better reflects its characteristics, and I'm thinking about resurrecting the old row-by-row fetcher later, because it can be useful for parameterized queries."
566,Bharathy,2022-10-11 22:50:09-05,38878bee1671a0288c562e8a4e0934bb4b06143a,Fix segementation fault during INSERT into compressed hypertable.,"INSERT into compressed hypertable with number of open chunks greater than ts_guc_max_open_chunks_per_insert causes segementation fault. New row which needs to be inserted into compressed chunk has to be compressed. Memory required as part of compressing a row is allocated from RowCompressor::per_row_ctx memory context. Once row is compressed, ExecInsert() is called, where memory from same context is used to allocate and free it instead of using ""Executor State"". This causes a corruption in memory.  Fixes: #4778"
567,Sven Klemm,2022-10-10 17:05:10-05,8f5698f49d11d41dfe7b97c63e98b543e6d5de7a,Show information about OOM killer in CI,Include OOM kill event logs into error printout. Previously these would only be visible by inspecting the postgres log and looking for killed by signal 9.
568,Markos Fountoulakis,2022-10-11 05:43:02-05,e33bd89727cd058d09d353c2595965da613b97ae,Adjust TAP tests permissions,"Starting with PG15, default permissions on the public schema is restricted for any non-superuser non-owner. Adjust TAP tests so as to not fail with ""permission denied for schema public"".  https://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=b073c3cc"
569,Sven Klemm,2022-10-10 16:35:23-05,cc7ea8efd10ba5ade6cbf0e9af988ade06d24011,Fix ereport call in dist_copy for PG 12.0,ereport in PG 12.0 requires extra parenthesis around the auxiliary function calls.
571,Alexander Kuzmenkov,2022-10-10 06:20:44-05,fbe4d3c1df1d82dca4e21bd09292ddaa7d3368d8,Fix a warning with clang 14,Mark the variable as used for asserts only.
572,Alexander Kuzmenkov,2022-10-04 04:14:37-05,7758f5959c8ed64499ab0e6bb66c30464b11dd81,Update .clang-format for version 14,The only configuration we're missing is the newline for braces after case labels. The rest of the differences looks like bugs/omissions of the version 8 that we use now.  Require clang-format-14 in cmake and use it in the CI check. We can't support versions earlier than 14 because they have some formatting differences that can't be configured.
573,Alexander Kuzmenkov,2022-10-10 06:36:04-05,30596c0c478a168344a550b35ca480c5ad5a4a9f,Batch rows on access node for distributed COPY,"Group the incoming rows into batches on access node before COPYing to data nodes. This gives 2x-5x speedup on various COPY queries to distributed hypertables.  Also fix the text format passthrough, and prefer text transfer format for text input to be able to use this passthrough. It saves a lot of CPU on the access node."
576,Sven Klemm,2022-10-08 15:39:49-05,efbd8a94b2bcf54e0772dc533949555bd98eaeb7,Increase timeout to wait for cluster start,By default pg_isready only waits for 3 seconds before giving up which is occasionally not enough in the windows tests. This patch bumps the timeout up to 30 seconds which should be plenty to have the cluster start up under all circumstances.
577,Fabrízio de Royes Mello,2022-09-22 16:29:36-05,e0bbd4042acf1bdcfdcc4cac54c3c3e5bcbdd8e4,Fix missing upgrade/downgrade tests DDL validation,Recently we fixed a DDL error (#4739) after upgrading to 2.8.0 version that surprisly the CI upgrade/dowgrade tests didn't complained during the development of the feature (#4552).  Fixed it by adding an specific query in the `post.catalog.sql` script to make sure we'll check all the constraints of our internal tables and catalog.
578,Jan Nidzwetzki,2022-10-05 07:49:12-05,2f739bb3286bca4db6fda9e71ba6e74c7f22ecff,Post-release fixes for 2.8.1,Bumping the previous version and adding tests for 2.8.1.
580,Sven Klemm,2022-10-05 16:02:15-05,45a8c0b5cf60ba8dbbb1c0236a51b6afa96517be,Improve formatting when printing coredump information,Add a newline between query and stacktrace when printing coredump information to make copying the query easier.
582,Sven Klemm,2022-10-04 07:14:31-05,d2f0c4ed202fbf0b6cd3161197ad032fed70ace3,Fix update script handling of bgw_job_stat,"Update scripts should not use ADD/DROP/RENAME and always rebuild catalog tables to ensure the objects are identical between new install, upgrade and downgrade."
583,Fabrízio de Royes Mello,2022-10-04 12:20:23-05,a76f76f4ee6ee1d5437fe31cbd551e0d6f51393f,Improve size utils functions and views performance,Changed queries to use LATERAL join on size functions and views instead of CTEs and it eliminate a lot of unnecessary projections and give a chance for the planner to push-down predicates.  Closes #4775
584,Sven Klemm,2022-10-01 14:19:28-05,8cda0e17ec89452f1d536f8b1d66f939c891ea4e,Extend the now() optimization to also apply to CURRENT_TIMESTAMP,The optimization that constifies certain now() expressions before hypertable expansion did not apply to CURRENT_TIMESTAMP even though it is functionally similar to now(). This patch extends the optimization to CURRENT_TIMESTAMP.
585,Jan Nidzwetzki,2022-09-29 07:50:15-05,12b7b9f665865fde19c0a6460006147a3e545095,Release 2.8.1,"This release is a patch release. We recommend that you upgrade at the next available opportunity.  **Bugfixes** * #4454 Keep locks after reading job status * #4658 Fix error when querying a compressed hypertable with compress_segmentby on an enum column * #4671 Fix a possible error while flushing the COPY data * #4675 Fix bad TupleTableSlot drop * #4676 Fix a deadlock when decompressing chunks and performing SELECTs * #4685 Fix chunk exclusion for space partitions in SELECT FOR UPDATE queries * #4694 Change parameter names of cagg_migrate procedure * #4698 Do not use row-by-row fetcher for parameterized plans * #4711 Remove support for procedures as custom checks * #4712 Fix assertion failure in constify_now * #4713 Fix Continuous Aggregate migration policies * #4720 Fix chunk exclusion for prepared statements and dst changes * #4726 Fix gapfill function signature * #4737 Fix join on time column of compressed chunk * #4738 Fix error when waiting for remote COPY to finish * #4739 Fix continuous aggregate migrate check constraint * #4760 Fix segfault when INNER JOINing hypertables * #4767 Fix permission issues on index creation for CAggs  **Thanks** * @boxhock and @cocowalla for reporting a segfault when JOINing hypertables * @carobme for reporting constraint error during continuous aggregate migration * @choisnetm, @dustinsorensen, @jayadevanm and @joeyberkovitz for reporting a problem with JOINs on compressed hypertables * @daniel-k for reporting a background worker crash * @justinpryzby for reporting an error when compressing very wide tables * @maxtwardowski for reporting problems with chunk exclusion and space partitions * @yuezhihan for reporting GROUP BY error when having compress_segmentby on an enum column"
586,Alexander Kuzmenkov,2022-10-03 11:36:20-05,b259191dfe27bc13cb33dbc2bd327cf825ded400,Add COSTS OFF to make test more stable,For some reason the cost estimates are different on my machine.
587,Konstantina Skovola,2022-10-05 02:01:32-05,8a5e59b0978209d5913cdffcb6c5f69487ac1273,Fix flaky bgw_custom test,"The test was flaky because the scheduler was launching the scheduled jobs, therefore their next_start field could differ depending on whether they had finished executing. Fixed by not selecting the next_start field in alter_job calls, when it is not of interest.  Fixes #4719"
588,Jan Nidzwetzki,2022-10-04 09:57:04-05,33e4e554d4a18284bb2b1f79398aa9bb5ff0cc88,Ensure that internal users don't trigger workflows,"This patch adds a membership check to the 'Waiting for Engineering' workflow. The check ensures that the workflow is only triggered by external users. Without this check, when someone from the database engineering team responded to an issue, the ""need-more-info"" flag was removed and the issue was erroneously moved to the ""waiting for engineering"" column."
589,Bharathy,2022-10-04 09:50:23-05,f1c6fd97a3ca38aecae518efc2b2cda649dc745d,Continue compressing other chunks after an error,"When a compression_policy is executed by a background worker, the policy should continue to execute even if compressing or decompressing one of the chunks fails.  Fixes: #4610"
590,Dmitry Simonenko,2022-10-04 02:06:06-05,ea5038f26387c47b6dba538f521f99cea3430870,Add connection cache invalidation ignore logic,"Calling `ts_dist_cmd_invoke_on_data_nodes_using_search_path()` function without an active transaction allows connection invalidation event happen between applying `search_path` and the actual command execution, which leads to an error.  This change introduces a way to ignore connection cache invalidations using `remote_connection_cache_invalidation_ignore()` function.  This work is based on @nikkhils original fix and the problem research.  Fix #4022"
593,Sven Klemm,2022-10-01 08:06:55-05,7a6ce372e7727c43e0e077b31eb51ae75f6457f8,Adjust pgspot CI check,Change the pgspot check to use downgrade_to_version instead of update_from_version. We use downgrade_to_version instead of update_from_version because when the release PR for a new version has been merged to main but the version is not tagged yet update_from will not exist yet. In all other situations update_from_version and downgrade_to_version should point to the same version.
594,Rafia Sabih,2022-09-28 08:33:26-05,f7c769c684d1cafe0b73c405b109f035fde57c61,Allow manual index creation in CAggs,"The materialised hypertable resides in the _timescaledb.internal schema which resulted in permission error at the time of manual index creation by non super user. To solve this, it now switches to timescaledb user before index creation of CAgg.  Fixes #4735"
595,Konstantina Skovola,2022-09-06 08:25:09-05,9bd772de250308946b0d7ec6bb71f47ec1ee583a,Add interface for troubleshooting job failures,"This commit gives more visibility into job failures by making the information regarding a job runtime error available in an extension table (`job_errors`) that users can directly query. This commit also adds an infromational view on top of the table for convenience. To prevent the `job_errors` table from growing too large, a retention job is also set up with a default retention interval of 1 month. The retention job is registered with a custom check function that requires that a valid ""drop_after"" interval be provided in the config field of the job."
599,Ante Kresic,2022-09-26 07:53:01-05,cc110a33a2e785e8619f82a72ec48745c75208eb,Move ANALYZE after heap scan during compression,"Depending on the statistics target, running ANALYZE on a chunk before compression can cause a lot of random IO operations for chunks that are bigger than the number of pages ANALYZE needs to read. By moving that operation after the heap is loaded into memory for sorting, we increase the chance of hitting cache and reducing disk operations necessary to execute compression jobs."
600,Ante Kresic,2022-09-26 07:35:23-05,9c819882f317f5d3d53e0bd040c6e7eac2c4bee2,Increase memory usage for compression jobs,"When compressing larger chunks, compression sort tends to use temporary files since memory limits (`work_mem`) are usually pretty small to fit all the data into memory. On the other hand, using `maintenance_work_mem` makes more sense since its generally safer to use a larger value without impacting general resource usage."
601,Mats Kindahl,2022-06-17 06:10:46-05,c0e193dd810d11e66d75a987b7b1291e6fe7a9f9,Keep locks after reading job status,"When reading the job status table `bgw_job_stat` and after that updating it, locks where released after the read, allowing a competing session to update the job status and trigger a concurrent update error either in a session doing the update or in the scheduler. Since the scheduler does not recover after aborting with an error, this caused the background worker subsystem to stop and not start new jobs.  This commit fixes this by upgrading `RowExclusiveLock` to `ShareRowExclusiveLock` to ensure that not two sessions tries to update the row at the same time, remove an initial speculative lock that are taken when a job status row can be added, and also keeps the lock until the end of the transaction to prevent other sessions to update. Since these updating transactions are short, it should not cause other threads to block long.  Fixes #4293"
602,Bharathy,2022-09-27 09:03:15-05,f6dd55a19161d7d64d224069adbe0ab48c10ea29,Hypertable FK reference to partitioned table,"Consider a hypertable which has a foreign key constraint on a referenced table, which is a parititioned table. In such case, foreign key constraint is duplicated for each of the partitioned table. When we insert into a hypertable, we end up checking the foreign key constraint multiple times, which obviously leads to foreign key constraint violation. Instead we only check foreign key constraint of the parent table of the partitioned table.  Fixes #4684"
603,Mats Kindahl,2022-09-26 09:05:46-05,244b3e637c2a290a0dfc3262339ce1b190f63c08,Move perltidyrc to root,"To allow perltidy to be used from editors and IDEs, the perltidyrc file is moved from `scripts/perltidyrc` to `.perltidyrc`. This will allow editors and IDEs to quickly find the style file in the root of the repository, where `perltidy` normally searches for the style file.  The workflow is then updated to use the new location, and the two options `--backup-file-extension` and `--backup-and-modify-in-place` are moved to the workflow file to allow editors and IDEs to use whatever method they find useful to process the file."
604,Alexander Kuzmenkov,2022-09-21 08:10:25-05,6011c1446e9ddcd7f1c3ac22a3580be5523ea36b,Fix error when waiting for remote COPY to finish,"Pass proper flags to WaitLatchOrSocket, and fix the swapped up arguments."
605,Markos Fountoulakis,2022-09-20 07:15:47-05,9d0d159ac10ee7784754437ed0fa26f01de0cccf,Port perl tests to support PG15,Port the timescaledb perl tests and the corresponding infrastructure to support the new perl namespace introduced in PG15.  https://github.com/postgres/postgres/commit/b3b4d8e6
606,Fabrízio de Royes Mello,2022-09-15 14:37:06-05,893faf8a6b096f98a01435d36cf88f385a827b4c,Fix Continuous Aggregate migration policies,After migrate a Continuous Aggregate from the old format to the new using `cagg_migrate` procedure we end up with the following problems: * Refresh policy is not copied from the OLD to the NEW cagg; * Compression setting is not copied from the OLD to the NEW cagg.  Fixed it by properly copying the refresh policy and setting the `timescaledb.compress=true` flag to the new CAGG.  Fix #4710
607,Sven Klemm,2022-09-16 16:12:47-05,2529ae3f68504b6375b159f36c9c51a7d708d6e7,Fix chunk exclusion for prepared statements and dst changes,The constify code constifying TIMESTAMPTZ expressions when doing chunk exclusion did not account for daylight saving time switches leading to different calculation outcomes when timezone changes. This patch adds a 4 hour safety buffer to any such calculations.
608,Fabrízio de Royes Mello,2022-09-21 08:15:32-05,217f514657bcf008a9973b9fbffb6cd414b5d98d,Fix continuous aggregate migrate check constraint,Instances upgraded to 2.8.0 will end up with a wrong check constraint in catalog table `continuous_aggregate_migrate_plan_step`.  Fixed it by removing and adding the constraint with the correct checks.  Fix #4727
609,Jan Nidzwetzki,2022-09-22 06:29:52-05,4cb38350c67a2e60c8725934b402b53c812c22ee,Change project management GH action,"In af8e3c6b12035dd45bc7bf61e9420489cfa56eaa, an GitHub action to assign issues to a project column has been introduced. However, this action does not work with organization-wide projects. This patch replaces the used project management action with a more recent one."
610,Jan Nidzwetzki,2022-09-06 09:24:20-05,de30d190e47c2874d27f3a5406efc5654fb1c3fc,Fix a deadlock in chunk decompression and SELECTs,"This patch fixes a deadlock between chunk decompression and SELECT queries executed in parallel. The change in a608d7db614c930213dee8d6a5e9d26a0259da61 requests an AccessExclusiveLock for the decompressed chunk instead of the compressed chunk, resulting in deadlocks.  In addition, an isolation test has been added to test that SELECT queries on a chunk that is currently decompressed can be executed.  Fixes #4605"
611,Jan Nidzwetzki,2022-09-21 03:29:22-05,5600fc06d65eb366cb304b8723eb1e953501c0fd,Updated the text of the stalebot,This PR improves the text of the stalebot and makes it clearer who is addressed by the message.
612,Sven Klemm,2022-09-19 10:57:32-05,7508d6663967816633beeeba55696d8c33e106c0,Fix gapfill function signature,Very recent compilers will warn about function pointers with empty argument list. While currently in C func() means a function with an unspecified argument list the next version of the C standard will change this to mean func(void).
613,Jan Nidzwetzki,2022-09-20 06:53:07-05,db66a194b28aca11850aef5c50f837fa47f21d43,Fix an ereport syntax error when using PG 12.0,"Since PG 12.3, the syntax of the ereport() function has changed slightly. The change 97a603fe5c5ba25aa6e4e596fd7da7a401051d7b introduces an ereport() call that only works using newer PG versions. This PR changes the ereport() call to a PG 12.0 compatible syntax."
614,Jan Nidzwetzki,2022-09-19 07:01:39-05,b451f0c521ee5e8c3f06c633cc455f0b4b111bd8,Update the name of the automation token,We now have an organization-wide token for automation. This patch changes the name in existing Github actions accordingly.
615,Jan Nidzwetzki,2022-09-19 02:34:56-05,a4d9c9fd6995f00b24edacde1eb6248aa35d9b08,Fix CI coredump information print,"The patch #4714 introduces the extraction of the running query from a coredump by calling gdb with two commands. These commands are separated by a newline character. This character may not always be correctly converted to a new line before being processed by gdb, resulting in the following error:  Invalid character '\' in expression.  This PR ensures the proper handling of the newline character."
616,Mats Kindahl,2022-09-14 07:02:15-05,af8e3c6b12035dd45bc7bf61e9420489cfa56eaa,Update procedural.yml,Fix the 'Waiting for Engineering' job. Issues that contain the label 'need-more-info' that receive an issue_comment are automatically moved to the 'Waiting for Engineering' column on the bug board and the 'need-more-info' is removed.
617,Konstantina Skovola,2022-09-15 07:57:28-05,97a603fe5c5ba25aa6e4e596fd7da7a401051d7b,Remove support for procedures as custom checks,Procedures doing their own transaction handling could lead to errors or even crashes.  Fixes #4703
618,Markos Fountoulakis,2022-09-16 04:33:21-05,042735f1f5d9b253e479075866d8b9cc24eaaa60,Fix vacuum_set_xid_limits_compat() macro,Fix an error in the PG15 portion of the vacuum_set_xid_limits_compat() preprocessor macro.
619,Bharathy,2022-09-16 21:09:04-05,d00a55772cd1762e295673a3d8f5ee0b58f9b964,error compressing wide table,"Consider a compressed hypertable has many columns (like more than 600 columns). In call to compress_chunk(), the compressed tuple size exceeds, 8K which causes error as ""row is too big: size 10856, maximum size 8160.""  This patch estimates the tuple size of compressed hypertable and reports a warning when compression is enabled on hypertable. Thus user gets aware of this warning before calling compress_chunk().  Fixes #4398"
620,Sven Klemm,2022-09-15 13:54:05-05,ffd9dfb7ebd05e2bc8d619ac87b4d5937dae0f23,Fix assertion failure in constify_now,The code added to support VIEWs did not account for the fact that varno could be from a different nesting level and therefore not be present in the current range table.
621,Sven Klemm,2022-09-16 06:02:29-05,85d0e16a982ab3110b39b0c713faa421dd82fd81,Fix flaky pg_dump test,Use DROP DATABASE WITH(FORCE) to drop the database in pg_dump test since occasionally there would still be connections to the database leading to test failures. Unfortunately PG12 does not support that syntax so we have to drop without that option on PG12.
622,Sven Klemm,2022-09-16 04:24:35-05,1cb3edddc1cc6057cf5cce74fbb24cfd4db1e215,Fix bgw_db_scheduler regresscheck configuration,Recent refactoring changed bgw_db_scheduler to also be run on non-Debug builds. Adjust the configuration so it is only included for debug builds.
623,Sven Klemm,2022-09-15 15:18:22-05,3e1ce8c34aae49b5e6dc5188a154f1986e56fe13,Include running query in CI coredump information,Pretty print the active query during a coredump when printing the stacktrace for a coredump.
624,Nikhil Sontakke,2022-09-05 07:47:14-05,7aadf1332fb21e8b61ff6a996e7c1d08c913cee9,Fix some clang compile warnings,Local compilation of timescaledb fails on MaCOSx with clang version 11.0.0. Fixes for a couple of warnings.
625,Alexander Kuzmenkov,2022-09-15 12:13:15-05,fee27484cec7a01c2a98c16133e364f089689181,Do not use row-by-row fetcher for parameterized plans,"We have to prepare the data node statement in this case, and COPY queries don't work with prepared statements."
626,Sven Klemm,2022-09-12 01:50:04-05,424f6f7648e88907b9373ef44ec851787db21080,Remove database port from test output,Don't include the used database ports into test output as this will lead to failing tests when running against a local instance or against a preconfigured cloud instance.
627,Sven Klemm,2022-09-13 05:38:21-05,1642750e47aaa953015f050f9f850d3f2b41b550,Remove multiple PG configurations from regresscheck-t,Using multiple different configurations in a single target will not work when running against a local instance or when running against a preconfigured cloud instance. With recent adjustments to the test cleanup this should not be needed anymore and if we really need different configuration we should make it a separate target to make it compatible with instances configured outside of pg_regress.
628,Sven Klemm,2022-09-13 13:42:23-05,cea1a21c01d76f0e421fcfd86e2ae116f92e1943,Improve SKIPS handling in regression tests,This patch changes the regression test schedule generation to only completely rebuild the schedule when TESTS is specified. In all other cases the existing schedule is amended. This means any parallelity present in the cmake generated schedule will be kept.
629,Sven Klemm,2022-09-13 16:05:30-05,063bfcfef912ad74ec86d8e9d0c0f15fff73b87d,Fix pgspot update script check,Previously the pgspot call used in CI did not properly check the update script. Unsafe function creations for functions with changed signatures could go undetected in the update script.
630,Konstantina Skovola,2022-09-12 06:44:38-05,3b3681858d0a29676098212acabb37ea4549e82a,Remove test case causing bgw_custom crash,"Patch #4425 introduced regression test failures, namely, a crash in function `ts_bgw_job_update_by_id`. The failures are due to the COMMIT statement in the custom check procedure. This patch removes that particular test case from bgw_custom."
631,Sven Klemm,2022-09-13 15:24:32-05,0144c75b3f1b932bac4cf28b432c7fb179307b6f,Remove flaky test from bgw_db_scheduler,"The out of background worker test in bgw_db_scheduler is flaky and fails very often, especially in the 32bit environment and on windows. This patch removes that specific test from bgw_db_scheduler. If we want to test this specific part of the scheduler this should be better rewritten in an isolation test."
632,Mats Kindahl,2022-09-12 06:04:51-05,88e2f24ea3e09f4c39256d27b558d997d0367738,Add automation for waiting on author,If an issue is labeled with `needs-more-info` it is automatically considered as waiting for author and we should move it to the column for that.
633,Fabrízio de Royes Mello,2022-09-12 14:26:55-05,02ad4f6b763a52a206fab9e231ab023d62360e60,Change parameter names of cagg_migrate procedure,"Removed the underline character prefix '_' from the parameter names of the procedure `cagg_migrate`. The new signature is:  cagg_migrate(     IN cagg regclass,     IN override boolean DEFAULT false,     IN drop_old boolean DEFAULT false )"
634,Fabrízio de Royes Mello,2022-09-09 12:26:01-05,6ecefff93ec2991505af56442b751c6c2925e303,Add CAGG migration permission tests,Timescale 2.8 released a migration path from the old format of Continuous Aggregate to the new format (#4552).  Unfortunately it lacks of proper tests when a non-superuser execute the migration. For a non-superuser execute the migration properly it requires SELECT/INSERT/UPDATE permissions in some catalog objects: * _timescaledb_catalog.continuous_agg_migrate_plan * _timescaledb_catalog.continuous_agg_migrate_plan_step * _timescaledb_catalog.continuous_agg_migrate_plan_step_step_id_seq  Improved the regression tests to cover the lack of permissions in the catalog objects for non-superusers.
635,Sven Klemm,2022-09-05 16:29:28-05,d2baef3ef361b4fa0d54fcbfd8e0b49cb4fa1975,Fix planner chunk exclusion for VIEWs,Allow planner chunk exclusion in subqueries. When we decicde on whether a query may benefit from constifying now and encounter a subquery peek into the subquery and check if the constraint references a hypertable partitioning column.  Fixes #4524
656,Sven Klemm,2022-08-31 10:28:18-05,1d4f90b1d31e7603f00d78d03e1992195e74865b,Pass PlannerInfo by reference to can_exclude_constraints_using_clauses,PlannerInfo is 608 bytes and should be passed by reference and not by value. Found by coverity.
657,Sven Klemm,2022-08-31 10:08:27-05,21673f2df2703bc7d7d8012c31ea593940c3a4fd,Fix copy-paste error in ts_bgw_job_insert_relation,Found by coverity.
658,Sven Klemm,2022-08-25 11:32:34-05,f432d7f93169a5c806f5c9da628513c69ca8eed2,Release 2.8.0,"This release adds major new features since the 2.7.2 release. We deem it moderate priority for upgrading.  This release includes these noteworthy features:  * time_bucket now supports bucketing by month, year and timezone * Improve performance of bulk SELECT and COPY for distributed hypertables * 1 step CAgg policy management * Migrate Continuous Aggregates to the new format  **Features** * #4188 Use COPY protocol in row-by-row fetcher * #4307 Mark partialize_agg as parallel safe * #4380 Enable chunk exclusion for space dimensions in UPDATE/DELETE * #4384 Add schedule_interval to policies * #4390 Faster lookup of chunks by point * #4393 Support intervals with day component when constifying now() * #4397 Support intervals with month component when constifying now() * #4405 Support ON CONFLICT ON CONSTRAINT for hypertables * #4412 Add telemetry about replication * #4415 Drop remote data when detaching data node * #4416 Handle TRUNCATE TABLE on chunks * #4425 Add parameter check_config to alter_job * #4430 Create index on Continuous Aggregates * #4439 Allow ORDER BY on continuous aggregates * #4443 Add stateful partition mappings * #4484 Use non-blocking data node connections for COPY * #4495 Support add_dimension() with existing data * #4502 Add chunks to baserel cache on chunk exclusion * #4545 Add hypertable distributed argument and defaults * #4552 Migrate Continuous Aggregates to the new format * #4556 Add runtime exclusion for hypertables * #4561 Change get_git_commit to return full commit hash * #4563 1 step CAgg policy management * #4641 Allow bucketing by month, year, century in time_bucket and time_bucket_gapfill * #4642 Add timezone support to time_bucket  **Bugfixes** * #4359 Create composite index on segmentby columns * #4374 Remove constified now() constraints from plan * #4416 Handle TRUNCATE TABLE on chunks * #4478 Synchronize chunk cache sizes * #4486 Adding boolean column with default value doesn't work on compressed table * #4512 Fix unaligned pointer access * #4519 Throw better error message on incompatible row fetcher settings * #4549 Fix dump_meta_data for windows * #4553 Fix timescaledb_post_restore GUC handling * #4573 Load TSL library on compressed_data_out call * #4575 Fix use of `get_partition_hash` and `get_partition_for_key` inside an IMMUTABLE function * #4577 Fix segfaults in compression code with corrupt data * #4580 Handle default privileges on CAggs properly * #4582 Fix assertion in GRANT .. ON ALL TABLES IN SCHEMA * #4583 Fix partitioning functions * #4589 Fix rename for distributed hypertable * #4601 Reset compression sequence when group resets * #4611 Fix a potential OOM when loading large data sets into a hypertable * #4624 Fix heap buffer overflow * #4627 Fix telemetry initialization * #4631 Ensure TSL library is loaded on database upgrades * #4646 Fix time_bucket_ng origin handling * #4647 Fix the error ""SubPlan found with no parent plan"" that occurred if using joins in RETURNING clause.  **Thanks** * @AlmiS for reporting error on `get_partition_hash` executed inside an IMMUTABLE function * @Creatation for reporting an issue with renaming hypertables * @janko for reporting an issue when adding bool column with default value to compressed hypertable * @jayadevanm for reporting error of TRUNCATE TABLE on compressed chunk * @michaelkitson for reporting permission errors using default privileges on Continuous Aggregates * @mwahlhuetter for reporting error in joins in RETURNING clause * @ninjaltd and @mrksngl for reporting a potential OOM when loading large data sets into a hypertable * @PBudmark for reporting an issue with dump_meta_data.sql on Windows * @ssmoss for reporting an issue with time_bucket_ng origin handling"
636,Bharathy,2022-09-12 06:55:58-05,b869f91e256df56461021dfe836c69a4c45836e3,Show warnings during create_hypertable().,"The schema of base table on which hypertables are created, should define columns with proper data types. As per postgres best practices Wiki (https://wiki.postgresql.org/wiki/Don't_Do_This), one should not define columns with CHAR, VARCHAR, VARCHAR(N), instead use TEXT data type. Similarly instead of using timestamp, one should use timestamptz. This patch reports a WARNING to end user when creating hypertables, if underlying parent table, has columns of above mentioned data types.  Fixes #4335"
637,Alexander Kuzmenkov,2022-09-09 10:45:06-05,4e47302c2c2a923c8ed30e34d3486a942767059b,Speed up chunk search by restriction clauses,"We don't have to look up the dimension slices for dimensions for which we don't have restrictions.  Also sort chunks by ids before looking up the metadata, because this gives more favorable table access patterns (closer to sequential).  This fixes a planning time regression introduced in 2.7."
638,Sven Klemm,2022-09-10 03:20:08-05,a26a5974dce08e1146bc7aa35eac7e1f6f1716e2,Improve space constraint exclusion datatype handling,This patch adjusts the operator logic for valid space dimension constraints to no longer look for an exact match on both sides of the operator but instead allow mismatched datatypes.  Previously a constraint like `col = value` would require `col` and `value` to have matching datatype with this change `col` and `value` can be different datatype as long as they have equality operator in btree family.  Mismatching datatype can happen commonly when using int8 columns and comparing them with integer literals. Integer literals default to int4 so the datatypes would not match unless special care has been taken in writing the constraints and therefore the optimization would never apply in those cases.
639,Sven Klemm,2022-09-09 06:57:16-05,f27e62734141ed7fca30401f22a5ad1ead86b9e2,Fix chunk exclusion for space partitions in SELECT FOR UPDATE queries,Since we do not use our own hypertable expansion for SELECT FOR UPDATE queries we need to make sure to add the extra information necessary to get hashed space partitions with the native postgres inheritance expansion working.
640,Sven Klemm,2022-09-08 11:10:10-05,6de979518d286a83783f68172fbf1623f8873e26,Fix compression_chunk_size primary key,"The primary key for compression_chunk_size was defined as chunk_id, compressed_chunk_id but other places assumed chunk_id is actually unique and would error when it was not. Since it makes no sense to have multiple entries per chunk since that reference would be to a no longer existing chunk the primary key is changed to chunk_id only with this patch."
641,Alexander Kuzmenkov,2022-09-08 04:18:06-05,8e4dcddad62ac9f70933a8d61ab02c9c687e0c4e,Make the copy_memory_usage test less flaky,Increase the failure threshold.
642,Sven Klemm,2022-09-05 06:22:51-05,d65cad94e3dadd8ec4423575940061c8daada550,Reorganize gapfill header files,Combine gapfill header files to only have gapfill.h and gapfill_internal.h and make C files globally unique.
643,Sven Klemm,2022-09-05 03:49:10-05,b34b91f18bcaf3d6d997a6053e0e23dc44108fc3,Add timezone support to time_bucket_gapfill,"This patch adds a new time_bucket_gapfill function that allows bucketing in a specific timezone.  You can gapfill with explicit timezone like so: `SELECT time_bucket_gapfill('1 day', time, 'Europe/Berlin') ...`  Unfortunately this introduces an ambiguity with some previous call variations when an untyped start/finish argument was passed to the function. Some queries might need to be adjusted and either explicitly name the positional argument or resolve the type ambiguity by casting to the intended type."
644,Markos Fountoulakis,2022-09-06 07:53:00-05,698084c0e1b5e2f6079db355367270f9f4f96a34,Fix bad TupleTableSlot drop,PostgreSQL 15 exposed a use after free bug that went undetected in previous versions.
645,Alexander Kuzmenkov,2022-09-05 05:51:42-05,533a138ae22ddabf8a576d23dcb8f88a842fb6e5,Fix a possible error while flushing the COPY data,"Apparently, pqFlush can report a success status, but the underlying socket can become invalid at the same time. Check for this.  In passing, also change to the proper error reporting functions that use the connection error information, where needed."
647,Mats Kindahl,2022-09-02 03:46:32-05,83eb635e53ea1332b96f402141c1b3f7e3e9fab2,Fix coverity warning about potential overflow,"There is a potential overflow in shifting in the code. The shift count should be less than 20, so this should be safe, but adding a cast to ensure that types match later usage and also extending the assert to capture bugs."
648,Sven Klemm,2022-07-12 05:44:59-05,fe8d823bef3561d3db8b56f2cefb7d4dd9de0be8,Add compat macros for PG15 signature changes,PG15 refactors INSERT/UPDATE/DELETE code and changes the signatures of some of the functions.  https://github.com/postgres/postgres/commit/25e777cf
649,Sven Klemm,2022-09-01 04:08:17-05,9eef2e70f27366602f4da9ae89e44eeccba7bd08,Don't error when compression metadata is missing,Since the process that adjusts the compressed chunk statistics gets run during VACUUM we do not want to throw any errors in that process to let the VACUUM succeed.
650,Sven Klemm,2022-07-12 05:15:51-05,0d7da62251cff811ea531c3d54522ed6f3021e06,Adjust reorder code for PG15,"PG15 removes ""recheck"" argument from check_index_is_clusterable and consolidate VACUUM xid cutoff logic.  https://github.com/postgres/postgres/commit/b940918d https://github.com/postgres/postgres/commit/efa4a946"
651,Sven Klemm,2022-09-01 05:00:39-05,5c8b0b52b4dfadb8d8ab4c5c5d42c2f75a24728c,Adjust ts_get_node_name for PG15,PG15 removes T_Expr nodeTag because it's an abstract type.  https://github.com/postgres/postgres/commit/85399291
652,Sven Klemm,2022-09-01 10:35:34-05,39a700a4de4090a76268181faca19ea3d689e9ea,Fix changelog entries added in wrong place,Commit ed212b44 added the changelog entries to the bottom of the changelog instead of the top.
653,Sven Klemm,2022-08-31 14:55:12-05,3722b0bf23e3f10c8c053a1fd21dd01ebbcc1012,Add 2.8.0 to update tests,Add 2.8.0 to update tests and adjust the downgrade script files.
654,Konstantina Skovola,2022-08-31 11:52:53-05,fca9078d6c7592d2df6e0e40428688401aa47110,Exponentially backoff when out of background workers,"The scheduler detects the following three types of job failures:  1.Jobs that fail to launch (due to shortage of background workers) 2.Jobs that throw a runtime error 3.Jobs that crash due to a process crashing  In cases 2 and 3, additive backoff is applied in calculating the next start time of a failed job. In case 1 we previously retried to launch all jobs that failed to launch simultaneously.  This commit introduces exponential backoff in case 1, randomly selecting a wait time in [2, 2 + 2^f] seconds at microsecond granularity. The aim is to reduce the collision probability for jobs that compete for a background worker. The maximum backoff value is 1 minute. It does not change the behavior for cases 2 and 3.  Fixes #4562"
655,Bharathy,2022-08-28 23:31:56-05,ed212b4442ca112d281dc5331e4dc8e677e6fae9,GROUP BY error when setting compress_segmentby with an enum column,When using a custom ENUM data type for compressed hypertable on the GROUP BY clause raises an error.  Fixed it by generating scan paths for the query by checking if the SEGMENT BY column is a custom ENUM type and then report a valid error message.  Fixes #3481
659,Alexander Kuzmenkov,2022-08-23 10:30:20-05,ae6773fca6f285a3f0f56cda27bff1d08d8af596,Fix joins in RETURNING,"To make it work, it is enough to properly pass the parent of the PlanState while initializing the projection in RETURNING clause."
661,Dmitry Simonenko,2022-08-29 08:55:18-05,c697700addbe50027ca744c5aabca4181c89c0e3,Add hypertable distributed argument and defaults,This PR introduces a new `distributed` argument to the create_hypertable() function as well as two new GUC's to control its default behaviour: timescaledb.hypertable_distributed_default and timescaledb.hypertable_replication_factor_default.  The main idea of this change is to allow automatic creation of the distributed hypertables by default.
662,Fabrízio de Royes Mello,2022-07-05 06:39:14-05,e34218ce2963358a500f6bc315aace0fad29c450,Migrate Continuous Aggregates to the new format,"Timescale 2.7 released a new version of Continuous Aggregate (#4269) that store the final aggregation state instead of the byte array of the partial aggregate state, offering multiple opportunities of optimizations as well a more compact form.  When upgrading to Timescale 2.7, new created Continuous Aggregates are using the new format, but existing Continuous Aggregates keep using the format they were defined with.  Created a procedure to upgrade existing Continuous Aggregates from the old format to the new format, by calling a simple procedure:  test=# CALL cagg_migrate('conditions_summary_daily');  Closes #4424"
663,Matvey Arye,2022-07-29 14:08:53-05,c43307387e6e96765a98deca041b47f57e3c3928,Add runtime exclusion for hypertables,"In some cases, entire hypertables can be excluded at runtime. Some Examples:     WHERE col @> ANY(subselect)    if the subselect returns empty set     WHERE col op (subselect)    if the op is a strict operator and    the subselect returns empty set.  When qual clauses are not on partition columns, we use the old chunk exclusion, otherwise we try hypertable exclusion.  Hypertable exclusion is executed once per hypertable. This is cheaper than the chunk  exclusion that is once-per-chunk."
664,Alexander Kuzmenkov,2022-08-24 09:14:15-05,706a3c0e50777819c6a1caea5f29e9fa8354b34d,Enable statement logging in the tests,"Remove 'client_min_messages = LOG' where not needed, and add the 'LOG: statement' output otherwise."
665,Sven Klemm,2022-08-22 01:16:14-05,5d934baf1de8b7eace6384128f0942d4798d3798,Add timezone support to time_bucket,"This patch adds a new function time_bucket(period,timestamp,timezone) which supports bucketing for arbitrary timezones."
666,Konstantina Skovola,2022-06-28 08:19:32-05,dc145b7485171f7d38ba563efecd0c311a00bdd2,Add parameter check_config to alter_job,"Previously users had no way to update the check function registered with add_job. This commit adds a parameter check_config to alter_job to allow updating the check function field.  Also, previously the signature expected from a check was of the form (job_id, config) and there was no validation that the check function given had the correct signature. This commit removes the job_id as it is not required and also checks that the check function has the correct signature when it is registered with add_job, preventing an error being thrown at job runtime."
667,Mats Kindahl,2022-06-03 05:39:20-05,e0f3e17575902b995fb10ffb70a47f4418aa3c00,Use new validation functions,"Old patch was using old validation functions, but there are already validation functions that both read and validate the policy, so using those. Also removing the old `job_config_check` function since that is no longer use and instead adding a `job_config_check` that calls the checking function with the configuration."
668,gayyappan,2022-08-18 13:36:15-05,7c55d0d5dcdd10e87d3479bf3f7ed4b317f6a045,Modify OSM chunk's constraint info in chunk catalog,"The OSM chunk registers a dummy primary dimension range in the TimescaleDB catalog. Use the max interval of the dimension instead of the min interval i.e use range like [Dec 31 294246 PST, infinity). Otherwise, policies can try to apply the policy on an OSM chunk.  Add test with policies for OSM chunks"
669,Alexander Kuzmenkov,2022-08-24 02:30:06-05,bc85fb1cf04a05bd5fd32427464aa5d56e7468fc,Fix the flaky dist_ddl test,Add an option to hide the data node names from error messages.
670,Sven Klemm,2022-08-22 00:21:09-05,f4ba6e3c8e37a23017c93d1921f8a0a1afa528b3,Fix time_bucket_ng origin handling,This patch makes time_bucket_ng work correctly with origin values that are after the timestamp value to be bucketed.
672,Jan Nidzwetzki,2022-08-18 07:41:07-05,0786226e43ae80b992fb7c43a4117679cc2ce4cc,Ensure TSL library is loaded on database upgrades,"This patch ensures that the TSL library is loaded when the database is upgraded and post_update_cagg_try_repair is called. There are some situations when the library is not loaded properly (see #4573 and Support-Dev-Collab#468), resulting in the following error message:  ""[..] is not supported under the current ""timescale"" license HINT:  Upgrade your license to 'timescale'"""
673,Alexander Kuzmenkov,2022-08-23 10:52:15-05,51259b31c4c62b87228b059af0bbf28caa143eb3,Fix OOM in large INSERTs,"Do not allocate various temporary data in PortalContext, such as the hyperspace point corresponding to the row, or the intermediate data required for chunk lookup."
674,Joshua Lockerman,2022-08-22 10:56:20-05,3acfbd02c13a35f3d66231251b701965ab6c7619,Schema-qualify function telemetry,"This commit unconditionally schema-qualifies the function telemetry. In the default setting `format_procedure()` only schema-qualifies things that aren't on the search path. This could cause to much variation across users. Further, it'd be nice to tell if functions in-use are from an experimental schema or a main one."
676,Sven Klemm,2022-08-21 11:56:45-05,c488fcdbc9931c271361d0e4997a2b60e6eddf19,"Allow bucketing by month, year, century in time_bucket","This patch allows bucketing by month for time_bucket with date, timestamp or timestamptz. When bucketing by month the interval must only contain month components. When using origin together with bucketing by month only the year and month components are honoured.  To bucket by month we get the year and month of a date and convert that to the nth month since origin. This allows us to treat month bucketing similar to int bucketing. During this process we ignore the day component and therefore only support bucketing by full months."
677,Dmitry Simonenko,2022-08-22 09:49:57-05,82fc2cac699901749084c96c5f38ebf07e5e428a,Fix rename for distributed hypertable,"Fix ALTER TABLE RENAME TO command execution on a distributed hypertable, make sure data node list is set and command is executed on the data nodes.  Fix #4491"
678,Dmitry Simonenko,2022-08-22 09:19:43-05,90cace417e86d995d7c28356d9ded2dbbf6c035d,Load TSL library on compressed_data_out call,"A call to `compressed_data_out` from a replication worker would produce a misleading error saying that your license is ""timescale"" and you should upgrade to ""timescale"" license, even if you have already upgraded.  As a workaround, we try to load the TSL module it in this function. It will still error out in the ""apache"" version as intended.  We already had the same fix for `compressed_data_in` function."
679,gayyappan,2022-08-18 10:43:55-05,c643173b8b440b1d1fe6170be60907d428953f13,Filter out osm chunks from chunks information view,Modify timescaledb_information.chunks view to filter out osm chunks. We do not want user to invoke chunk specific operations on OSM chunks.
680,Sven Klemm,2022-08-19 01:53:59-05,0ffb7dadedcf2fa7bf83a22cc7196e8db6672336,Improve windows package test,Change the windows package test to use explicit postgres versions and test both minimum and maximum supported postgres minor version.
681,Sven Klemm,2022-08-19 04:17:24-05,3e03afb5ba1cac271d906b75fa3ad5b8e493a886,Fix ABI test,Recent CI settings refactoring made in commit 0c6f4e24 broke the ABI test. This patch fixes the test and adds setting for the minimum ABI compatible version.
684,Sven Klemm,2022-08-18 02:14:50-05,324201bb7fab6ecd7dc46766368c3ee65ebc7194,Enable Windows workflow in nightly CI run,Enable the Windows CI workflow in nightly runs. In nightly runs the CI run will also test release configuration in addition to the debug configuration run on PRs. This patch also removes the hard coded postgres version numbers from the workflow and reads it from ci settings.
685,gayyappan,2022-08-04 22:53:30-05,6beda28965dc980a4555165b3d6a1ee368a2547b,Modify chunk exclusion to include OSM chunks,OSM chunks manage their ranges and the timescale catalog has dummy ranges for these dimensions. So the chunk exclusion logic cannot rely on the timescaledb catalog metadata to exclude an OSM chunk.
686,gayyappan,2022-08-03 15:49:29-05,847919a05f014f7ff2d7e6ccd27c316ad8ba64cf,Add osm_chunk field to chunk catalog table,Setting this field to true indicates that this is an OSM chunk.
687,Fabrízio de Royes Mello,2022-08-17 16:44:57-05,8f920b393a6cfbe45428221af49e16214a4155df,Add missing gitignore entry,Pull request #4416 introduced a new template SQL test file but missed to add the properly gitgnore entry to ignore generated test files.
688,Sven Klemm,2022-08-12 09:15:51-05,4c9e89c80e5dc8576ded919d934e91beb4638bba,Migrate windows tests to GitHub Actions,This patch changes our windows test to run in GitHub actios instead of appveyor. This allows us to drop appveyor and have all CI workflows in one place. This also improves our windows testing as appveyor was only running on PG12 and this patch changes the windows tests to run on all supported PG versions. This patch also changes the windows tests to use the same configuration as the other tests. Since our test infrastructure heavily relies on shell scripting the test runner cannot run natively on Windows. We use WSL instead to get a unix environment to execute the test runner.
689,Sven Klemm,2022-08-15 13:50:46-05,30785726168aed99706a56a6327959c5618841ed,Fix regresschecklocal-t,Commit 05dd4787d1 changed regresscheck to run with multiple configurations but did not make similar adjustments to regresschecklocal-t.
690,Joshua Lockerman,2022-08-16 13:56:02-05,b5a16c4e0ba21c714c8838ffa22e200ff06886b8,Fix telemetry initialization,"We cannot rely on the planner initializing the shared hashmap reference since there's no guarantee we'll actually run SPI code in the BGW before we marshal the request. This commit adds initialization checking to `ts_function_telemetry_read()` itself to ensure it will always read from an initialized state if possible. As this code run very infrequently, it currently checks every time and does not skip if it once saw a lack of entries. This is unlikely to matter.  It would be nice to test this more explicitly, but that will likely have to wait until we have better reporting about BGW internals. For now I've relied on manual testing. Look at the telemetry report for the DB with UUID `d412018f-a435-492f-9ab6-b09d480ab1c1` received at timestamp `2022-08-16 18:56:53+00` for an example."
691,Sven Klemm,2022-08-15 10:51:23-05,5c96d25058e26874376f0d5d473469438234c7f1,Clean up multinode databases in tests,Not cleaning up created databases will prevent multiple regresschecklocal runs against the same instance cause it will block recreating the test users as they are still referenced in those databases.
692,Markos Fountoulakis,2022-08-16 11:10:07-05,9c6433e6edf3ba7fa4ea9207220da3e0db8e87e8,Handle TRUNCATE TABLE on chunks,"Make truncating a uncompressed chunk drop the data for the case where they reside in a corresponding compressed chunk.  Generate invalidations for Continuous Aggregates after TRUNCATE, so as to have consistent refresh operations on the materialization hypertable.  Fixes #4362"
693,Fabrízio de Royes Mello,2022-08-16 13:05:28-05,fc865de6e0d3890b0e4c8cae5db24942fcee1304,Change usage of scanint8 to pg_strtoint64,"PostgreSQL 15 changed `scanint8` to `pg_strtoint64`, so added it to the compatibility layer and use just of pg_strtoint64 in source code.  postgres/postgres@cfc7191dfea330dd7a71e940d59de78129bb6175"
694,Fabrízio de Royes Mello,2022-08-16 10:21:13-05,e97fa59839e20108840e3ee206ca2f4a69333f39,Remove unused header file,The Postgres header utils/int8.h is not necessary and it was removed in version 15 by commit postgres/postgres@cfc7191dfea330dd7a71e940d59de78129bb6175
695,Markos Fountoulakis,2022-08-16 09:51:57-05,d3d30c054766c5f1a3e9e978e383d8d28314efae,Fix heap buffer overflow,Fix heap buffer overflow when C-strings are shorter than NameData types.  Fixes #4571
696,Sven Klemm,2022-08-16 01:21:35-05,d256758fd771e603d81ef99431daaa541de033d3,Disable local line in pg_hba.conf on Windows,Windows does not support unix domain socket connections so we comment out `local` lines in pg_hba.conf. On PG12 on Windows this will prevent server start otherwise.
697,Markos Fountoulakis,2022-08-16 04:37:42-05,060cd893270f206cff1fa547801ef4786125c2d7,Fix identical branches,The same code is executed because the 'then' and 'else' branches are identical. Discovered by Coverity CID 380009.
698,Sven Klemm,2022-08-15 07:30:50-05,f92fe0cdd449008e758923f77979505516a35d26,Make test settings explicit,Our tests assume timezone to be US/Pacific and will fail with different values.
699,Sven Klemm,2022-08-16 00:41:46-05,7bd25e093165c3b354b7bbcac4c5ed60fd58ffe6,Explicitly set hash table memory context,We require hash_create to have an explicit memory context so they are not accidentally created in TopMemoryContext leading to memory leaks if unintended.
700,Sven Klemm,2022-08-12 02:29:12-05,ee4423c18181846d30d8910a02ff13ffeb79323a,Fix coccinelle hash_create script,Previously the hash_create coccinelle script would not detect hash_create calls that declared the return variable in the same statement or that used the return value of hash_create as return value.
702,Joshua Lockerman,2022-08-11 13:43:11-05,03defb33ef1d54ac7585e1eb6ee6caabaf4b27dc,Fix telemetry BGW memory leak,Applies the same fix as PR 4534 to the hash table created in the telemetry background worker.
703,Joshua Lockerman,2022-08-10 09:18:41-05,a3cfc091e89962e8860faf5a1db651d349dec5ac,Re-enable telemetry tests,They should be functioning after 2.7.2
704,Sven Klemm,2022-08-15 03:28:19-05,a6cda9c9f051b8526a94dabeeb3f2055bf9072cb,Fix chunk_utils_internal test,Change chunk_utils_internal test to not use oid but instead use the role name. Using the oid can lead to failing tests when oid assignment is different especially when run with regresschecklocal-t.
705,Sven Klemm,2022-08-11 10:57:24-05,131773a902580ffefebd7a6807b29dc4627807bc,Reset compression sequence when group resets,The sequence number of the compressed tuple is per segment by grouping and should be reset when the grouping changes to prevent overflows with many segmentby columns.
706,Sven Klemm,2022-08-11 04:12:52-05,0c6f4e24587bfb7ab56642a6df75172ee4472845,Centralize CI settings,This patch adds a new settings file for common github action settings. Instead of repeating latest pg versions in every github workflow we can read the settings from this central file.
707,Sven Klemm,2022-08-15 03:01:28-05,1c4c2bfd8ad5e69459775e92b2792d3b4043be76,Bump pgspot version used in CI to 0.3.3,Use pgspot 0.3.3 in CI and remove the obsolote check for 2.6.1.
709,Fabrízio de Royes Mello,2022-08-05 13:19:24-05,500c2259992ca6ea166ae7cd698e0dfcf76f77ac,Handle properly default privileges on CAggs,If a default privilege is configured and applied to a given Continuous Aggregate during it creation just the user view has the ACL properly configured but the underlying materialization hypertable no leading to permission errors.  Fixed it by copying the privileges from the user view to the materialization hypertable during the Continous Aggregate creation.  Fixes #4555
710,Rafia Sabih,2022-07-07 14:07:54-05,16fdb6ca5e3a42c120a45c7dc635e03c12f4b2e9,Checks for policy validation and compatibility,"At the time of adding or updating policies, it is checked if the policies are compatible with each other and to those already on the CAgg. These checks are: - refresh and compression policies should not overlap - refresh and retention policies should not overlap - compression and retention policies should not overlap  Co-authored-by: Markos Fountoulakis <markos@timescale.com>"
711,Rafia Sabih,2022-05-09 06:55:07-05,088f688780d2c015cc3e807573381acc7f2f2839,Miscellaneous,-Add infinity for refresh window range  Now to create open ended refresh policy  use +/- infinity for end_offset and star_offset  respectivly for the refresh policy. -Add remove_all_policies function  This will remove all the policies on a given  CAgg. -Remove parameter refresh_schedule_interval -Fix downgrade scripts -Fix IF EXISTS case  Co-authored-by: Markos Fountoulakis <markos@timescale.com>
712,Rafia Sabih,2022-05-04 05:26:14-05,bca65f46975f61e36e2ce57dc1ae7d80d6cccf05,1 step CAgg policy management,"This simplifies the process of adding the policies for the CAggs. Now, with one single sql statements all the policies can be added for a given CAgg. Similarly, all the policies can be removed or modified via single sql statement only.  This also adds a new function as well as a view to show all the policies on a continuous aggregate."
713,Sven Klemm,2022-08-11 12:43:02-05,be429eb3d9419440c56f33f7e32422ebd697fe29,Regenerate SSL certs,The SSL certs used for testing expired today. This patch adds new certificates that last for 100 years.
714,gayyappan,2022-08-08 16:49:25-05,95cc330e0cd7d81778864601b89082b32212ebf1,Add inherited check constraints to OSM chunk,"When a table is added to an inheritance hierrachy, PG checks if all check constraints are present on this table. When a OSM chunk is added as a child of a hypertable with constraints, make sure that all check constraints are replicated on the child OSM chunk as well."
715,Sven Klemm,2022-08-08 13:59:05-05,d843fdd3235d03fe104bc01ccc7918d188c315fb,Disable build_info test on appveyor,The build_info seems to be very flaky on appveyor since a couple days and often timing out instead of completing successfully. This patch temporarily disables that test on appveyor.
716,Sven Klemm,2022-08-05 07:54:20-05,a6107020e6e90387d390115de347224f58961178,Fix segfaults in compression code with corrupt data,Sanity check the compression header for sane algorithm before using it as index into an array. Previously this would result in a segfault and could happen with corrupted compressed data.
717,Fabrízio de Royes Mello,2022-08-08 08:36:07-05,ea7a9db97258f8804c956f75d1d72055f0f352da,Add primnodes.h to ts_get_node_name function,In partitioning.c the function `resolve_function_argtype` deal with some primitive nodes in order to resolve the argument type for hashing functions.  Refactoring the code a bit and added the primitive nodes (primnodes.h) to the `ts_get_node_name` and improve the error message of `resolve_function_argtype` showing the name of the node type instead of the code when an unsupported node type is detected.
718,Fabrízio de Royes Mello,2022-08-07 13:20:57-05,5c129be60fc0d23d9903b44953511b28c67a5ba7,Fix partitioning functions,When executing `get_partition_{hash|for_key}` inside an IMMUTABLE function we're getting the following error:  `ERROR: unsupported expression argument node type 112`  This error is because the underlying `resolve_function_argtype` was not dealing with `T_Param` node type.  Fixed it by dealing properly with `T_Param` node type returning the `paramtype` for the argument type.  Fixes #4575
719,Fabrízio de Royes Mello,2022-08-07 11:04:00-05,d35ea0f997bf815f652c2a4713924e1b51709012,Fix assertion in GRANT .. ON ALL TABLES IN SCHEMA,When working on a fix for #4555 discovered that executing `{GRANT|REVOKE} .. ON ALL TABLES IN SCHEMA` in an empty schema lead to an assertion because we change the way that command is executed by collecting all objects involved and processing one by one.  Fixed it by executing the previous process utility hook just when the list of target objects is not empty.  Fixes #4581
720,Sven Klemm,2022-08-03 00:54:55-05,13df260089b50c79ed590eee19228f66eb540de2,Dont run isolation tests on PG 12.0 and 13.2,The output format of the isolation tester got changed by pg upstream and backported to earlier versions. This means PG versions before the backport have different output format then latest PG version. We used to add all the isolation tests to the IGNORE list for those PG versions but that is error prone and often forgotten when new isolation tests are added. This patch skips the isolation test on PG versions with incompatible output format.
721,Alexander Kuzmenkov,2022-08-02 10:01:42-05,ce94cf126614e305a1bb4bd0f905cfedefb7fe17,Use non-blocking data node connections for COPY,"This PR switches the postgres connections to data nodes to non-blocking mode for the duration of the COPY. Other operations on these connections are unaffected.  It serves as a preparation for working with multiple connections simultaneously. No functional changes are introduced, but it's done as a separate PR to simplify review and debugging/bisect in case we introduce an error."
722,Sven Klemm,2022-08-02 11:41:20-05,17844a4087a69ec51aa75ef8b815d0109c019a40,Fix ABI test git errors,The most recent versions of the postgres docker image no longer contain git by default. The version available also contains a fix for the git vulnerability that changed the git behaviour to check file ownership. Since we bind mount the source checkout into the build container the user of the checkout is unlikely to match the user inside the container. This patch configures git to skip the owner check for the bind-mounted directory.  https://github.blog/2022-04-12-git-security-vulnerability-announced/
723,Sven Klemm,2022-08-02 11:42:57-05,bbfdcaccd5807751a8753d5c814ddf44fcd89bb2,Fix cmake when build directory not child of source directory,When the build directory was not a child of the source directory fetching the git information about the current commit would fail leading to this information not being included in the executable which in turn resulted in failing tests.
724,Jan Nidzwetzki,2022-08-02 01:30:12-05,d639cd89858e214709055ec7a5df498e3357e7af,Change the stalebot close message.,GitHub issues that are closed by a collaborator (or the stalebot) cannot be re-opened by the issue author. This commit changes the close message of the stale bot to reflect this.
725,Erik Nordström,2022-04-28 01:07:09-05,025bda6a81e63db42c34287802a35bc11b1001c3,Add stateful partition mappings,"Add a new metadata table `dimension_partition` which explicitly and statefully details how a space dimension is split into partitions, and (in the case of multi-node) which data nodes are responsible for storing chunks in each partition. Previously, partition and data nodes were assigned dynamically based on the current state when creating a chunk.  This is the first in a series of changes that will add more advanced functionality over time. For now, the metadata table simply writes out what was previously computed dynamically in code. Future code changes will alter the behavior to do smarter updates to the partitions when, e.g., adding and removing data nodes.  The idea of the `dimension_partition` table is to minimize changes in the partition to data node mappings across various events, such as changes in the number of data nodes, number of partitions, or the replication factor, which affect the mappings. For example, increasing the number of partitions from 3 to 4 currently leads to redefining all partition ranges and data node mappings to account for the new partition. Complete repartitioning can be disruptive to multi-node deployments. With stateful mappings, it is possible to split an existing partition without affecting the other partitions (similar to partitioning using consistent hashing).  Note that the dimension partition table expresses the current state of space partitions; i.e., the space-dimension constraints and data nodes to be assigned to new chunks. Existing chunks are not affected by changes in the dimension partition table, although an external job could rewrite, move, or copy chunks as desired to comply with the current dimension partition state. As such, the dimension partition table represents the ""desired"" space partitioning state.  Part of #4125"
726,Sven Klemm,2022-08-01 02:52:19-05,49b6486dad4458dbab5ead98bd9304208639762c,Change get_git_commit to return full commit hash,This patch changes get_git_commit to always return the full hash. Since different git versions do not agree on the length of the abbreviated hash this made the length flaky. To make the length consistent change it to always be the full hash.
727,Dmitry Simonenko,2022-08-01 02:36:11-05,65b5dc900f0ea5e2a76b3a592a4fc25126da6d2b,Support add_dimension() with existing data,"This change allows to create new dimensions even with existing chunks.  It does not modify any existing data or do migration, instead it creates full-range (-inf/inf) dimension slice for existing chunks in order to be compatible with newly created dimension.  All new chunks created after this will follow logic of the new dimension and its partitioning.  Fix: #2818"
728,Sven Klemm,2022-07-31 10:43:04-05,336f4b513f27fb2be6e041cdb3e4c289bf1adf9f,Fix hash_create calls without HASH_CONTEXT flag,This patch fixes callsites that set an explicit memory context in the control structure but do not specify the HASH_CONTEXT flag leading to the hash table being created in TopMemoryContext. This patch also changes call sites that want to create the hash table in TopMemoryContext to be explicit about this. Additionally this patch adds a coccinelle script to detect these errors and prevent adding similar code in the future.
729,Fabrízio de Royes Mello,2022-06-13 15:25:59-05,28440b79008230ef8c50da2f8d4640456bba8e02,Enable ORDER BY on Continuous Aggregates,Users often execute TopN like queries over Continuous Aggregates and now with the release 2.7 such queries are even faster because we remove the re-aggregation and don't store partials anymore.  Also the previous PR #4430 gave us the ability to create indexes direct on the aggregated columns leading to performance improvements.  But there are a noticable performance difference between `Materialized-Only` and `Real-Time` Continuous Aggregates for TopN queries.  Enabling the ORDER BY clause in the Continuous Aggregates definition result in:  1) improvements of the User Experience that can use this so commom    clause in SELECT queries  2) performance improvements because we give the planner a chance to    use the MergeAppend node by producing ordered datasets.  Closes #4456
730,Sven Klemm,2022-07-18 01:10:05-05,eccd6df7821ed94400f8693300ac29f5ef6ebf58,Throw better error message on incompatible row fetcher settings,When a query has multiple distributed hypertables the row-by-by fetcher cannot be used. This patch changes the fetcher selection logic to throw a better error message in those situations. Previously the following error would be produced in those situations: unexpected PQresult status 7 when starting COPY mode
731,Rafia Sabih,2022-07-19 12:31:08-05,4ab158d729cb8b1495c9024b6027e6d40022d86e,Fix gitcommit.h generation,"The commands for generating gitcommit.h need to be executed on every make run and not on cmake run to detect branch switches, commit changes or local modifications. That's why we add the commands in a custom target and run them on every make run. We do the generation part in a temporary file and only overwrite the actual file when the content is different to not trigger unnecessary recompilations."
732,Alexander Kuzmenkov,2022-07-28 04:51:23-05,08fb8e44894bcae0b0d9b5ccf9349c725fc250e0,Add chunks to baserel cache on chunk exclusion,This speeds up the planning by avoiding the repeated lookup of the chunk metadata.
733,Sven Klemm,2022-07-27 04:06:34-05,008c6cf37187ed706e3abab67305f60721fd0cae,Fix dump_meta_data for windows,The dump_meta_data sql script used an external call to date to get the current data which does not work on windows. This patch changes the script to use now() instead to get the current time and date.  Fixes #3674
734,Sven Klemm,2022-07-28 00:12:41-05,6db09c7f2e7aa2000776062fd57244793e3fba92,Fix timescaledb_post_restore GUC handling,In the session timescaledb_post_restore() was called the value for timescaledb.restoring might not be changed because the reset_val for the GUC was still on. We have to use explicit SET in this session to adjust the GUC.
735,Rafia Sabih,2022-07-21 12:46:15-05,a584263179ce7c9ffbf3700fa3323a831963c8d4,Fix alter column for compressed table,"Enables adding a boolean column with default value to a compressed table. This limitation was occurring due to the internal representation of default boolean values like 'True' or 'False', hence more checks are added for this.  Fixes #4486"
736,Sven Klemm,2022-07-26 15:25:38-05,8a308deb85d14fa6bb48bfb1262ba0d1e5e4c2cd,Add a generic memory leak test to CI,"Unlike the existing memory leak test which tests a single INSERT of 100.000.000 rows in a single transaction, this runs 1000000 SELECT, 10K UPDATE and 50K INSERT queries in the same backend."
737,Jan Nidzwetzki,2022-07-26 03:11:06-05,5fd13d3875ee9b1338c23460055632f760308ccc,Trigger bug board assignment on issue comment,"So far, issues were added to the bug board when they are labeled; issues that are not on the board and receive a response were not automatically added. This patch extends the workflow trigger; all bug issues that receive a response are also assigned to the board."
738,Sven Klemm,2022-07-25 02:48:58-05,990824a38365a2b811c858a790ebe093934c540d,Remove unused function and macro,This patch removes ts_chunk_add_foreign_table_as_chunk function which never had an implementation added and the ASSERT_IS_NULL_OR_VALID_CHUNK macro which has no user.
739,Sven Klemm,2022-07-25 02:30:04-05,7608cb8719ecf717fa8eba11f1e74efed00eccd4,2.7.2 Post-release,Add 2.7.2 to update tests and adjust downgrade script generation for 2.7.2.
742,Sven Klemm,2022-07-21 03:36:16-05,851fffb1ae723e899d25147fc2dca0f325528d90,Release 2.7.2,"This release is a patch release. We recommend that you upgrade at the next available opportunity. Among other things this release fixes several memory leaks, handling of TOASTed values in GapFill and parameter handling in prepared statements.  **Bugfixes**  * #4517 Fix prepared statement param handling in ChunkAppend * #4522 Fix ANALYZE on dist hypertable for a set of nodes * #4526 Fix gapfill group comparison for TOASTed values * #4527 Handle stats properly for range types * #4532 Fix memory leak in function telemetry * #4534 Use explicit memory context with hash_create * #4538 Fix chunk creation on hypertables with non-default statistics  **Thanks**  * @3a6u9ka, @bgemmill, @hongquan, @stl-leonid-kalmaev and @victor-sudakov for reporting a memory leak * @hleung2021 and @laocaixw  for reporting an issue with parameter handling in prepared statements"
743,Sven Klemm,2022-07-21 16:22:31-05,90c7c652b19a46c07c2a7f63dc1efc9a0d39f18d,Fix chunk creation on hypertables with non-default statistics,When triggering chunk creation on a hypertable with non-default statistics targets by a user different from the hypertable owner the chunk creation will fail with a permission error. This patch changes the chunk table creation to run the attribute modification as the table owner.  Fixes #4474
744,Sven Klemm,2022-07-21 23:29:54-05,ce9672aee3643eba503fb58ebc5262b6ccdaac98,Fix dist_copy_long test on macOS,On macOS zcat expects the file to end in .Z appending that extension when the supplied filename does not have it. Leading to the following error for the dist_copy_long test:  zcat: can't stat: data/prices-10k-random-1.tsv.gz (data/prices-10k-random-1.tsv.gz.Z): No such file or directory  This patch changes the dist_copy_long test to use the shell to read the file instead and use input redirection so zcat never sees the filename.
745,gayyappan,2022-07-07 17:14:19-05,6b0a9937c59d4648297ee753e12b2445e50e14cd,Fix attach_osm_table_chunk,Add chunk inheritance when attaching a OSM tabel as a chunk of the hypertable
746,Sven Klemm,2022-07-21 11:59:17-05,ba41a92d9ea966304102406390f58f6dfc45e374,Bump macOS version used in CI to 11,macOS 10.15 is deprecated as github action environment and will be unsupported by end of august. This PR switches our CI to use macOS 11 environment instead.  https://github.com/actions/virtual-environments/issues/5583
747,Alexander Kuzmenkov,2022-07-21 07:53:15-05,1f6b0240a361f70d87769c61011edb27080462d7,Add a distributed COPY test with more data,Use data sets with 10k and 100k rows with a variety of partitioning settings. This may help to catch some rare corner cases.
748,Alexander Kuzmenkov,2022-06-28 09:49:23-05,296601b1d7aba7f23aea3d47c617e2d6df81de3e,Synchronize chunk cache sizes,"Specifically, flush the chunk multi-insert states when the number of it reaches the size of the chunk cache. Otherwise, the chunks corresponding to the multi-insert states go out of cache and have to be looked up again when the multi-insert state is flushed, which leads to a performance hit."
749,Sven Klemm,2022-07-20 16:13:10-05,7c841b8d924b017cf65c3e3ce631a686bd7afa00,Use explicit memory context with hash_create,This changes the hash_create calls in the function cache and in the cagg code to use explicit memory context. Without this hash_create will create the hash table in TopMemoryContext potentially leading to memory leaks.
750,Dmitry Simonenko,2022-07-21 03:57:02-05,4ed116b6f6aef5d913df1cce9b80b8eb96af351d,Fix ANALYZE on dist hypertable for a set of nodes,Make sure ANALYZE can be run on a specific set of data nodes assigned to the distributed hypertable  Fix #4508
751,Konstantina Skovola,2022-07-20 13:39:05-05,ca3c85ba1e31c29276e8af505da56dc3b272a022,Remove CODEOWNERS file to activate pull-review,"In #4525 a workflow was introduced for running pull-review to automate reviewer assignment based on code ownership. However the CODEOWNERS file was not removed.  As reviewers will be assigned based on it if it exists, it prevents the pull-review workflow from taking effect. This commit removes CODEOWNERS to allow pull-review to do the reviewer assignment."
752,Sven Klemm,2022-07-20 12:36:13-05,47d6a226f572611b31d393fc54ec2c965515590a,Fix memory leak in function telemetry,hash_create will create the hash table in TopMemoryContext unless explicitly requested to create in different memory context. The function telemetry code did not explicitly request a different context leading to a memory leak.
753,Nikhil Sontakke,2022-07-19 08:42:37-05,63a80eec0d23aae3be48bbca4df33f945b3a4a58,Handle stats properly for range types,"For range types, the operator entry in statistics table is invalid. Also, certain range types don't have ""VALUES"" but only have ""NUMBERS"" entries."
754,Konstantina Skovola,2022-07-20 00:55:30-05,0e9dd3c70bf9dc5fc8f92a9efda171a880696957,Add config file and workflow for pull-review,"This commit adds a workflow for running pull-review in the serverless mode, using docker container CLI."
755,Sven Klemm,2022-07-19 06:04:48-05,d5619283f3a9d04b926227be2cdf87db8104191a,Fix gapfill group comparison,The gapfill mechanism to detect an aggregation group change was using datumIsEqual to compare the group values. datumIsEqual does not detoast values so when one value is toasted and the other value is not it will not return the correct result. This patch changes the gapfill code to use the correct equal operator for the type of the group column instead of datumIsEqual.
756,Sven Klemm,2022-07-18 00:16:19-05,0d175b262e3f34f790318e1c4ea28eae104d6b77,Fix prepared statement param handling in ChunkAppend,This patch fixes the param handling in prepared statements for generic plans in ChunkAppend making those params usable in chunk exclusion. Previously those params would not be resolved and therefore not used for chunk exclusion.  Fixes #3719
757,Sven Klemm,2022-07-15 02:21:02-05,cfac68ec3d8ed80f65b35f594d6c83b0b7002fae,Ignore compression_chunk_race test in 12.0 and 13.2,The isolation tester in earlier PG versions is lacking features we rely on in our tests so the results of isolation tests are ignored when testing on the earliest version of a major pg version.
758,Jan Nidzwetzki,2022-07-18 00:46:12-05,91c3820d4e693ed0eec5e75ca74300f73a7bd83c,Activate a stalebot to close issues,"In #4499, a stalebot configuration was introduced. According to the configuration, the stalebot runs in dry mode. The output of a dry mode run was analyzed and issues with an outdated  ""need-more-info"" label have been cleaned up. So, the stalebot can now be activated."
759,Sven Klemm,2022-07-17 06:44:49-05,597b71881ad20f8498938c918e23aa0abe280532,Fix assertion hit in row_by_row_fetcher_close,When executing multinode queries that initialize row-by-row fetcher but never execute it the node cleanup code would hit an assertion checking the state of the fetcher. Found by sqlsmith.
760,Sven Klemm,2022-07-15 05:31:41-05,d55ceb34c311fe1f8de0f6ad90e32199307e8895,Fix unaligned pointer access,The sanitizer found unaligned pointers. On amd64 this is not a problem but other platforms like arm are more strict regarding pointer alignment.
761,Jan Nidzwetzki,2022-07-07 03:59:50-05,93bad0098d14bd9be6f6c984ab487efc508ca6a4,Introduce a stalebot to close issues,"This PR introduces a configuration for the 'actions/stale' stalebot, which is used to automatically flag and close stale issues after a few days of inactivity.  Note: The configuration runs in dry-mode to test the behavior of the       bot on our repository first."
764,Nikhil Sontakke,2022-07-05 10:34:41-05,fdb12f7abe72950e61f20956d781a9fa9c2966af,Handle timescaledb versions aptly in multinode,The current check where we deem a DN incompatible if it's on a newer version is exactly the opposite of what we want it to be. Fix that and also add relevant test cases.
765,Nikhil Sontakke,2022-07-06 08:26:40-05,0c03ed954d0933b107319178cd5b04a592c4f1d2,Ignore telemetry test for now,Temporarily ignore to allow the release of 2.7.1
766,Fabrízio de Royes Mello,2022-07-05 07:59:34-05,335f298ef756a2e7cb11f4a4fb0f866b853e9ba5,Segfault when executing IMMUTABLE functions,Executing an IMMUTABLE function that has parameters and exception handling block multiple times in the same transaction causes a null pointer segfault when try to reset a non-initialized ts_baserel_info.  Fixed it by preventing to reset a non-initialized `ts_baserel_info`.  Fixes #4489
767,Jan Nidzwetzki,2022-06-29 08:12:46-05,a608d7db614c930213dee8d6a5e9d26a0259da61,Fix race conditions during chunk (de)compression,"This patch introduces a further check to compress_chunk_impl and decompress_chunk_impl. After all locks are acquired, a check is made to see if the chunk is still (un-)compressed. If the chunk was (de-)compressed while waiting for the locks, the (de-)compression operation is stopped.  In addition, the chunk locks in decompress_chunk_impl are upgraded to AccessExclusiveLock to ensure the chunk is not deleted while other transactions are using it.  Fixes: #4480"
768,Alexander Kuzmenkov,2022-06-30 03:18:58-05,1bbb6059cba0e75872325fe1e88ffdc8d25d4383,Add more tests for distributed INSERT and COPY,"More interleavings of INSERT/COPY, and test with slow recv() to check waiting."
769,Mats Kindahl,2022-07-04 02:58:59-05,07c8ed6af3fa01efa8e3943711c5456834acdf4e,Add workflow to move issue to triage,Add workflow to automatically add all issues opened or labeled to the bug board for triage.
770,gayyappan,2022-06-24 09:41:29-05,6c20e74674a88164c77e9d6b14dd50aa71d65752,Block drop chunk if chunk is in frozen state,"A chunk in frozen state cannot be dropped. drop_chunks will skip over frozen chunks without erroring. Internal api , drop_chunk will error if you attempt to  drop a chunk without unfreezing it.  This PR also adds a new internal API to unfreeze a chunk."
771,Joshua Lockerman,2022-06-02 15:11:44-05,dd4bc4c24601ca39b43f49d4cde32a218f42b8bd,Add telemetry about replication,This commit adds telemetry about replication status to our telemetry gatherer. It adds a new sub object `replication` containing two fields:   - `is_wal_receiver` is a boolean which is true if-and-only-if the      current cluster has a `wal_receiver`.   - `num_wal_senders` is the number of `wal_senders` that the current      cluster has.
772,Nikhil Sontakke,2022-06-24 02:49:31-05,8bf6c887a93d2c013f84647248182d46607a91f3,Better superuser handling for move_chunk,"The current code was assuming the bootstrap superuser for the actual move chunk operation. However, we can make it further flexible by using the logged in credentials if those happen to have superuser privileges."
773,Sven Klemm,2022-06-24 12:12:46-05,0b76a8e5c37a9d0b3d1f9d82c74548c187966a36,Remove travis config,We migrated all our CI jobs to GitHub actions quite a while ago but never removed the travis config file even though we were not using it anymore.
774,gayyappan,2022-06-21 16:14:43-05,79bf4f53b144ed25569b19b76d0aafbbcc17a15f,Add api to associate a hypertable with custom jobs,"This PR introduces a new SQL function to associate a hypertable or continuous agg with a custom job. If this dependency is setup, the job is automatically deleted when the hypertable/cagg is dropped."
776,gayyappan,2022-04-28 16:51:27-05,131f58ee602af8dcdc4b901587cb72beb3f51d2e,Add internal api for foreign table chunk,Add _timescaledb_internal.attach_osm_table_chunk. This treats a pre-existing foreign table as a hypertable chunk by adding dummy metadata to the catalog tables.
777,Markos Fountoulakis,2022-06-23 02:59:24-05,6c38c60b9782797b538129ba92f61268b4da42c6,Repair numeric partial state on the fly,The numeric format changed between PG13 and PG14 to include infinities. As a result the serialized partial state of numeric aggregates also changed format.  If a user that has stored partials (e.g. by using Continuous Aggregates) upgrades to PG14 then the partial state deserialization will lead to errors due to the mismatch with the PG14 code.  Repair the deserialization process on the fly by appending zeroed plus-infinity and minus-infinity counts for the numeric aggregate state to use.  Fixes #4427
778,Alexander Kuzmenkov,2022-06-20 03:33:32-05,93e9d421931a50d443da02c078f1e8ae93388517,Use COPY protocol in row-by-row fetcher,This gives about 2x speedup for bulk data transfer queries.
779,Nikhil Sontakke,2022-06-21 09:51:31-05,e3b2fbdf1513ee8b2db111a3a7c1c0f5d42b1b26,Fix empty bytea handlng with distributed tables,"The ""empty"" bytea value in a column of a distributed table when selected was being returned as ""null"". The actual value on the datanodes was being stored appropriately but just the return code path was converting it into ""null"" on the AN. This has been handled via the use of PQgetisnull() function now.  Fixes #3455"
780,Sven Klemm,2022-06-21 01:15:19-05,db6447378624aa09745ce0c7762205e12fa33a76,Use PG 14.4 in update test,Switch update/downgrade test to use 14.4. This has been split off the CI PR switching the rest of CI to 14.4 because it usually takes more then a week for the upstream docker images to become available.
781,Jacob Champion,2022-06-20 11:18:35-05,f400c75b79db73d49245728139ef0c26df3d4551,Tweak commit-msg hook's line character limits,"The commit-msg hook was counting the end-of-line characters in the 50- and 72-character limit, so commit messages had to be wrapped slightly shorter in order to avoid the hook complaint. Strip the EOL characters during the check instead.  Since we use universal newline handling during the file read, stripping LF ('\n') should be enough to handle both Windows and Unix development environments."
782,Jacob Champion,2022-06-20 10:52:49-05,bab5cc101d451f160f409d5848eab053908ca04d,Update commit-msg hook to Python 3,"This git hook is copied automatically during CMake, and breaks commits immediately afterwards if Python 2 isn't installed. Since Python 2 has been end-of-life for a while now, take this chance to upgrade."
783,Jacob Champion,2022-06-20 10:48:06-05,1221977c9c922accab2255507113bfa83d854216,Inspect pg_config.h for SSL support,"We parse apart the output of `pg_config --configure` to see whether or not OpenSSL is enabled. This is a bit brittle; it has broken in the past with the change from --with-openssl to --with-ssl, and upstream is currently testing a change to the build system (Meson, which doesn't rely on autotools during configuration) that is likely to further interfere with this approach.  As an alternative, we can just look at the header files to get this information.  USE_OPENSSL is not defined in pg_config.h if SSL support is not compiled in."
784,Dmitry Simonenko,2022-06-21 05:38:27-05,79aa0f637834066f4e7e470bd49bb629bab9fdd7,Do not allow dist ddl during create extension,"This change fixes logic for the extension loading check, by moving it before the distributed_ddl guc check."
785,Sven Klemm,2022-06-12 08:50:07-05,02d4aefb85340dcafea6729141bac033fe3f3ef1,Fix flaky data_node_bootstrap test,Copy collation and chartype before releasing syscache since we need them past the lifetime of the current context.
787,Nikhil Sontakke,2022-06-14 06:59:54-05,1a9d775a2c88cee3bd8bb4cf1ddd47cb670a31f3,Fix perms in copy/move chunk,"We mandate that a superuser or a user with REPLICATION privileges can invoke copy_chunk or move_chunk procedures. Internally, many stages are carried out to complete the activity and different stages need different user permissions. To keep things uniform we now switch to the bootstrap superuser in each stage. Care is taken to ensure that the original hypertable ownership is retained on the new chunk post the move operation."
788,Sven Klemm,2022-06-17 14:51:35-05,048d86e7e7a73f30002eeb9f85b25d84c6206cf7,Fix duplicate header guard,The compression code had 2 files using the same header guard. This patch renames the file with floating point helper functions to float_utils.h and renames the other file to compression/api since that more clearly reflects the purpose of the functions.
790,Sven Klemm,2022-06-15 23:48:17-05,cb096757fa8abe8eea4d4f1959f641e385ba71ea,Bump Postgres version used in CI to 14.4,Make CI use just released PG 14.4. We skip the version bump for the update script as docker images with 14.4 are nowhere to be seen.
792,Fabrízio de Royes Mello,2022-06-15 10:01:07-05,42f197e5799b06c7f33198f79a48531c582cdc7b,Explicit constraint names in schema definition,"In `src/ts_catalog/catalog.c` we explicit define some constraints and indexes names into `catalog_table_index_definitions` array, but in our pre-install SQL script for schema definition we don't, so let's be more explicit here and prevent future surprises."
793,Nikhil Sontakke,2022-06-16 06:53:42-05,ed55654a32c2b542419bcb57da14f0bda9c55652,Retain hypertable ownership on attach_data_node,If a superuser is used to invoke attach_data_node on a hypertable then we need to ensure that the object created on this data node has the same original ownership permissions.  Fixes #4433
795,Erik Nordström,2022-06-03 06:58:44-05,19b3f67b9c5cf92184ba1fce3495df2cc2947e11,Drop remote data when detaching data node,"Add a parameter `drop_remote_data` to `detach_data_node()` which allows dropping the hypertable on the data node when detaching it. This is useful when detaching a data node and then immediately attaching it again. If the data remains on the data node, the re-attach will fail with an error complaining that the hypertable already exists.  The new parameter is analogous to the `drop_database` parameter of `delete_data_node`. The new parameter is `false` by default for compatibility and ensures that a data node can be detached without requiring communicating with the data node (e.g., if the data node is not responding due to a failure).  Closes #4414"
796,Alexander Kuzmenkov,2022-06-01 06:01:40-05,56945b37b8920cb7590dd61dc3d8a625e7d650a2,Enable ON CONFLICT ON CONSTRAINT for hypertables,It now works since we started to rely on Postgres' arbiter index inference.
797,Sven Klemm,2022-06-12 04:13:10-05,1784e83d77bc3004c83af73a0fe6de8307f2583c,Fix segfault in subscription_exec,Add a check for NULL input to subscription_exec as the function currently segfaults on NULL input. Found by sqlsmith.
798,Fabrízio de Royes Mello,2022-06-09 07:59:55-05,07c5f7281c0256a57e45e459cf15c944963801ad,Create index on Continuous Aggregates,Timescale 2.7 released a new version of Continuous Aggregate (#4269) that allows users efectivelly create and use indexes in the materialization hypertable. The boring part of it is that users should discover what is the associated materialization hypertable to issue a `CREATE INDEX` statement.  Improved it by allowing users to easily create indexes in the materialization hypertable by simple executing a `CREATE INDEX` direct in the Continuous Aggregate.  Example: `CREATE INDEX name_of_the_index ON continuous_agregate (column);`
799,Sven Klemm,2022-06-13 01:35:16-05,1ac06c71c2bbfd0f878d0078ab9ab01367302c1c,Refactor make_partfunc_call,Refactor make_partfunc_call to only accept fnoid and rettype instead of PartitioningFunc which also gets rid of a coverity warning about that parameter being passed by value instead of reference because that parameter was longer than 128 bytes.
801,Luigi Servini,2022-05-09 09:38:44-05,eb9f466582762e00ae2aefdece09d658903dfc44,Update dump_meta_data.sql,Added `c.dropped is false ` for table _timescaledb_catalog.chunk to skip dropped chunks in the size calculation.
802,Fabrízio de Royes Mello,2022-06-08 11:54:21-05,f72a277577a76678b8f903230a76567cdbd19718,Fix continuous aggregates deprecated tests,Previous pull request #4269 introduced new format for Continuous Aggregates and we also added regression tests for the `deprecated` version to make sure it will keep working until we decide to completely deprecate and remove the old version.  Unfortunately for some deprecated continous aggregates regression tests we miss to set properly the flag `timescaledb.finalized=false`.  Fixed it by properly setting the `timecaledb.finalized=false` during the continuous aggregate creation.
803,Rajakavitha Kodhandapani,2022-06-07 05:10:58-05,c6dc0caf1deee6510b6bddd6a0395f0a7368d89f,Update README.md,fixes: https://github.com/timescale/docs/issues/1162
804,Alexander Kuzmenkov,2022-05-26 02:53:48-05,3c56d3ecebbf476293ff43ded142bc9e5087f6de,Faster lookup of chunks by point,"Don't keep the chunk constraints while searching. The number of candidate chunks can be very large, so keeping these constraints is a lot of work and uses a lot of memory. For finding the matching chunk, it is enough to track the number of dimensions that matched a given chunk id. After finding the chunk id, we can look up only the matching chunk data with the usual function.  This saves some work when doing INSERTs."
805,Sven Klemm,2022-05-23 16:01:46-05,216ea65937455dc2194b155e0713093f14a39f96,Enable chunk exclusion for space dimensions in UPDATE/DELETE,"This patch transforms constraints on hash-based space partitions to make them usable by postgres constraint exclusion.  If we have an equality condition on a space partitioning column, we add a corresponding condition on get_partition_hash on this column. These conditions match the constraints on chunks, so postgres' constraint exclusion is able to use them and exclude the chunks.  The following transformations are done:  device_id = 1 becomes ((device_id = 1) AND (_timescaledb_internal.get_partition_hash(device_id) = 242423622))  s1 = ANY ('{s1_2,s1_2}'::text[]) becomes ((s1 = ANY ('{s1_2,s1_2}'::text[])) AND (_timescaledb_internal.get_partition_hash(s1) = ANY ('{1583420735,1583420735}'::integer[])))  These transformations are not visible in EXPLAIN output as we remove them again after hypertable expansion is done."
806,Sven Klemm,2022-06-06 07:57:53-05,ce59820678075875f8db3871caee28b99c635bdd,Fix removal of constified constraints,Commit dcb7dcc5 removed the constified intermediate values used during hypertable expansion but only did so completely for PG14. For PG12 and PG13 some constraints remained in the plan.
807,Konstantina Skovola,2022-05-23 10:48:09-05,b6a974e7f3c7439c1ef423ea64431ce2a907e068,Add schedule_interval to policies,Add a parameter `schedule_interval` to retention and compression policies to allow users to define the schedule interval. Fall back to previous default if no value is specified.  Fixes #3806
808,Sven Klemm,2022-05-31 04:56:43-05,96202a99bdf19f1b459c29d3e12ab42ecf9bc6a5,Adjust code to PG15 pg_database changes,PG15 changes the type of collate and ctype from name to text.  https://github.com/postgres/postgres/commit/54637508
809,Sven Klemm,2022-06-01 22:55:33-05,1ef515eb7a5ca0c00f78c552ba9f7c20b67423a5,Add shmem_request_hook,This patch consolidates all shared memory requests in a shmem_request_hook. While there are no strict requirements when to request shared memory for PG < 15 in PG 15 it has to happen in the shmem_request_hook otherwise the request will be blocked.  https://github.com/postgres/postgres/commit/4f2400cb
810,Konstantina Skovola,2022-05-27 07:39:31-05,f059e00fad24f7438bb4a275b42e56522bebf8a3,Create composite index on segmentby columns,"Previously we created one index per segmentby column, of the form `(col, _ts_meta_sequence_num)`. Compressed data is ordered by segmentby, then by orderby within the segments and lastly by the sequence number of the batch. So if segmentby columns are missing from the index, that index cannot be used to produce ordered output, requiring an extra sort step.  A composite index containing all segmentby columns removes the additional sort step and gives better plans.  Fixes #4314"
811,Erik Nordström,2022-05-30 09:08:45-05,8f9975d7be856df984763ed837e6b17b36768b4d,Fix crash during insert into distributed hypertable,"For certain inserts on a distributed hypertable, e.g., involving CTEs and upserts, plans can be generated that weren't properly handled by the DataNodeCopy and DataNodeDispatch execution nodes. In particular, the nodes expect ChunkDispatch as a child node, but PostgreSQL can sometimes insert a Result node above ChunkDispatch, causing the crash.  Further, behavioral changes in PG14 also caused the DataNodeCopy node to sometimes wrongly believe a RETURNING clause was present. The check for returning clauses has been updated to fix this issue.  Fixes #4339"
812,Erik Nordström,2022-06-01 10:17:50-05,65b0dda97b11224e79f3aa7580b714337718308c,Fix CI coredump handling,The CI task for getting coredumps after crashes of a 32-bit build didn't handle a variable correctly. Fix to avoid errors.
813,Sven Klemm,2022-05-31 02:19:11-05,cae1f16367ca734b7eca32f7add2f19cc957d537,Use our implementation of find_em_expr_for_rel for PG15+,PG13 added an implementation of find_em_expr_for_rel to postgres core code. Which is removed again in PG15. This patch adjusts our macros to deal with the removal in PG15.  https://github.com/postgres/postgres/commit/f3dd9fe1
814,Konstantina Skovola,2022-05-26 06:39:13-05,f28131cff5b23ede1a316b5d81452918bc815fd9,Don't ask for orderby column if default already set,"When enabling compression on a hypertable, the orderby option can be omitted, which will set the default value of ""time DESC"". However previously, when executing the same command twice not setting orderby, the second time we would get an error that orderby was previously set and must be specified. For example when executing `alter table set (timescaledb.compress, timescaledb.segmentby = '..')`  The reason for that error was that it's unclear if no orderby means leave as is, or if it means set the default value. But in the case where orderby is already set to the default value, there is no ambiguity and both cases are equivalent, so the default value can be reset without giving an error.  Fixes #4331"
815,Alexander Kuzmenkov,2022-06-01 07:26:29-05,ed948b17721a45301604433c1fd8572ed3fa1901,Don't force clang-tidy on,"If the compiler is gcc, clang-tidy might not recognize some of its flags and error out. It will be enabled by default if the compiler is clang."
816,Mats Kindahl,2022-05-24 03:47:57-05,533e849c574934eb8f00e254a6467376391622bb,Pass parameters to workers as a struct,"Non-functional change.  Parameters to workers were passed in as a serialized string, which then needs to be serialized and deserialized using dedicated functions.  This commit refactors code to pass parameters to workers as a struct, which is then just copied into the `bgw_extra` field of `BackgroundWorker`. The struct contains simple values and can therefore be copied using memcpy(3c)."
817,Alexander Kuzmenkov,2022-05-06 06:14:48-05,5c0110cbbf644ec79b2d2b0249eb3e112c081c4b,Mark partialize_agg as parallel safe,"Postgres knows whether a given aggregate is parallel-safe, and creates parallel aggregation plans based on that. The `partialize_agg` is a wrapper we use to perform partial aggregation on data nodes. It is a pure function that produces serialized aggregation state as a result. Being pure, it doesn't influence parallel safety. This means we don't need to mark it parallel-unsafe to artificially disable the parallel plans for partial aggregation. They will be chosen as usual based on the parallel-safety of the underlying aggregate function."
818,Jan Nidzwetzki,2022-05-31 03:33:38-05,1d0670e703862b284c241ab797404f851b25b5df,Fix flaky copy test by generating fixed test data,The copy test is flaky because some test data is generated dynamically based on the current date. This patch changes the data generation to a time series with fixed dates.
819,Sven Klemm,2022-05-28 06:40:10-05,1fbe2eb36f3ff4b12023549b76c8c3df0b8a79b9,Support intervals with month component when constifying now(),"When dealing with Intervals with month component timezone changes can result in multiple day differences in the outcome of these calculations due to different month lengths. When dealing with months we add a 7 day safety buffer. For all these calculations it is fine if we exclude less chunks than strictly required for the operation, additional exclusion with exact values will happen in the executor. But under no circumstances must we exclude too much cause there would be no way for the executor to get those chunks back."
820,Sven Klemm,2022-05-29 08:08:55-05,2715b5564a3be4bb9c3fdb29f81a5c3fb3c0aae6,Replace pg_atoi with pg_strtoint16/32,"PG 15 removes pg_atoi, so this patch changes all callers to use pg_strtoint16/32.  https://github.com/postgres/postgres/commit/73508475"
821,Sven Klemm,2022-05-26 06:34:40-05,12574dc8ecda70d60a3ee0cf4a8489aa23bccb8f,Support intervals with day component when constifying now(),The initial patch to use now() expressions during planner hypertable expansion only supported intervals with no day or month component. This patch adds support for intervals with day component.  If the interval has a day component then the calculation needs to take into account daylight saving time switches and thereby a day would not always be exactly 24 hours. We mitigate this by adding a safety buffer to account for these dst switches when dealing with intervals with day component. These calculations will be repeated with exact values during execution. Since dst switches seem to range between -1 and 2 hours we set the safety buffer to 4 hours.  This patch also refactors the tests since the previous tests made it hard to tell the feature was working after the constified values have been removed from the plans.
822,Alexander Kuzmenkov,2022-05-13 07:49:00-05,a6b5f9002cf4f3894aa8cbced7f862a73784cada,More clear clang-tidy options,Enable a closed list of checks and treat everything as errors.
826,Sven Klemm,2022-05-25 13:16:46-05,7aec25d69e45292569fbe27364c2f68d269d416e,Adjust coccinelle and shellcheck CI config,"Currently coccinelle and shellcheck get run an additional time for every merged commit to master. This patch adjusts the config so they are only run on pull request or on push to prerelease_test instead of push to any branch, similar to how all the other workflows are set up."
827,Joshua Lockerman,2022-04-11 11:45:01-05,c35e9bf611bf52459b1771ac66c0a71a4afe1ad0,Function telemetry,"This commit contains extends our telemetry system with function call telemetry. It gathers function call-counts from all queries, and send back counts for those functions that are built in or from our related extensions."
828,Mats Kindahl,2022-05-25 06:39:16-05,34bf69544453f0a941f53aa5f2878426bfa5e353,Add initializer to auto variable,Compilers are not smart enough to check that `conn` is initialized inside the loop so not initializing it gives an error. Added an initializer to the auto variable to get rid of the error.
829,Sven Klemm,2022-05-25 03:59:33-05,f0556dc9026342152bf6c34a7d495d5e0b410ee3,Skip 001_extension test on PG13.2 in CI,The extension test uses the background_psql function which is not present in the 13.2 PostgresNode module. This function is only available in PG 13.5+.  https://github.com/postgres/postgres/commit/a9d0a540
830,Sven Klemm,2022-05-25 01:10:45-05,0b6a09f027c521dc543b2b3bd3394af593702659,Add support for SKIPS to provecheck,This patch adds support for skipping individual tests in a provecheck run similar to what we have for our regression checks.
831,Sven Klemm,2022-05-22 23:06:58-05,dcb7dcc5064a7f843d435482d5c0cf643e7f9021,Remove constified now() constraints from plan,Commit 35ea80ff added an optimization to enable expressions with now() to be used during plan-time chunk exclusion by constifying the now() expression. The added constified constraints were left in the plan even though they were only required during the hypertable explansion. This patch marks those constified constraints and removes them once they are no longer required.
832,Jan Nidzwetzki,2022-05-20 05:30:50-05,d249954be0407d358ef12590e9a92f2f9926269a,Improved buffer management in the copy operator,"The multi-buffer copy optimization creates a multi-insert buffer per ChunkInsertState. However, chunks can be closed. When ts_chunk_dispatch_get_chunk_insert_state is called for a closed chunk again, a new ChunkInsertState is returned. So far, also a new multi-insert buffer has been created. Therefore, multiple batch operations could be executed per chunk, which reduces the efficiency of the optimization.  This patch introduces an HTAB that maps from the chunk_id to the multi-insert buffer. Even when a chunk is closed, all tuples for a chunk are stored in the same buffer."
833,Sven Klemm,2022-05-24 03:42:19-05,cf9b6267941964fb134c1e2122b91c473a81f7e9,Post-Release 2.7.0,Adjust version.config and add 2.7.0 to update/downgrade test.
834,Sven Klemm,2022-05-18 05:09:10-05,0a682095632ed4c553703e6d27fc7bad6da71c59,Release 2.7.0,"This release adds major new features since the 2.6.1 release. We deem it moderate priority for upgrading.  This release includes these noteworthy features:  * Optimize continuous aggregate query performance and storage * The following query clauses and functions can now be used in a continuous   aggregate: FILTER, DISTINCT, ORDER BY as well as [Ordered-Set Aggregate](https://www.postgresql.org/docs/current/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE)   and [Hypothetical-Set Aggregate](https://www.postgresql.org/docs/current/functions-aggregate.html#FUNCTIONS-HYPOTHETICAL-TABLE) * Optimize now() query planning time * Improve COPY insert performance * Improve performance of UPDATE/DELETE on PG14 by excluding chunks  This release also includes several bug fixes.  If you are upgrading from a previous version and were using compression with a non-default collation on a segmentby-column you should recompress those hypertables.  **Features** * #4045 Custom origin's support in CAGGs * #4120 Add logging for retention policy * #4158 Allow ANALYZE command on a data node directly * #4169 Add support for chunk exclusion on DELETE to PG14 * #4209 Add support for chunk exclusion on UPDATE to PG14 * #4269 Continuous Aggregates finals form * #4301 Add support for bulk inserts in COPY operator * #4311 Support non-superuser move chunk operations * #4330 Add GUC ""bgw_launcher_poll_time"" * #4340 Enable now() usage in plan-time chunk exclusion  **Bugfixes** * #3899 Fix segfault in Continuous Aggregates * #4225 Fix TRUNCATE error as non-owner on hypertable * #4236 Fix potential wrong order of results for compressed hypertable with a non-default collation * #4249 Fix option ""timescaledb.create_group_indexes"" * #4251 Fix INSERT into compressed chunks with dropped columns * #4255 Fix option ""timescaledb.create_group_indexes"" * #4259 Fix logic bug in extension update script * #4269 Fix bad Continuous Aggregate view definition reported in #4233 * #4289 Support moving compressed chunks between data nodes * #4300 Fix refresh window cap for cagg refresh policy * #4315 Fix memory leak in scheduler * #4323 Remove printouts from signal handlers * #4342 Fix move chunk cleanup logic * #4349 Fix crashes in functions using AlterTableInternal * #4358 Fix crash and other issues in telemetry reporter  **Thanks** * @abrownsword for reporting a bug in the telemetry reporter and testing the fix * @jsoref for fixing various misspellings in code, comments and documentation * @yalon for reporting an error with ALTER TABLE RENAME on distributed hypertables * @zhuizhuhaomeng for reporting and fixing a memory leak in our scheduler"
835,Sven Klemm,2022-05-20 08:34:59-05,26da1b503547a63157e7d974be5b3cdba6e4ac4f,Show warning for caggs with numeric,Postgres changes the internal state format for numeric aggregates which we materialize in caggs in PG14. This will invalidate the affected columns when upgrading from PG13 to PG14. This patch adds a warning to the update script when we encounter this configuration.
836,Sven Klemm,2022-05-22 00:27:01-05,4f58132d37d6a60c3b087d5c34a787228cadbf11,Fix PG13.2 isolation tests,The deadlock_recompress_chunk isolation test uses syntax not supported by earlier versions of the isolation tester leading to a parse failure when processing that file on PG13.2. This patch skips that particular test on PG13.2.
837,Sven Klemm,2022-05-20 14:09:47-05,94ca9c66f420db92ba44384aa10361b39e235b86,Ignore telemetry isolation test on PG13.2,Commit 7b9d8673 added an isolation test for telemetry but did not add it to the ignore list for earlier postgres versions. PG14 changed the output format for isolation tests which got backported to PG12 and PG13 which makes the output of earlier PG12 and PG13 different from the latest one so we ignore isolation tests on those earlier versions.
838,Sven Klemm,2022-05-20 10:54:45-05,aa0b36d5bafd30934a7539aa02d5eebc767d6532,Skip telemetry test on PG12.0,With recent refactorings the telemetry test seems to trigger the same use-after-free bug that got triggered by tablespace test so we skip that test on PG12.0 as well.
839,Sven Klemm,2022-05-20 06:32:02-05,b845f9423b2cb2905d78b67a2f6671a169748b2e,Skip tablespace test on PG12.0,The tablespace test causes a segfault on PG12.0 due to an upstream bug in the event trigger handling.
840,Erik Nordström,2022-05-12 05:53:03-05,7b9d86735861b925def2bff34f58ea7dbc01038d,Fix crash and other issues in telemetry reporter,"Make the following changes to the telemetry reporter background worker:  - Add a read lock to the current relation that the reporter collects   stats for. This lock protects against concurrent deletion of the   relation, which could lead to errors that would prevent the reporter   from completing its report. - Set an active snapshot in the telemetry background process for use   when scanning a relation for stats collection.  - Reopen the scan iterator when collecting chunk compression stats for   a relation instead of keeping it open and restarting the scan. The   previous approach seems to cause crashes due to memory corruption of   the scan state. Unfortunately, the exact cause has not been   identified, but the change has been verified to work on a live   running instance (thanks to @abrownsword for the help with   reproducing the crash and testing fixes).  Fixes #4266"
841,Sven Klemm,2022-05-20 06:04:11-05,46c95c426c909bf0ffef68a86c1b3724aabcd576,Ignore pg_dump test on appveyor,On appveyor the pg_dump is flaky and seems to fail every other time. This patch makes appveyor ignore the result of that test.
842,Sven Klemm,2022-05-20 05:41:29-05,b7472c82ce907490e776d53c76c261db92f855ad,Remove dead code,Remove noop ternary operator in cagg_rebuild_view_definition. We return if finalized is true on line 2475 so the NIL would never be reached in the ternary operator. Found by coverity.
843,Sven Klemm,2022-05-19 13:53:59-05,8c5c7bb4ad6eff5a8c650447c5e7b625389ad40f,Filter out chunk ids in shared tests,Multinode queries use _timescaledb_internal.chunks_in to specify the chunks from which to select data. The chunk id in regresscheck-shared is not stable and may differ depending on execution order leading to flaky tests.
844,Sven Klemm,2022-05-19 07:21:53-05,eab4efa323c760e57e3748dfd89bbcf077ab00ac,Move metrics_dist1 out of shared_setup,The table metrics_dist1 was only used by a single test and therefore should not be part of shared_setup but instead be created in the test that actually uses it. This reduces executed time of regresscheck-shared when that test is not run.
845,Erik Nordström,2022-05-18 04:56:10-05,9b91665162df58e8fe40ca80ad269eed3fe328b2,Fix crashes in functions using AlterTableInternal,"A number of TimescaleDB functions internally call `AlterTableInternal` to modify tables or indexes. For instance, `compress_chunk` and `attach_tablespace` act as DDL commands to modify hypertables. However, crashes occur when these functions are called via `SELECT * INTO FROM <function_name>` or the equivalent `CREATE TABLE AS` statement. The crashes happen because these statements are considered process utility commands and therefore sets up an event trigger context for collecting commands. However, the event trigger context is not properly set up to record alter table statements in this code path, thus causing the crashes.  To prevent crashes, wrap `AlterTableInternal` with the event trigger functions to properly initialize the event trigger context."
846,Dmitry Simonenko,2022-05-19 07:52:12-05,54d6b41e65729ed28cb84f44cb390ba917a228f2,Fix move chunk cleanup logic,"Add a new stage ""complete"" in the ""chunk_copy_operation"" to indicate successful move/copy chunk operations. Make the ""cleanup_copy_chunk_operation"" procedure more robust and make it only delete chunk operation entries from the catalog without doing any other unwanted cleanup if called on successful operations."
847,Jan Nidzwetzki,2022-05-18 11:02:38-05,8375b9aa536a619a5ac2644e0dae3c25880a4ead,Fix a crash in the copy multi-buffer optimization,"This patch solves a crash in the multi-buffer copy optimization, which was introduced in commit bbb2f414d2090efd2d8533b464584157860ce49a.  This patch handles closed chunks (e.g., caused by timescaledb.max_open_ chunks_per_insert) properly. The problem is addressed by:  1) Re-reading the ChunkInsertState before the data is stored, which    ensures that the underlying table is open.  2) A TSCopyMultiInsertBuffer is deleted after the data of the buffer    is flushed. So, operations like table_finish_bulk_insert are    executed and the associated chunk can properly be closed."
848,Sven Klemm,2022-05-19 01:03:50-05,43c8e51510b274a648f6919d183d44cfa6d7ce6d,Fix Var handling for Vars of different level in constify_now,This patch fixes the constify_now optimization to ignore Vars of different level. Previously this could potentially lead to an assertion failure cause the varno of that varno might be bigger than the number of entries in the rangetable. Found by sqlsmith.
849,Sven Klemm,2022-05-18 11:34:55-05,5193af739624224b8ef288ec2c71bf78ab08b6bd,Test attnum stays consistent in update,This patch adds a test for attnum consistency to our update scripts. When attnum between fresh install and updated install is different the updated installation will not be able to correctly process affected catalog tables.
850,Dmitry Simonenko,2022-05-18 10:45:10-05,f1575bb4c31c2ad3d35a361bcf9c991ee7a37911,Support moving compressed chunks between data nodes,This change allows to copy or move compressed chunks between data nodes by including compressed chunk into the chunk copy command stages.
851,Sven Klemm,2022-05-18 05:59:42-05,11c6813b1d56a0d896b430a98493d7b82c6152dd,Fix flaky regresscheck-shared,"While we do filter out chunk ids and hypertable ids from the test output, the output was still unstable when those ids switch between single and double digit as that changes the length of the query decorator in EXPLAIN output. This patch removes this decorator entirely from all shared test output."
852,Fabrízio de Royes Mello,2022-05-11 17:36:58-05,f266f5cf564fcc5509b91493a39eb201c6f5914a,Continuous Aggregates finals form,"Following work started by #4294 to improve performance of Continuous Aggregates by removing the re-aggregation in the user view.  This PR get rid of `partialize_agg` and `finalize_agg` aggregate functions and store the finalized aggregated (plain) data in the materialization hypertable.  Because we're not storing partials anymore and removed the re-aggregation, now is be possible to create indexes on aggregated columns in the materialization hypertable in order to improve the performance even more.  Also removed restrictions on types of aggregates users can perform with Continuous Aggregates: * aggregates with DISTINCT * aggregates with FILTER * aggregates with FILTER in HAVING clause * aggregates without combine function * ordered-set aggregates * hypothetical-set aggregates  By default new Continuous Aggregates will be created using this new format, but the previous version (with partials) will be supported.  Users can create the previous style by setting to `false` the storage paramater named `timescaledb.finalized` during the creation of the Continuous Aggregate.  Fixes #4233"
853,Nikhil Sontakke,2022-05-09 08:16:10-05,ddd02922c9607b4d81185f7c7e6af8d424ea0328,Support non-superuser move chunk operations,"The non-superuser needs to have REPLICATION privileges atleast. A new function ""subscription_cmd"" has been added to allow running subscription related commands on datanodes. This function implicitly upgrades to the bootstrapped superuser and then performs subscription creation/alteration/deletion commands. It only accepts subscriptions related commands and errors out otherwise."
854,Sven Klemm,2022-05-18 03:59:36-05,4988dac27349251591595622baa6180c662d59b2,Fix sqlsmith CI workflow,Commit 3b35da76 changed the setup script for regresscheck-shared to no longer be usable directly by the sqlsmith workflow. This patch set TEST_DBNAME at the top of the script so it is easier to use the script outside of regression check environment.
855,Sven Klemm,2022-05-17 21:14:51-05,747b532be6bfa3e93decdcb6f89c9432e5e58b48,Bump pg version in update test,"Bump PG versions to 12.11, 13.7 and 14.3 in update tests. The update test was skipped with the previous PG version bump because upstream docker images were not yet available which we rely on in the update test."
856,Sven Klemm,2022-05-18 00:26:03-05,d1ed09ce98d5f9a6a849e681536ebdc0ef168981,Fix build failure on PG12.0,Building against PG12.0 failed with test_utils.c:99:5: error: expected ‘;’ before ‘errmsg’ because the errmsg argument was not in parens.
857,Sven Klemm,2022-05-16 06:59:26-05,35ea80ffdffcdcf3c81af7881c509f33bbeb9924,Enable now() usage in plan-time chunk exclusion,"This implements an optimization to allow now() expression to be used during plan time chunk exclusions. Since now() is stable it would not normally be considered for plan time chunk exclusion. To enable this behaviour we convert `column > now()` expressions into `column > const AND column > now()`. Assuming that time always moves forward this is safe even for prepared statements. This optimization works for SELECT, UPDATE and DELETE. On hypertables with many chunks this can lead to a considerable speedup for certain queries.  The following expressions are supported: - column > now() - column >= now() - column > now() - Interval - column > now() + Interval - column >= now() - Interval - column >= now() + Interval  Interval must not have a day or month component as those depend on timezone settings.  Some microbenchmark to show the improvements, I did best of five for all of the queries.  -- hypertable with 1k chunks -- with optimization select * from metrics1k where time > now() - '5m'::interval; Time: 3.090 ms  -- without optimization select * from metrics1k where time > now() - '5m'::interval; Time: 145.640 ms  -- hypertable with 5k chunks -- with optimization select * from metrics5k where time > now() - '5m'::interval; Time: 4.317 ms  -- without optimization select * from metrics5k where time > now() - '5m'::interval; Time: 775.259 ms  -- hypertable with 10k chunks -- with optimization select * from metrics10k where time > now() - '5m'::interval; Time: 4.853 ms  -- without optimization select * from metrics10k where time > now() - '5m'::interval; Time: 1766.319 ms (00:01.766)  -- hypertable with 20k chunks -- with optimization select * from metrics20k where time > now() - '5m'::interval; Time: 6.141 ms  -- without optimization select * from metrics20k where time > now() - '5m'::interval; Time: 3321.968 ms (00:03.322)  Speedup with 1k chunks: 47x Speedup with 5k chunks: 179x Speedup with 10k chunks: 363x Speedup with 20k chunks: 540x"
858,Sven Klemm,2022-05-14 00:20:22-05,01c6125de66b142a4a7c676c10c04a182c5f8575,Reduce repetitions in workflows,Derive postgres major version from full version so we don't have to specify both in matrix.
862,Konstantina Skovola,2022-05-16 03:51:23-05,79d1d3cb3eec221bfe0fe52d9fd5a70c7369d45f,"Add GUC ""bgw_launcher_poll_time""",This GUC permits configuring the TIMEOUT parameter for the background worker launcher in the loader.  Fixes #4217
863,Fabrízio de Royes Mello,2022-05-16 09:47:08-05,047d6b175bac4839e90861d25fc25b1446b54bd2,"Revert ""Pushdown of gapfill to data nodes""",This reverts commit eaf3a38fe9553659e515fac72aaad86cf1a06d1e.
864,Fabrízio de Royes Mello,2022-05-16 09:39:53-05,4083e48a1cf62b99e0bc6e40e1ecd454ef5752ec,"Revert ""Add missing gitignore entry""",This reverts commit 57411719fb1f5e4d5863089bb4b840abea3bc3db.
865,Fabrízio de Royes Mello,2022-05-16 09:35:36-05,557023c3a7c5edf6e45c0f6003b75796fdab4b7a,"Revert ""Ignore flaky ""dist_gapfill_pushdown"" test""",This reverts commit cf83b3b9c2d899fa0156d5c94dc4848b7584e3c9.
866,Alexander Kuzmenkov,2022-05-11 11:15:32-05,3b35da7607072de2d7e8fa29eca80a6258bb6026,More tests for errors when fetching from data nodes,Add a special function that allows to inject these errors.
867,Jan Nidzwetzki,2022-05-16 01:06:43-05,08a3fca2879a962187ff16709ed6ffadb1d2c057,PostgreSQL license file reference added,This patch adds a reference to the PostgreSQL license file.
868,Sven Klemm,2022-05-15 09:29:31-05,16d4b3e0b2570622a546625c9a86b9b1752b91b8,Restructure planner code,This patch moves the apache licensed planner code into it's own subdirectory.
869,Alexander Kuzmenkov,2022-05-12 13:58:12-05,c6c64c4021bb475cb488e0c2043b0a8df40baf7b,Remove unused memory allocation in tuplefactory,"It's in a temporary context, so not a memory leak, but it's just not needed.  In passing, clean up the tuple factory error handling code, again."
870,Nikhil Sontakke,2022-05-05 08:27:35-05,92f7e5d36164c1fd955c8b9ef5ed5cd4837ae026,Support op_id in copy/nove chunk API,"Allow users to specify an explicit ""operation_id"" while carrying out a copy/move operation. If it's specified then that is used as the identifier for the copy/move operation. Otherwise, am implicit id as before gets created and used."
871,Mats Kindahl,2022-05-12 07:28:03-05,1d504d5153e757207dc57441fa93a96a0b2a4957,Remove printouts from signal handlers,"In #4199 existing calls of `ereport` were replaced with calls of `write_stderr` to eliminate the use of signal-unsafe calls, in particular calls to `malloc`. Unfortunately, `write_stderr` contains a call to `vfprintf`, which allocates memory as well, occationally causing servers that are shutting down to become unresponsive.  Since the existing signal handlers just called `die` after printing out a useful message, this commit fixes this by using `die` as a signal handler.  Fixes #4200"
872,Fabrízio de Royes Mello,2022-05-12 08:05:10-05,356dfa8eb1df807e20af7691d4c0dcb5312f3be5,Refactor upgrade/downgrade setup sql test scripts,The setup scripts for upgrade/downgrade tests of Continuous Aggregates has too many duplicated code for pre-2.0 tests. Refactor it a bit removing the duplicated code by using `\if \else \endif` psql meta-commands.  Also added a properly `round` function to all functions that returns `float8` in SQL scripts  because in rare cases it lead to flaky tests.  This is part of #4269.
874,Sven Klemm,2022-05-12 00:56:57-05,efce68ce0de1789b928306933495a9fe731d5c0a,Remove _PG_fini functions,_PG_fini is meant to be the hook to run code on module unload but this code is not working and has not been working for over 12 years because upstream does not support it.  https://github.com/postgres/postgres/commit/ab02d702ef08343fba30d90fdf7df5950063e8c9
875,Jan Nidzwetzki,2022-05-05 02:28:00-05,bbb2f414d2090efd2d8533b464584157860ce49a,Multi-buffer copy optimization backport,"This commit backports the Postgres multi-buffer / bulk insert optimization into the timescale copy operator. If the target chunk allows it (e.g., if no triggers are defined on the hypertable or the chunk is not compressed), the data is stored in in-memory buffers first and then flushed to the chunks in bulk operations.  Implements: #4080"
876,gayyappan,2022-04-06 10:57:33-05,5d56b1cdbcff50c868b067f156d6a7a77d7dda6f,Add api _timescaledb_internal.drop_chunk,Add an internal api to drop a single chunk. This function drops the storage and metadata associated with the chunk. Note that chunk dependencies are not affected. e.g. Continuous aggs are not updated when this chunk is dropped.
878,Alexander Kuzmenkov,2022-05-11 02:25:09-05,c73c5a74b9d40491958c069fd2d6f671845afadd,Avoid extra projection when scanning a compressed chunk,"We don't have to project the result of the underlying scan of DecompressChunk, because the latter can selectively decompress the resulting columns anyway. The projection slows down the things especially when we use JIT, when the projection has to be compiled for each chunk.  To avoid this unnecessary work, use the physical targetlist for compressed scan if we can. This commit rewrites the function that determines what we have to decompress, and makes it use the targetlist generated by the planner, instead of building it from scratch. Then we can give it the physical targetlist when possible."
879,Sven Klemm,2022-05-10 18:56:37-05,a0e25d9e2447abf17984dd12e5ca6ba04c68f475,Remove Ubuntu 21.04 from packages tests,Ubuntu 21.04 is EOL since january so we no longer need to support and test it.
880,gayyappan,2022-04-07 16:10:47-05,9f4dcea30135d1e36d1c452d631fc8b8743b3995,Add _timescaledb_internal.freeze_chunk API,"This is an internal function to freeze a chunk for PG14 and later.  This function sets a chunk status to frozen. Operations that modify the chunk data (like insert, update, delete) are not supported. Frozen chunks can be dropped.  Additionally, chunk status is cached as part of classify_relation."
881,Fabrízio de Royes Mello,2022-05-09 08:37:30-05,e81e32fe5c56c39b67f2b24942deed26c0552388,Telemetry Stats for CAggs finals form,Introduced by #4294 and #4269 PRs the default implementation of Continuous Aggregates get rid of `chunk_id` in the materialization hypertable and `partialize_agg`/`finalize_agg` aggregate functions.  A new counter named `num_caggs_finalized` was added to telemetry report in this PR to count the number of Continuos Aggregates created in this new format.
882,Sven Klemm,2022-05-08 23:41:22-05,bb241ffcd18b9f3c279e35a0873e0d91f82670c0,Add missing ignores for dist_gapfill_pushdown,The patch to ignore result of dist_gapfill_pushdown missed some places that overwrite installcheck_args.
883,Sven Klemm,2022-05-08 23:28:50-05,958489fedbae1905eacbde18e3ea6a2b55358921,Disable downgrade test for ubuntu jammy,ubuntu jammy only has 1 timescaledb version packaged at the moment so downgrade is not possible.
884,Fabrízio de Royes Mello,2022-04-29 14:03:36-05,1e8d37b54ebfed36ff117f208064f37c8916a0f3,Remove `chunk_id` from materialization hypertable,First step to remove the re-aggregation for Continuous Aggregates is to remove the `chunk_id` from the materialization hypertable.  Also added new metadata column named `finalized` to `continuous_cagg` catalog table in order to store information about the new following finalized version of Continuous Aggregates that will not need the partials anymore. This flag is important to maintain backward compatibility with previous Continuous Aggregate implementation that requires the `chunk_id` to refresh data properly.
908,Alexander Kuzmenkov,2022-04-21 05:27:26-05,eeb9d133170140540eb41ee8dfb54ebc769b7781,Avoid compiling extra projection when constifying the tableoid column,"If we didn't constify anything, no need to recompile the projection. It can be costly if we're using JIT."
885,Alexander Kuzmenkov,2022-04-25 09:53:16-05,6e26a1187a1a6c89156deb387ab425a8b10866bb,Use binary format in row-by-row fetcher,"The general idea is to have two types of fetcher: ""fast"" and ""general purpose"". We use the row-by-row fetcher as the ""fast"" one. This commit removes support of text protocol in this fetcher, because it's only needed for some niche types that don't have a binary serialization, and is also slower than binary one. Because the row-by-row fetcher now only understands binary protocol, we must check that the binary serialization is actually available for the participating data types. If not, we have to revert to using the cursor fetcher unless row-by-row was explicitly requested by the user. This happens at execution time, precisely, at creation of TupleFactory, because that's when we look up the conversion functions.  The rest of the commit is removing the text protocol support from row-by-row, plus EXPLAIN changes (we don't know the fetcher type at the planning stage anymore, so not showing it)."
887,Konstantina Skovola,2022-05-04 10:44:44-05,b48a727b77d306f8298479ca19b6810297faaaf7,Fix refresh window cap for cagg refresh,Fixes #4252
888,Sven Klemm,2022-05-04 07:19:58-05,e7076f3be1dbd228b8dafe75e1e90ada6c18b667,Remove unused function prototypes,Commit 01c724b9 added some prototypes to extension.h but did not add an implementation. This patch removes the unused prototypes.
889,Alexander Kuzmenkov,2022-05-02 10:52:55-05,9b5b86c490bf314b9a709a5e0cdbc9fe525ca444,Make the test remote_copy stable by adding ORDER BYs,Currently it depends on the (unspecified) order in which the original rows were inserted.
891,Konstantina Skovola,2022-04-29 03:44:00-05,cf83b3b9c2d899fa0156d5c94dc4848b7584e3c9,"Ignore flaky ""dist_gapfill_pushdown"" test","This is a *temporary* change to our workflows to ignore the test because it is flaky and requires many jobs to be restarted before a commit can be merged.  Does not address the issue that causes the flakiness, which needs to be investigated."
892,Sven Klemm,2022-05-02 00:16:55-05,a4081516cae24f17eeb6241c758d84d90db3d4f6,Append pg_temp to search_path,Postgres will prepend pg_temp to the effective search_path if it is not present in the search_path. While pg_temp will never be used to look up functions or operators unless explicitly requested pg_temp will be used to look up relations. Putting pg_temp in search_path makes sure objects in pg_temp will be considered last and pg_temp cannot be used to mask existing objects.
893,Alexander Kuzmenkov,2022-04-28 04:08:37-05,9012e2a20d16d5219dde6260127c39cc748b8f87,Do not create a memory context for each Chunk,"For some reason, we create a MemoryContext for each Chunk. This context then is almost never used. Just don't do this."
894,Sven Klemm,2022-04-28 17:15:45-05,4f67ed0656be4168b43b2bb465b5c1c426b1853e,Fix unqualified type reference in time_bucket,This was not caught earlier and is not currently caught by CI because the check for unqualified casts is currently only in main branch of pgspot and not yet in the tagged version we use as part of PR checks.
895,Alexander Kuzmenkov,2022-04-22 06:15:18-05,97078dde057cf24f0e93176cf72806d3321a2294,Enable DecompressChunk node to skip some input columns,"No functional change.  This refactoring adds explicit output attnos to the metadata of column decompression, enabling DecompressChunk node to skip some input columns of the compressed scan. This is ultimately needed to support physical targetlists in compressed scan."
896,Konstantina Skovola,2022-04-21 06:22:58-05,687e7c7233d898c08aa3b1a461b0b7296beb0a3e,"Fix option ""timescaledb.create_group_indexes""","Previously this option was ignored when creating a continuous aggregate, even when explicitly set to true.  Fixes #4249"
897,gayyappan,2022-04-20 12:57:43-05,3e042bd9dc3389bd476e4f9a1bd9c14571b438e7,Fix compress_chunk error message,"When we have a hypertable with a cagg defined on it, a call to the compress_chunk with the hypertable's chunk returns an unexpected status error. Fix the error message to return ""compression not enabled"""
898,Mats Kindahl,2022-04-22 12:10:45-05,aaffc1d5a69e5242067dd7ce81e5096e3765ec85,Set null vector for insert into compressed table,"As part of inserting into a compressed table, the tuple is materialized, which computes the data size for the tuple using `heap_compute_data_size`. When computing the data size of the tuple, columns that are null are not considered and are just ignored. Columns that are dropped are, however, not explicitly checked and instead the `heap_compute_data_size` rely on these columns being set to null.  When reading tuples from a compressed table for insert, the null vector is cleared, meaning that it by default is non-null. Since columns that are dropped are not explicitly processed, they are expected to have a defined value, which they do not have, causing a crash when an attempt to dereference them are made.  This commit fixes this by setting the null vector to all null, and the code after will overwrite the columns with proper null bits, except the dropped columns that will be considered null.  Fixes #4251"
899,Mats Kindahl,2022-04-25 06:36:13-05,e3f0fbfeff052c72897c527f6cb71c5e83ee4a39,Disable 001_extension test on PG12,"PostgreSQL code for PG12 does not have the method `background_psql` available in `PostgresNode` class for use in TAP testing, causing the test `001_extension` to fail.  Fixes #4226"
900,Sven Klemm,2022-04-23 14:34:22-05,9fda658075d28b83a5f77fa523f66281648643ba,Fix flaky remote_connection test,"Postgres changed the connection error reporting in the stable branches of PG12, PG13 and PG14 leading to our snapshot tests always failing. This patch removes the checks that now fail with the upstream change.  https://github.com/postgres/postgres/commit/ae27b1ac"
902,Alexander Kuzmenkov,2022-04-21 03:22:31-05,730a72880ee4af60b1380c8bd145800fa0d5b133,NFC. Don't add whole row var to compressed scan tlist,No functional change.  Construction a whole row var with compressed columns doesn't make sense. It has to be constructed from decompressed columns by projection of DecompressChunk node.
903,Sven Klemm,2022-04-21 07:05:36-05,368e9bb702574a9fc4de9dbdf712432e12668b64,Add pgspot to CI,Add pgspot to check our installation scripts for following best practices.
904,Sven Klemm,2022-04-22 02:34:31-05,d137c1aa831b9178cae94d3a91b41ab2a3601f7d,Fix unsafe procedure creation in update script,post_update_cagg_try_repair was created with `CREATE OR REPLACE` instead of CREATE. Additionally the procedure was created in the public schema. This patch adjusts the procedure to be created with `CREATE` and in a timescaledb internal schema. Found by pgspot.
905,Sven Klemm,2022-04-22 04:11:16-05,ebd232c4fc64c1d59fa75b0707a9317bf0a00ef9,Ignore spelling fixes in git blame,This patch adds the spelling fix commit to the git blame ignore list and adds a thank you to the changelog for the author. The git blame change couldnt be done in the spelling PR because it references the commit hash.
906,Josh Soref,2022-03-14 10:11:06-05,68aec9593c0f37dddbaa4f2e2b34a9ba3f5b11d9,Fix various misspellings,"This patch fixes various misspellings of committed, constraint and insufficient in code, comments and documentation."
907,Fabrízio de Royes Mello,2022-04-20 14:17:14-05,b0695e788a7589189b334fb4c72a1176c33ba326,Fix flaky distributed grant tests,Commit 9b180915 introduce some tests that are a bit flaky because are checking a DEBUG message from Postgres catcache rehashing.  Fixed it by teaching our test runner to ignore this Postgres DEBUG message.
909,Sven Klemm,2022-04-20 21:03:45-05,a2c39b9afedc705513725a9a56b2c5c832e6e13f,Fix logic bug in init_privs query,The query to get the list of saved privileges during extension upgrade had a bug and only applying the classoid restriction for a subset of the entries when it should have been applied to all returned rows leading to a failure during extension update when init privileges for other classoids existed on any of the relevant objects.
910,Sven Klemm,2022-04-19 01:49:12-05,fca865ced948b8cd605fc8dad55a2d0311984b1e,Mark hypertable parent as dummy rel for UPDATE,When postgres expands an inheritance tree it also adds the parent hypertable as child relation. Since for a hypertable the parent will never have any data we can mark this relation as dummy relation so it gets ignored in later steps. This is only relevant for code paths that use the postgres inheritance code as we don't include the hypertable as child when expanding the hypertable ourself.  This is similar to 3c40f924 which did the same adjustment for DELETE.  This patch also moves the marking into get_relation_info_hook so it happens a bit earlier and prevents some additional cycles.
911,Sven Klemm,2022-04-20 06:45:12-05,6d40c30d10ef43e8428bd6b3a1415bc5c282ea3a,Fix DELETE statement trigger on hypertables for PG14,Commit 3c40f924 accidently broke DELETE statement triggers on PG14 that were only defined on the hypertable itself. This patch fixes the issue and also makes the trigger test no longer pg version specific.
912,Sven Klemm,2022-04-18 21:45:31-05,0f70ae87bfb32122e94978b1d12ccb0ccea58ff4,Fix flaky truncate test,Change truncate test to ignore warnings about potentially orphaned files when dropping the test database. This seems to happen quite frequently on appveyor causing the test to be flaky.
913,Sven Klemm,2022-04-18 06:08:00-05,3c40f924bedbffeb32236c51d0aecdcaa442368a,Mark hypertable parent as dummy rel,When postgres expands an inheritance tree it also adds the parent hypertable as child relation. Since for a hypertable the parent will never have any data we can mark this relation as dummy relation so it gets ignored in later steps. This is only relevant for code paths that use the postgres inheritance code as we don't include the hypertable as child when expanding the hypertable ourself.
914,Alexander Kuzmenkov,2022-04-14 11:13:09-05,0ab2d39f25d75bec08e724acc9ee5a457b51882e,Set correct collation for segmentby columns of compressed chunks,"We don't do this currently, so some queries return the wrong ordering of rows if there is an index on the compressed chunk. The fix only works for the newly created chunks. We could add a migration that corrects the old compressed chunks, but it seems to be too heavy and not to lend itself well to automation -- we'll have to recreate the indexes if there are any. So the old chunks continue to return a wrong result."
915,Sven Klemm,2022-04-17 12:51:30-05,472a68726c04f608421626dfef239be199e8b380,Make hook prefixes consistent,Replace the timescale_ prefix on hooks to timescaledb_ because the latter is the correct prefix and to be consistent with names of our other hooks.
916,Sven Klemm,2022-04-17 12:47:17-05,fae7e7cfdc51bf0fb5487e96c1a2029b0dca9546,Don't export local reorder function,Change reorder_rel function to not be exported and also removes the timescale_ function name prefix from local functions.
917,Markos Fountoulakis,2022-04-18 03:31:01-05,fab16f3798d7219532f9c5a6587b52073b0489d5,Fix segfault in Continuous Aggregates,"Add the missing variables to the finalization view of Continuous Aggregates and the corresponding columns to the materialization table. Cover the case of targets that contain Aggref nodes and Var nodes that are outside of the Aggref nodes at the same time.  Stop rebuilding the Continuous Aggregate view with ALTER MATERIALIZED VIEW. Attempt to repair the view at post-update time instead, and fail gracefully if it is not possible to do so without raw hypertable schema or data modifications.  Stop rebuilding the Continuous Aggregate view when switching realtime aggregation on and off. Instead, manipulate the User View by either:   1. removing the UNION ALL right-hand side and the WHERE clause when      disabling realtime aggregation   2. adding the Direct View to the right of a UNION ALL operator and      defining WHERE clauses with the relevant watermark checks when      enabling realtime aggregation  Fixes #3898"
918,Markos Fountoulakis,2022-04-14 07:19:49-05,f3ad916912e640b2a3eb006c2a0d49783f63c8c9,Fix Coverity defect 376602,Fix the potential NULL pointer dereference in ts_dimension_slice_scan_iterator_get_by_id().
919,Sven Klemm,2022-04-17 16:18:54-05,ca6122e01a12938a4049b193aa422c3477008a7c,Fix downgrade test git failure,Due to a security vulnerability in git recent git versions now check ownership of files and error out if it doesnt match current user. Since we bind mount the source checkout into the build container the user of the checkout is unlikely to match the user inside the container. This patch configures git to skip the owner check for the bind-mounted directory.  https://github.blog/2022-04-12-git-security-vulnerability-announced/
920,Fabrízio de Royes Mello,2022-04-08 15:10:12-05,bb6dc173a0e23981404746a032e487cfd16f73fb,Set PG14 as default scripts version,Set latest PG14 as the default Postgres version for scripts and some minor fixes.
922,Konstantina Skovola,2022-04-08 06:59:58-05,731a39a1226e31efb2d1715156ba2ea2cade3195,Fix TRUNCATE error as non-owner on hypertable,"Stop throwing error ""must be owner of hypertable"" when a user with TRUNCATE privilege on the hypertable attempts to TRUNCATE.  Previously we had a check that required TRUNCATE to only be performed by the table owner, not taking into account the user's TRUNCATE privilege, which is sufficient to allow this operation.  Fixes #4183"
923,Fabrízio de Royes Mello,2022-04-08 15:01:46-05,57411719fb1f5e4d5863089bb4b840abea3bc3db,Add missing gitignore entry,Pull request #4033 introduced a new template SQL test file but missed to add the properly gitgnore entry to ignore generated test files.
924,Rafia Sabih,2022-04-05 06:02:29-05,4d47271ad3c365a87c74b911ab16bdf13c0846bd,2.6.1 (2022-04-11) This release is patch release. We recommend that you upgrade at the next available opportunity.,**Bugfixes** * #3974 Fix remote EXPLAIN with parameterized queries * #4122 Fix segfault on INSERT into distributed hypertable * #4142 Ignore invalid relid when deleting hypertable * #4159 Fix ADD COLUMN IF NOT EXISTS error on compressed hypertable * #4161 Fix memory handling during scans * #4186 Fix owner change for distributed hypertable * #4192 Abort sessions after extension reload * #4193 Fix relcache callback handling causing crashes  **Thanks** * @abrownsword for reporting a crash in the telemetry reporter * @daydayup863 for reporting issue with remote explain
925,Rafia Sabih,2022-01-28 03:00:11-06,eaf3a38fe9553659e515fac72aaad86cf1a06d1e,Pushdown of gapfill to data nodes,"Allow the calls of time_bucket_gapfill to be executed at the data nodes for improved query performance. With this, time_bucket_gapfill is pushed to data nodes in the following conditions,  1. when only one data node has all the chunks 2. when space dimension does not overlap across data nodes 3. when group-by matches space dimension"
955,Erik Nordström,2022-03-14 07:47:43-05,f00bdadf0c4d7465411fe7f349a19953ca401498,Trigger Sqlsmith tests manually or by push to branch,"Add workflow events to allow manually running Sqlsmith tests or when pushing to the 'sqlsmith' branch. This is useful when submitting PRs that one wants to run extra checks on, including Sqlsmith."
926,Mats Kindahl,2022-04-06 10:30:18-05,1b2926c07626280245efddde3fe4103c79d85a2e,Do not modify aggregation state in finalize,"The function `tsl_finalize_agg_ffunc` modified the aggregation state by setting `trans_value` to the final result when computing the final value. Since the state can be re-used several times, there could be several calls to the finalization function, and the finalization function would be confused when passed a final value instead of a aggregation state transition value.  This commit fixes this by not modifying the `trans_value` when computing the final value and instead just returns it (or the original `trans_value` if there is no finalization function).  Fixes #3248"
927,Sven Klemm,2022-04-06 04:04:58-05,ae50a534852277dc0e2affaaeeedcdac7203b41e,Add chunk exclusion for UPDATE for PG14,"Currently only IMMUTABLE constraints will exclude chunks from an UPDATE plan, with this patch STABLE expressions will be used to exclude chunks as well. This is a big performance improvement as chunks not matching partitioning column constraints don't have to be scanned for UPDATEs. Since the codepath for UPDATE is different for PG < 14 this patch only adds the optimization for PG14.  With this patch the plan for UPDATE on hypertables looks like this:   Custom Scan (HypertableModify) (actual rows=0 loops=1)    ->  Update on public.metrics_int2 (actual rows=0 loops=1)          Update on public.metrics_int2 metrics_int2_1          Update on _timescaledb_internal._hyper_1_1_chunk metrics_int2          Update on _timescaledb_internal._hyper_1_2_chunk metrics_int2          Update on _timescaledb_internal._hyper_1_3_chunk metrics_int2          ->  Custom Scan (ChunkAppend) on public.metrics_int2 (actual rows=0 loops=1)                Output: '123'::text, metrics_int2.tableoid, metrics_int2.ctid                Startup Exclusion: true                Runtime Exclusion: false                Chunks excluded during startup: 3                ->  Seq Scan on public.metrics_int2 metrics_int2_1 (actual rows=0 loops=1)                      Output: metrics_int2_1.tableoid, metrics_int2_1.ctid                      Filter: (metrics_int2_1.""time"" = length(version()))"
928,Alexander Kuzmenkov,2022-04-04 04:50:22-05,ff945a7a9449ee3b2b6c1b7cb78f8fa87dbdaa4e,Data node scan doesn't support system columns: move this check to an appropriate place,"Before, we would complain that we don't support fetching the system columns with per-data node queries enabled, but still execute the code that fetches it. Don't do this and complain earlier."
929,Konstantina Skovola,2022-04-04 06:23:58-05,a064fd3b483abdd01d1d1b6f86b6a4cc1e6e6b98,Add logging for retention policy,Also remove unused code from compression_api. The function policy_compression_get_verbose_log was unused. Moved it to policy_utils and renamed to policy_get_verbose_log so that it can be used by all policies.
931,Dmitry Simonenko,2022-04-04 01:40:36-05,a4b151b024ea6398595041dbb2660f27c3465790,Fix owner change for distributed hypertable,Allow ALTER TABLE OWNER TO command to be used with distributed hypertable.  Fix #4180
932,Alexander Kuzmenkov,2022-03-31 09:49:31-05,4ee8872177908f3ae48be1491cb098fae577e54d,Use virtual tuples in row-by-row fetcher,"We needlessly form/deform the heap tuples currently. Sometimes we do need this when we have row marks and need a ctid (UPDATE RETURNING), but not in this case. The implementation has three parts:  1. Change data fetcher interface to store a tuple into given slot instead of returning a heap tuple.  2. Expose the creation of virtual tuple in tuple factory.  3. Use these facilities in row-by-row fetcher.  This gives some small speedup. It will become more important in the future, as other parts of row-by-row fetcher are optimized."
933,Sven Klemm,2022-04-01 02:51:39-05,e16908ccd73f212ce75ed646d8fa9b72a37c65bc,Ignore bulk formatting changes in git blame,This patch adds a .git-blame-ignore-revs that contains a list of commits with bulk formatting changes to be ignored with git blame. This file will be used by GitHub but to use it locally you need to tell git about it eg. with the following command: `git config blame.ignoreRevsFile .git-blame-ignore-revs`
934,Konstantina Skovola,2022-03-31 15:26:55-05,a154ae56e969a73e1431d6c0a1cac4989f4163ac,Fix ADD COLUMN IF NOT EXISTS error on compressed hypertable,"Stop throwing exception with message ""column of relation already exists"" when running the command ALTER TABLE ... ADD COLUMN IF NOT EXISTS ... on compressed hypertables.  Fix #4087"
935,Erik Nordström,2022-03-25 08:48:35-05,972afe0096149001d204e87f8cc6addea008df52,Add TAP tests for extension state,"Add a TAP test that checks that the extensions state is updated across concurrent sessions/backends when the extension is ""dropped"" or ""created""."
936,Erik Nordström,2022-03-24 03:31:52-05,01c724b9be2ca0eb04d30b043152f051c9b7ea36,Fix relcache callback handling causing crashes,"Fix a crash that could corrupt indexes when running VACUUM FULL pg_class.  The crash happens when caches are queried/updated within a cache invalidation function, which can lead to corruption and recursive cache invalidations.  To solve the issue, make sure the relcache invalidation callback is simple and never invokes the relcache or syscache directly or indirectly.  Some background: The extension is preloaded and thus have planner hooks installed irrespective of whether the extension is actually installed or not in the current database. However, the hooks need to be disabled as long as the extension is not installed. To avoid always having to dynamically check for the presence of the extension, the state is cached in the session.  However, the cached state needs to be updated if the extension changes (altered/dropped/created). Therefore, the relcache invalidation callback mechanism is (ab)used in TimescaleDB to signal updates to the extension state across all active backends.  The signaling is implemented by installing a dummy table as part of the extension and any invalidation on the relid for that table signals a change in the extension state. However, as of this change, the actual state is no longer determined in the callback itself, since it requires use of the relcache and causes the bad behavior. Therefore, the only thing that remains in the callback after this change is to reset the extension state.  The actual state is instead resolved on-demand, but can still be cached when the extension is in the installed state and the dummy table is present with a known relid. However, if the extension is not installed, the extension state can no longer be cached as there is no way to signal other backends that the state should be reset when they don't know the dummy table's relid, and cannot resolve it from within the callback itself.  Fixes #3924"
952,Fabrízio de Royes Mello,2022-03-14 12:10:14-05,332dffeebc72cf16afbde802e2f19aead9bd7836,Rename `master` branch to `main`,Following what many communities already did we agreed in renaming the `master` branch to `main`.  Resources: - https://sfconservancy.org/news/2020/jun/23/gitbranchname/ - https://postgr.es/m/20200615182235.x7lch5n6kcjq4aue@alap3.anarazel.de  Closes #4163
953,Sven Klemm,2022-03-15 09:22:14-05,077b2edbc556bbcfb104992effdb6fe6dd89cc3b,Change ChunkAppend file organization,"This patch changes the organization of the ChunkAppend code. It removes all header files except chunk_append/chunk_append.h. It also merges exec.c and explain.c to remove unnecessary function exports, since the code from explain.c was only used by exec.c"
956,Sven Klemm,2022-03-14 05:44:05-05,ab6b90caff19e0193ceb58ba298280daf03a6adb,Reference CVE ID in CHANGELOG,The CVE ID was already referenced in the commit introducing the fix but not in the CHANGELOG.
937,Mats Kindahl,2022-03-24 06:46:22-05,9c46a5d5c6e2d5020db5c07de52b2fc15e0e4131,Abort sessions after extension reload,"If a session is started and loads (and caches, by OID) functions in the extension to use them in, for example, a `SELECT` query on a continuous aggregate, the extension will be marked as loaded internally.  If an `ALTER EXTENSION` is then executed in a separate session, it will update `pg_extension` to hold the new version, and any other sessions will see this as the new version, including the session that already loaded the previous version of the shared library.  Since the pre-update session has loaded some functions from the old version already, running the same queries with the old named functions will trigger a reload of the new version of the shared library to get the new functions (same name, but different OID), but since this has already been loaded in a different version, it will trigger an error that GUC variables are re-defined.  Further queries after that will then corrupt the database causing a crash.  This commit fixes this by recording the version loaded rather than if it has been loaded and check that the version did not change after a query has been analyzed (in the `post_analyze_hook`). If the version changed, it will generate a fatal error to force an abort of the session.  Fixes #4191"
939,Mats Kindahl,2022-03-29 11:16:11-05,81b71b685ce56388e0851c4362de36781cfcc646,Remove signal-unsafe calls from signal handlers,Functions `elog` and `ereport` are unsafe to use in signal handlers since they call `malloc`. This commit removes them from signal handlers.  Fixes #4200
940,Sven Klemm,2022-01-31 17:32:51-06,347b45f109e0a7ce642377715b931fe900333e2f,Add chunk exclusion for DELETE for PG14,"Currently only IMMUTABLE constraints will exclude chunks from a DELETE plan, with this patch STABLE expressions will be used to exclude chunks as well. This is a big performance improvement as chunks not matching partitioning column constraints don't have to be scanned for DELETEs. Additionally this improves usability of DELETEs on hypertables with some chunks compressed. Previously you weren't able to do DELETE on those hypertables which had non-IMMUTABLE constraints. Since the codepath for DELETE is different for PG < 14 this patch only adds the optimization for PG14.  With this patch the plan for DELETE on hypertables looks like this:   Custom Scan (HypertableModify) (actual rows=0 loops=1)    ->  Delete on metrics (actual rows=0 loops=1)          Delete on metrics metrics_1          Delete on _hyper_5_8_chunk metrics          Delete on _hyper_5_11_chunk metrics          Delete on _hyper_5_12_chunk metrics          Delete on _hyper_5_13_chunk metrics          Delete on _hyper_5_14_chunk metrics_2          ->  Custom Scan (ChunkAppend) on metrics (actual rows=1 loops=1)                Chunks excluded during startup: 4                ->  Seq Scan on metrics metrics_1 (actual rows=0 loops=1)                      Filter: (""time"" > (now() - '3 years'::interval))                ->  Bitmap Heap Scan on _hyper_5_14_chunk metrics_2 (actual rows=1 loops=1)                      Recheck Cond: (""time"" > (now() - '3 years'::interval))                      Heap Blocks: exact=1                      ->  Bitmap Index Scan on _hyper_5_14_chunk_metrics_time_idx (actual rows=1 loops=1)                            Index Cond: (""time"" > (now() - '3 years'::interval))"
941,Alexander Kuzmenkov,2022-03-09 10:31:28-06,935684c83a4de3f82dabf1d2aa0ae7bae3a18ca2,Cache whether a rel is a chunk in classify_relation,Use a per-query hash table for this. This speeds up the repeated calls to classify_relation by avoiding the costly chunk lookup.
942,Alexander Kuzmenkov,2022-03-15 10:41:40-05,ae79ba6eb4d13a2f4bd6140bc2db5ea8e192385c,Scan less chunk metadata when planning ForeignModify,"Instead of loading the entire Chunk struct, just look up the data nodes."
944,Erik Nordström,2022-02-22 12:03:43-06,c1cf067c4f63c5a40fd61a1782b3422d11336090,Improve restriction scanning during hypertable expansion,"Improve the performance of metadata scanning during hypertable expansion.  When a hypertable is expanded to include all children chunks, only the chunks that match the query restrictions are included. To find the matching chunks, the planner first scans for all matching dimension slices. The chunks that reference those slices are the chunks to include in the expansion.  This change optimizes the scanning for slices by avoiding repeated open/close of the dimension slice metadata table and index.  At the same time, related dimension slice scanning functions have been refactored along the same line.  An index on the chunk constraint metadata table is also changed to allow scanning on dimension_slice_id. Previously, dimension_slice_id was the second key in the index, which made scans on this key less efficient."
945,Nikhil Sontakke,2022-03-17 03:32:31-05,966c5eb2c2007374371b3880a33c06d8446a2724,Fix remote EXPLAIN with parameterized queries,"In certain multi-node queries, we end up using a parameterized query on the datanodes. If ""timescaledb.enable_remote_explain"" is enabled we run an EXPLAIN on the datanode with the remote query. EXPLAIN doesn't work with parameterized queries. So, we check for that case and avoid invoking a remote EXPLAIN if so.  Fixes #3974  Reported and test case provided by @daydayup863"
946,Sven Klemm,2022-03-18 10:24:40-05,e101b3ea60760f1091fa19634c1c15982d4768dd,Set minimum required cmake version to 3.10,"cmake > 3.10 is not packaged for some of the platforms we build packages eg old ubuntu and debian version. Currently we modify the CMakeLists.txt in those build environments and set the minimum version to 3.10 already, which proofes that timescaledb builds fine with cmake 3.10."
947,Sven Klemm,2022-03-17 23:33:35-05,566a4ff104924c99ef452e7feccaee2ad31fc47d,Route UPDATE through HypertableModify,Route UPDATE on Hypertables through our custom HypertableModify node. This patch by itself does not make any other changes to UPDATE but is the foundation for other features regarding UPDATE on hypertables.
948,Erik Nordström,2022-03-16 05:10:29-05,846878c6bb93fd8481af0d8a547e12897b7f4876,Ensure scan functions use long-lived memory context,"PostgreSQL scan functions might allocate memory that needs to live for the duration of the scan. This applies also to functions that are called during the scan, such as getting the next tuple. To avoid situations when such functions are accidentally called on, e.g., a short-lived per-tuple context, add a explicit scan memory context to the Scanner interface that wraps the PostgreSQL scan API."
949,Erik Nordström,2022-03-11 10:46:05-06,b954c00fa8136c87870302ad0297e704f280e2c7,Fix memory handling during scans,"Scan functions cannot be called on a per-tuple memory context as they might allocate data that need to live until the end of the scan. Fix this in a couple of places to ensure correct memory handling.  Fixes #4148, #4145"
950,Sven Klemm,2022-03-17 05:32:23-05,a759b2b2b9b855975f48d71e6bdf7af1cbf8c917,Show number of chunks excluded in ConstraintAwareAppend EXPLAIN,This patch changes the ConstraintAwareAppend EXPLAIN output to show the number of chunks excluded instead of the number of chunks left. The number of chunks left can be seen from other EXPLAIN output while the actual number of exclusions that happened can not. This also makes the output consistent with output of ChunkAppend.
954,Sven Klemm,2022-03-15 07:14:43-05,cc89f1dc847bf825b5502d72769b931e3502373f,Remove duplicate contain_param functions,This patch exports the contain_param function in planner.c and changes ChunkAppend to use that version instead of having two implementations of that function.
957,Mats Kindahl,2022-03-04 08:10:09-06,f5fd06cabb044a4b7f1fba694aeb9b1fbec87afc,Ignore invalid relid when deleting hypertable,"When running `performDeletion` is is necessary to have a valid relation id, but when doing a lookup using `ts_hypertable_get_by_id` this might actually return a hypertable entry pointing to a table that does not exist because it has been deleted previously. In this case, only the catalog entry should be removed, but it is not necessary to delete the actual table.  This scenario can occur if both the hypertable and a compressed table are deleted as part of running a `sql_drop` event, for example, if a compressed hypertable is defined inside an extension. In this case, the compressed hypertable (indeed all tables) will be deleted first, and the lookup of the compressed hypertable will find it in the metadata but a lookup of the actual table will fail since the table does not exist.  Fixes #4140"
958,Dmitry Simonenko,2022-03-11 02:25:38-06,f82df7ca4a6e6521daeb02f83db1548e7ed380ae,Allow ANALYZE command on a data node directly,Allow execution of VACUUM/ANALYZE commands on a data node without enabling timescaledb.enable_client_ddl_on_data_nodes GUC  Fix #4157
959,Sven Klemm,2022-03-07 10:51:59-06,8f56ced825c2a1df543ac1c7646707c10f400202,Add workflow for running sqlsmith,sqlsmith is a random SQL query generator and very useful for finding bugs in our implementation as it tests complex queries and thereby hits codepaths and interactions between different features not tested in our normal regression checks.
960,Sven Klemm,2022-03-08 04:15:22-06,06d837559484c21165cb3429dfdd91c8606973f3,Enhance extension function test,This patch changes the extension function list to include the signature as well since functions with different signature are separate objects in postgres. This also changes the list to include all functions. Even though functions in internal schemas are not considered public API they still need be treated the same as functions in other schemas with regards to extension upgrade/downgrade.  This patch also moves the test to regresscheck-shared since we do not dedicated database to run these tests.
961,Fabrízio de Royes Mello,2022-03-07 08:12:46-06,33bbdccdcd7ad805c3463ba6426dea0510497ed1,Refactor function `hypertable_local_size`,"Reorganize the code and fix minor bug that was not computing the size of FSM, VM and INIT forks of the parent hypertable.  Fixed the bug by exposing the `ts_relation_size` function to the SQL level to encapsulate the logic to compute `heap`, `indexes` and `toast` sizes."
962,Fabrízio de Royes Mello,2022-03-03 12:14:18-06,18afcfd62f3c7235c839eb15235c2318fbeec85c,Refactor function `ts_relation_size`,Current implementation iterate over fork types to calculate the size of each one by calling `pg_relation_size` PostgreSQL function and other calls to calculate indexes and table size (six function calls).  Improving it by halving PostgreSQL function calls to calculate the size of the relations (now three function calls).
963,Markos Fountoulakis,2022-03-03 03:47:36-06,e9fb9acbbbe2c7da2c3769fb7374dda9a7d8ad11,Fix regressions found in nightly CI,Add concurrent_query_and_drop_chunks to ignore-list and fix C compiler warning.
964,Mats Kindahl,2021-11-25 03:18:10-06,15d33f0624595470cdb3b417deecefe8c12639eb,Add option to compile without telemetry,"Add option `USE_TELEMETRY` that can be used to exclude telemetry from the compile.  Telemetry-specific SQL is moved, which is only included when extension is compiled with telemetry and the notice is changed so that the message about telemetry is not printed when Telemetry is not compiled in.  The following code is not compiled in when telemetry is not used: - Cross-module functions for telemetry. - Checks for telemetry job in job execution. - GUC variables `telemetry_level` and `telemetry_cloud`.  Telemetry subsystem is not included when compiling without telemetry, which requires some functions to be moved out of the telemetry subsystem: - Metadata handling is moved out of the telemetry module since it is   used not only with telemetry. - UUID functions are moved into a separate module instead of being   part of the telemetry subsystem. - Telemetry functions are either added or removed when updating from a   previous version.  Tests are updated to: - Not use telemetry functions to get UUID or Metadata and instead use   the moved UUID and metadata functions. - Not include telemetry information in tests that do not require it. - Configuration files do not set telemetry variables when telemetry is   not compiled in. - Replaced usage of telemetry functions in non-telemetry tests with   other sources of same information.  Fixes #3931"
965,Sven Klemm,2022-03-02 05:30:16-06,642a745767c925a12b4b68359c69ff496a276bac,Fix arm64 apt package test,This patch changes the workflow to run apt-get update before installing any packages in case the local package database is outdated and references packages no longer available.
966,Mats Kindahl,2022-02-24 07:49:10-06,b909d4857db23ed38b0cb4a825cefe76f0ef7401,Fixes to smoke update tests,"Smoke tests where missing critical files and some tests had changed since last run and did not handle update smoke tests, so fixing all necessary issues."
967,Erik Nordström,2022-02-18 10:05:18-06,14deea6bd59c3afa81a4469012911d265242a49a,Improve chunk scan performance,"Chunk scan performance during querying is improved by avoiding repeated open and close of relations and indexes when joining chunk information from different metadata tables.  When executing a query on a hypertable, it is expanded to include all its children chunks. However, during the expansion, the chunks that don't match the query constraints should also be excluded. The following changes are made to make the scanning and exclusion more efficient:  * Ensure metadata relations and indexes are only opened once even   though metadata for multiple chunks are scanned. This avoids doing   repeated open and close of tables and indexes for each chunk   scanned. * Avoid interleaving scans of different relations, ensuring better   data locality, and having, e.g., indexes warm in cache. * Avoid unnecessary scans that repeat work already done. * Ensure chunks are locked in a consistent order (based on Oid).  To enable the above changes, some refactoring was necessary. The chunk scans that happen during constraint exclusion are moved into separate source files (`chunk_scan.c`) for better structure and readability.  Some test outputs are affected due to the new ordering of chunks in append relations."
968,Erik Nordström,2022-02-10 09:42:56-06,32c1e3aef293bcece63550e09c3dd3496368a687,Allow control of relation open/close in Scanner,"Make the Scanner module more flexible by allowing optional control over when the scanned relation is opened and closed. Relations can then remain open over multiple scans, which can improve performance and efficiency.  Closes #2173"
969,Erik Nordström,2022-02-10 08:55:28-06,0f351ff6128e9765f993fe5ed713bd274a3fd2a7,Simplify Scanner by embedding internal state,"As part of adding a scan iterator interface on top of the Scanner module (commit 8baaa98), the internal scanner state that was previously private, was made public. Now that it is public, it makes more sense to make it part of the standard user-facing `ScannerCtx` struct, which also simplifies the code elsewhere."
970,Sven Klemm,2022-02-23 10:33:45-06,91820e26f663eb2210080092b75a290a491424f6,Improve planner error messages regarding nodes,Change error messages when unexpected nodes are encountered to actually show the node name instead of the node id.
971,Sven Klemm,2022-02-23 06:19:34-06,3f303c7d42da6e76f610bfb67d017134318cf7ed,Refactor tsl_debug_append_path,This patch splits the node name logic from the child path logic to allow getting a string representation for any postgres node. This adds a new function: const char * ts_get_node_name(Path *path)  This patch doesn't add any new callers to the function but it will be used in subsequent patches to produce more user friendly error messages when unexpected node types are encountered during planning.
972,Dmitry Simonenko,2022-02-22 04:27:14-06,57368dd98ecba8290008e36c578534612b9e7533,Fix RENAME TO/SET SCHEMA on distributed hypertable,"This PR fixes ON_END logic for distributed DDL execution by removing old leftover check, which marked those commands as unsupported.  Fix: #4106"
973,Sven Klemm,2022-02-22 19:59:19-06,a4648b11b4979ecc2d5d393585a7ce1a812dff0a,Fix segfault on INSERT in distributed hypertables,When inserting in a distributed hypertable with a query on a distributed hypertable a segfault would occur when all the chunks on the query would get pruned.
974,Sven Klemm,2022-02-20 15:12:34-06,58fd0c5cef56e638f8ed03204c399c0e76e9a164,Test APT ARM64 packages,Add tests for ARM64 Debian and Ubuntu packages
975,Alexander Kuzmenkov,2022-02-21 04:20:47-06,37190e8a8aaaecd3a906b092c54527ca987e02e5,Cache chunk data when performing chunk exclusion,"We cache the Chunk structs in RelOptInfo private data. They are later used to estimate the chunk sizes, check which data nodes they belong to, et cetera. Looking up the chunks is expensive, so this change speeds up the planning."
976,Dmitry Simonenko,2022-02-21 02:11:28-06,0f7ab056c23f6a5b214858885123d40b4a42065e,Move copy/move chunk tests to separate file,Fix flaky data_node test by moving tests associated with copy/move chunk functionality into a separate test file/groupand with increased background workers (using max_bgw_8.conf).  Issue: #4047
977,Aleksander Alekseev,2022-02-16 07:38:24-06,eedaaecc46966c75983f34e269fb28f7c8ae7277,Custom origin's support in CAGGs,"This patch allows using custom origin's in CAGGs, for instance:  time_bucket_ng('7 days', day, '2000-01-03' :: date) AS bucket  For weekly buckets this allows the user to choose what should be considered the beginning of the week - Sunday or Monday. Also by shifting the origin one second forward or backward user can tweak the inclusiveness of the buckets.  This works for date's, timestamp's and timestamptz's. The bucket size is considered variable-sized in all these cases. CAGGs on top of distributed hypertables, compressed hypertables and compressed distributed hypertables are supported as well.  Additionally, this patch does several small refactorings. Firstly, it makes sure that experimental features of CAGGs will be tested in both Debug and Release builds. This was previously overlooked. Secondly, it renames the tests so that a person who is working on experimental features in CAGGs will be able to easily execute all the related tests: `TESTS='exp_cagg_*' make installcheck`  Last but not least the patch refactors is_valid_bucketing_function() and renames it to function_allowed_in_cagg_definition(). The reason to do it in this patch is that otherwise, the logic of the function gets rather confusing which complicates code review.  fix"
978,Sven Klemm,2022-02-12 11:05:20-06,45cf7f1fa1e2c00afde506bfada625af9d7d4eb3,Document requirements for statements in sql files,Since we now lock down search_path during update/downgrade there are some additional requirements for writing sql files.
980,Sven Klemm,2022-02-17 14:26:52-06,df49c2b4cbba831cbd6c8d5e4f88a7a7b781c3ac,Fix non-debug build with older cmake,"On non-debug builds we might end up with an empty list of tests when generating the schedule. On older cmake versions < 3.14 trying to sort an empty list will produce an error, so we check for empty list here."
981,Alexander Kuzmenkov,2022-02-14 03:19:41-06,9e7fbf7f6982bad80cd0129d38dcf6273baad2a2,Release 2.6.0,"This release is medium priority for upgrade. We recommend that you upgrade at the next available opportunity.  This release adds major new features since the 2.5.2 release, including:  Compression in continuous aggregates Experimental support for timezones in continuous aggregates Experimental support for monthly buckets in continuous aggregates It also includes several bug fixes. Telemetry reports are switched to a new format, and now include more detailed statistics on compression, distributed hypertables and indexes.  **Features**  * #3768 Allow ALTER TABLE ADD COLUMN with DEFAULT on compressed hypertable * #3769 Allow ALTER TABLE DROP COLUMN on compressed hypertable * #3943 Optimize first/last * #3945 Add support for ALTER SCHEMA on multi-node * #3949 Add support for DROP SCHEMA on multi-node  **Bugfixes**  * #3808 Properly handle max_retries option * #3863 Fix remote transaction heal logic * #3869 Fix ALTER SET/DROP NULL contstraint on distributed hypertable * #3944 Fix segfault in add_compression_policy * #3961 Fix crash in EXPLAIN VERBOSE on distributed hypertable * #4015 Eliminate float rounding instabilities in interpolate * #4019 Update ts_extension_oid in transitioning state * #4073 Fix buffer overflow in partition scheme  **Improvements**  Query planning performance is improved for hypertables with a large number of chunks.  **Thanks**  * @fvannee for reporting a first/last memory leak * @mmouterde for reporting an issue with floats and interpolate"
983,Erik Nordström,2022-02-16 04:17:07-06,2bc3efa44ed50e6d3cffd6ed3276a473650df33d,Add missing OS-information in telemetry,Some telemetry fields were removed by mistake as part of an earlier refactoring. Add back the missing fields that provide information about the operating system.
984,Sven Klemm,2022-02-15 15:27:24-06,a956faaa16db96b6feda50c5d454fbcc7adc2562,Set search_path again after COMMIT,SET LOCAL is only active until end of transaction so we set search_path again after COMMIT in functions that do transaction control. While we could use SET at the start of the function we do not want to bleed out search_path to caller.
985,gayyappan,2022-02-07 12:59:37-06,264540610e02f7f46d3213c103b66b07c1ce12c5,Fix tablespace for compressed chunk's index,"When a hypertable uses a non default tablespace, based on attach_tablespace settings, the compressed chunk's index is still created in the default tablespace. This PR fixes this behavior and creates the compressed chunk and its indexes in the same tablespace.  When move_chunk is executed on a compressed chunk, move the indexes to the specified destination tablespace.  Fixes #4000"
986,Sven Klemm,2022-02-12 10:02:03-06,72d03e6f7d30cc4794c9263445f14199241e2eb5,Remove search_path modifications from reverse-dev,Resetting search_path in reverse-dev was necessary before the release of 2.5.2 as the previous timescaledb version scripts didn't handle locked down search_path. We can remove setting search_path too as the downgrade script includes pre-update.sql which locks down search_path.
988,Sven Klemm,2022-02-10 12:32:22-06,531f7ed8b16e4d1a99021d3d2b843bbc939798e3,Don't use pg_temp in update scripts,Scripts run under pgextwlist cannot use pg_temp so we replace pg_temp usage in update scripts with _timescaledb_internal.
1026,Sven Klemm,2022-01-24 14:05:39-06,64ed2db8695e471b7a2d938762b84f26ccf32b6f,Fix ALTER TABLE EventTrigger initialization,"When ALTER TABLE SET (timescaledb.compress) is executed, the compression table is created as part of this command. But when run as part of an extension installation the EventTrigger for ALTER TABLE does not get initialized leading to a segfault.  Fixes #4017"
989,Aleksander Alekseev,2022-02-10 15:21:25-06,ee9b9ad318719b715caeebd20875692e80ff82dc,time_bucket_ng() bugfix,"time_bucket_ng() may return wrong results for monthly buckets with overwritten origin. As an example:  time_bucket_ng('1 month', '2001-01-01' :: date, origin => '2000-06-01')  ... returns 2000-01-01 instead of 2001-01-01. In fact, our tests showed this but we overlooked the fact that the results are incorrect. The reason is that the month of the origin is not accounted for when calculating the resulting year.  This patch fixes this.  Also it updates linux-32bit-build-and-test.yaml workflow due to todays 14.2/13.6/12.10 releases. ""Memoize"" test doesn't pass on 14.2 thus the patch adds it to the list of skipped tests. The patch doesn't rename the checks because we list ""PG14.1 Debug linux-i386"" etc as required checks on GitHub. This will be addressed in the follow-up patches."
991,Sven Klemm,2022-02-09 17:16:52-06,c56b33d2e778fbf5975cfadfae3748df218d6731,Fix openssl detection for PG14,PG14 introduced an alternative way to specify for postgres to be built with openssl support. --with-ssl=openssl may be used instead of --with-openssl. This patch adjusts our openssl check to detect both versions.  https://github.com/postgres/postgres/commit/fe61df7f82
992,Sven Klemm,2022-02-10 03:59:22-06,85e3307dd130c61b8dab20b6551ca66a1703dd6b,Adjust pg_md5_hash compat macro to upstream changes,The API for pg_md5_hash was changed for PG15 which was initially backported to the PG14 branch but that commit was later reverted to not introduce an ABI breakage for extensions.  https://github.com/postgres/postgres/commit/ad5b6f24 https://github.com/postgres/postgres/commit/b69aba74
993,Sven Klemm,2022-02-09 16:52:34-06,c07fe1f88374a68707e54121c983b27fd59e5a20,Post Release 2.5.2,Add 2.5.2 to update/downgrade scripts
994,Sven Klemm,2022-02-09 12:47:10-06,f0d163603c1797442e2acffc31ce135a0d38b820,Release 2.5.2,This release contains bug fixes since the 2.5.1 release. This release is high priority for upgrade. We strongly recommend that you upgrade as soon as possible.  **Bugfixes** * #3900 Improve custom scan node registration * #3911 Fix role type deparsing for GRANT command * #3918 Fix DataNodeScan plans with one-time filter * #3921 Fix segfault on insert into internal compressed table * #3938 Fix subtract_integer_from_now on 32-bit platforms and improve error handling * #3939 Fix projection handling in time_bucket_gapfill * #3948 Avoid double PGclear() in data fetchers * #3979 Fix deparsing of index predicates * #4015 Eliminate float rounding instabilities in interpolate * #4020 Fix ALTER TABLE EventTrigger initialization * #4024 Fix premature cache release call * #4037 Fix status for dropped chunks that have catalog entries * #4069 Fix riinfo NULL handling in ANY construct * #4071 Fix extension installation privilege escalation * #4073 Fix buffer overflow in partition scheme  **Thanks** * @carlocperez for reporting crash with NULL handling in ANY construct * @erikhh for reporting an issue with time_bucket_gapfill * @fvannee for reporting a first/last memory leak * @kancsuki for reporting drop column and partial index creation not working * @mmouterde for reporting an issue with floats and interpolate * Pedro Gallegos for reporting a possible privilege escalation during extension installation  Security: CVE-2022-24128
995,Markos Fountoulakis,2022-02-09 10:35:19-06,8a33a79e0c489ddc15f252e710dab61aa96c4e82,Fix buffer overflow in partition scheme,Reallocate the partitioning attributes arrays when forcing GROUP BY aggregates down.  Fixes #4050
996,Sven Klemm,2022-02-08 13:02:55-06,6dddfaa54e8f29e3ea41dab2fe7d9f3e37cd3aae,Lock down search_path in install scripts,"This patch locks down search_path in extension install and update scripts to only contain pg_catalog, this requires that any reference in those scripts is fully qualified. Additionally we add explicit create commands to all update scripts for objects added to the public schema. This change will make update scripts fail if a function with identical signature already exists when installing or upgrading instead reusing the existing object."
997,Sven Klemm,2022-02-08 12:55:28-06,c8b8516e466c2bb7d2ae6a4b0b2e8e60b24b24a2,Fix extension installation privilege escalation,"TimescaleDB was vulnerable to a privilege escalation attack in the extension installation script. An attacker could precreate objects normally owned by the extension and get those objects used in the installation script since the script would only try to create them if they did not already exist. Thanks to Pedro Gallegos for reporting the problem.  This patch changes the schema, table and function creation to fail and abort the installation when the object already exists instead of using the existing object.  Security: CVE-2022-24128"
998,Erik Nordström,2022-02-09 01:34:18-06,7fb3feb60d72fef164940b5dd6d26ef1201e4988,Use proper types in telemetry JSON,Many fields in the generated telemetry JSON output values as strings instead of using integer or boolean types. Change those fields to use the type that match the original data.
999,Erik Nordström,2022-02-08 02:57:23-06,5af9f45488d51027804cac16362811f71a89bb64,Add extra telemetry for continuous aggregates,Add the following telemetry fields for continuous aggregates:  * The number of continuous aggregates created on distributed   hypertables * The number of continuous aggregates using real-time aggregation
1000,Nikhil Sontakke,2022-02-09 01:37:44-06,e19fffc1482cf0a532ab252ac440bf52f9d8bf96,Fix riinfo NULL handling in ANY construct,"If the ANY construct contains a singleton NULL then the logic in ""dimension_values_create_from_array"" barfs causing a crash. Fix it appropriately in the caller ""hypertable_restrict_info_add_expr"" function."
1001,Aleksander Alekseev,2022-02-08 03:33:30-06,454d32539ce47c1775c136e68a615d3453ee3ac1,"Fix ""nm: unknown argument -defined-only"" error on MacOS","On MacOS Monterey with LLVM 13.0.1 when executing export_prefix_check.sh we get an error:  /usr/local/opt/llvm/bin/nm: error: : unknown argument '-defined-only', did you mean '--defined-only'?  This patch adds a better check of which flag does `nm` expect."
1002,Erik Nordström,2022-02-04 04:40:39-06,418d71c2fef7978ec6350669fb01d2d076039d8f,Rename telemetry test,"All TSL-related telemetry testing was moved into the `telemetry_community` test, which has a name that is no longer accurate (given that it tests non-community features). Rename this test to `telemetry_stats`, which also doesn't conflict with the non-TSL `telemetry` test."
1003,Erik Nordström,2022-01-28 01:38:08-06,e56b95daec0f9fbf55d99b984d6236c1bbd4358c,Add telemetry stats based on type of relation,"Refactor the telemetry function and format to include stats broken down on common relation types. The types include:  - Tables - Partitioned tables - Hypertables - Distributed hypertables - Continuous aggregates - Materialized views - Views  and for each of these types report (when applicable):  - Total number of relations - Total number of children/chunks - Total data volume (broken into heap, toast, and indexes). - Compression stats - PG stats, like reltuples  The telemetry function has also been refactored to return `jsonb` instead of `text`. This makes it easier to query and manipulate the resulting JSON format, and also gives cleaner output.  Closes #3932"
1004,Erik Nordström,2022-01-27 10:14:26-06,7f05448d2af964264cd9bde9624a05522e786e31,Add rescan support to internal Scanner,"This change adds rescan support to the internal Scanner module.  When scanning TimescaleDB catalog data, it is sometimes useful to be able to restart an index scan without having to close and reopen the scanned relation."
1005,Alexander Kuzmenkov,2022-02-03 09:35:51-06,7d413809916035d1f3e4dfb3210878dbd1b34744,Fix double initialization of TsFdwRelInfo,"This doesn't cause any problems except degrading the performance.  The TsFdwRelInfo is initialized twice for hypertable on access node:  1) when using it for chunk size estimation in `estimate_chunk_size`, 2) when expanding the distributed hypertable in `tsl_set_rel_pathlist` callback.  The initialization has to happen inside the TSL module, this is why we can't easily move it up the call graph to, say, `import/ts_set_append_rel_size`, which is FDW-agnostic. As a fix, just accept that TimescaleDBPrivate structure can be allocated already, and don't discard the previous one."
1006,Aleksander Alekseev,2022-01-31 06:24:54-06,c9d24703a86a63933b3d64ef5a9681ef6f714230,Make add_continuous_aggregate_policy() work with variable-sized buckets,"Variable-sized buckets don't work with CAGG policies:  SELECT add_continuous_aggregate_policy('conditions_summary_policy',     start_offset => INTERVAL '65 days',     end_offset => INTERVAL '1 day',     schedule_interval => INTERVAL '1 hour'); ERROR:  bucket width is not defined for a variable bucket  This patch fixes it and adds corresponding tests.  Reported by Miranda Auhl <miranda@timescale.com>"
1007,Sven Klemm,2022-02-06 11:06:03-06,b330dd94e13746069ec21d8cdcd2e43ade17f01d,Fix privilege check in extension_load_without_preload,The privilege check in extension_load_without_preload used is_member_of_role which does not respect the inherit flag of roles instead of has_privs_of_role.  For additional context: https://www.postgresql.org/message-id/flat/CAGB
1011,Dmitry Simonenko,2022-02-07 01:13:16-06,9b1809152d7b6c1bc3a8977889328b2f1312b4ce,Add a way to disable dist ddl on database objects,"This change adds `timescaledb_experimental.enable_distributed_ddl` session variable which allows to enable or disable distributed DDL operations on a database objects such as SCHEMA, DATABASE or execute GRANT command.  Basically everything that does not involve particular distributed hypertable associated with the command can be disabled.  Affected commands:  CREATE/DROP SCHEMA ALTER SCHEMA ALTER DATABASE REASSING OWNER DROP OWNED ALTER DEFAULT PRIVILEGES GRANT ON DATABASE/TABLE GRANT IN SCHEMA  This change also set this variable to false by default, which disables forwarding DDL commands on database objects to data nodes.  Fix: #4001"
1012,Dmitry Simonenko,2022-02-07 00:28:55-06,5e50cb51dbf3543cd6ba2e44220b46ee2af137af,Fix cmake file to run template tests,"Recent CMakeLists.txt related refactoring accidently missed one case with TEST_FILES variable renaming, which lead to the exclusion of template tests."
1013,Sven Klemm,2022-02-06 01:33:12-06,485af7c19fc0a9f29858211d1703d2e81d8f6588,Fix centos 8 package test,Since CentOS 8 went EOL in december last year most of the repository URLs error out now. This patch switches the test to use Rockylinux which is a drop in replacement for CentOS.
1014,Mats Kindahl,2022-01-28 06:37:21-06,9714e881e3422a38032bdb2e18384657fa18134d,Increase default background workers,Increate the number of background workers since we have flaky tests as a result of groups running out of background workers. We also increate the total number of workers to 24 for the default configuration when running tests.  Fixes #4023
1015,Mats Kindahl,2022-01-28 06:30:32-06,05dd4787d1cd287d0636cd34d9295f29dda7c769,Support test groups with different configurations,"To support tests with different configuration options, we split the tests into *test configurations*. Each test configuration NAME will have  - A configuration template file `NAME.conf.in` that is used to run the   suite of tests. - A variable `TEST_FILES_<NAME>` listing the test files available for   that test suite. - A variable `SOLO_TESTS_<NAME>` that lists the tests that need to be   run as solo tests.  The code to generate test schedules is then factored out into a separate file and used for each configuration."
1017,Sven Klemm,2022-01-30 14:03:27-06,481bf8b5a8b304c40ccba1d12fb232e8684807fd,Fix downgrade test artifact upload,The artifact upload for the downgrade test was using the filenames of the update test and so would never upload any files.
1018,gayyappan,2022-01-27 15:03:24-06,e5db6a9eecdba2de76eeaee00b459fe35a556baa,Fix status for dropped chunks that have catalog entries,Chunks that are dropped but preserve the catalog entries have an incorrect status when they are marked as dropped. This happens if the chunk was previously compressed and then gets dropped - the status in the catalog tuple reflects the compression status. This should be reset since the data is now dropped.
1019,Aleksander Alekseev,2022-01-31 03:57:24-06,cf6aed71b94d473e5eb70536571d3ef0a6970222,Refactoring: Use consistent naming for cagg_*.sql tests,"Rename cagg_variable_size_buckets.sql to cagg_monthly.sql. This makes the naming for the tests much more consistent. This file actually contains only tests for monthly buckets, not for all possible variable-sized buckets. CAGGs with timezones are covered in cagg_with_timezone.sql."
1020,Alexander Kuzmenkov,2022-01-31 05:46:14-06,22f9cf689dca8a831fee1249f141b4acff5aaf6b,Don't leak Chunks in classify_relation,"This function is called often, at least 4 times per chunk, so these add up. Freeing these chunks allows us to save memory. Ideally, we should fix the function not to look up the chunk anew each time.  Also make a couple other tweaks that reduce memory usage for planning."
1021,Aleksander Alekseev,2022-01-27 08:11:42-06,ae02934c6a43524b6c17b562b283bac08a2bf4be,Timezones support in CAGGs,"This patch allows using timezones in CAGGs, for instance:  time_bucket_ng(bucket_size, ts, 'Europe/Moscow')  Both months/years and days/hours/minutes can be used in bucket_size. The bucket size is considered variable-sized in all these cases because DST affects days/hours/minutes as well.  CAGGs on top of distributed hypertables, compressed hypertables and compressed distributed hypertables are supported as well."
1022,gayyappan,2022-01-27 13:05:41-06,c0050a1315f9e79e593ba9f8906d6064606d5efc,Improve error message for compress_chunk,Fix the error message for compress chunks so that it specifies the cagg name instead of the materialized hypertable name.
1023,Sven Klemm,2022-01-26 17:05:05-06,9e16d9f4e44dff336e15c234632e4d280e68d64d,Add coccinelle scripts for detecting use-after-free,This patch adds coccinelle scripts for detecting use-after-free bugs in relation to ts_cache_release.  This will find code using the following pattern: Use of the hypertable from cache after releasing the cache and use of the dimension of the hypertable from a cache after the cache was released.
1024,gayyappan,2022-01-26 13:39:50-06,7f5c5fd10d518916a435b8e52fb48e38b407a55e,Verify compression settings before adding policy,"When a compression policy is added for a continuous aggregate, verify that compression has been enabled, before adding a policy."
1025,gayyappan,2022-01-25 13:23:12-06,44be03b5c6856a4d5a8ee43f606ddc5befeb263f,Fix premature cache release call,"The cache entry for a hypertable is created by calling ts_hypertable_from_tupleinfo. This sets up the ht->space structure, which in turn sets up the dimension information. These structures are allocated in the cache's memory context. The dimension information is accessed after the cache is released in ts_subtract_integer_from_now. This PR fixes this by releasing the cache before returning from the function.  Fixes #4014"
1027,Dmitry Simonenko,2022-01-25 05:03:50-06,b30d005657c5803c35ff609637644e484458f9ad,Fix crash when execute add_data_node,"On disconnect libpq will create a result object without creating event data, which is usually done for a regular errors.  This fix handles this case and forces the result object to always have have associated connection meta data.  Fix: #3951"
1028,Alexander Kuzmenkov,2022-01-14 05:54:55-06,2e725dc0e2cf6d9f715cc91278a6713f6a847b06,Use cached Chunks for data node assignment,"Before assigning chunks to data nodes, we have to look up the same Chunk structs to estimate the chunk relation size. Cache them in chunk RelOptInfos to avoid the costly lookup."
1029,Alexander Kuzmenkov,2022-01-11 08:37:58-06,60edef6ebaa35c4961398c8ba2cf7c1be911bdbc,Don't guess historical/current chunk by ids,"For the chunks that don't have the ANALYZE stats, we have to estimate the number of pages and tuples in some other ways. One thing we factor in this estimation is the expected fill factor of the chunk. There are two ways we calculate the fill factor: 1. For time dimensions, by looking at whether the now() belongs to the chunks [begin, end) interval and calculating the fill factor accordingly.  2. For space dimensions, by looking at whether the chunk is one of the last chunks in the hypertable.  To check (2), we used to compare the chunk ids. Turns out it didn't work correctly, because the chunk ids are global for all hypertables, and looking at them doesn't tell us anything if there are many hypertables.  The code that did that was also very slow.  This commit just removes this logic and considers all the chunks on space dimensions w/o the ANALYZE stats as recent chunks with fill factor of 0.5."
1030,Sven Klemm,2022-01-22 16:49:00-06,c2bfc5d17c01359621ed2ce00fb198653cda4ffa,Route delete on Hypertables through HypertableModify node,This patch changes DELETE handling for hypertables to have the postgres ModifyTable node be wrapped in a custom HypertableModify node. By itself this does not change DELETE handling for hypertables but instead enables subsequent patches to implement e.g. chunk exclusion for DELETE or DELETE on compressed chunks. Since PG 14 codepath for INSERT is different from previous versions this PR will only change the plan for PG14+. DELETE handling for distributed hypertables is not changed as part of this patch.
1032,Mats Kindahl,2022-01-24 08:14:23-06,616d3b16b8e1dfca760cc4e4472f9af159bf44bd,Update ts_extension_oid in transitioning state,"When executing the post-update scripts we get an assertion failure and subsequent segfault because `ts_extension_oid` is still set to `InvalidOid`. This happen because `ts_extension_oid` is not set up until after the post-update state has finished, but we need to set it up already in the extension transitioning state used during post-update execution.  The reason for not setting up the `ts_extension_oid` is because we cannot issue queries to the catalog, but this is possible already in the post-update stage, so we add code to set up `ts_extension_oid` already in this state.  Fixes #4009"
1033,Sven Klemm,2022-01-23 13:33:40-06,89dceeabd4017f0843ddfc0a17bc505eb0eff895,Make pgtest input/output dir optional,PG15 removes the input and output directory from the pg regression test files and moves those files into sql and output directories. Currently cmake errors when those directories are not present in PG_SOURCE_DIR. This patch makes those directories optional so cmake does not fail when PG_SOURCE_DIR contains the checkout of a version without these directories.  https://github.com/postgres/postgres/commit/d1029bb5 https://github.com/postgres/postgres/commit/dc9c3b0f
1034,Sven Klemm,2022-01-23 04:38:57-06,5c22ef3da2958a6e5eb876f984539be4d86f4def,Rename continuous aggregate tests,Change the prefix for continuous aggregate tests from continuous_aggs_ to cagg_. This is similar to commit 6a8c2b66 which did this adjustment for isolation tests because we were running into length limitations for the spec name. This patch adjusts the remaining tests to be consistent with the naming used in isolation tests.
1035,Sven Klemm,2022-01-23 12:58:28-06,29856fd0ace1ab0a1b27452e3b835f5971014848,Eliminate float rounding instabilities in interpolate,When interpolating float values the result of the calculation might be unstable for certain values when y0 and y1 are equal. This patch short circuits the formula and returns y0 immediately when y0 and y1 are identical.  Fixes #1528
1036,gayyappan,2022-01-12 13:59:17-06,e603478e472b99c348881b22e401f31505b77985,Support cagg trigger for distributed compressed chunks,AFTER ROW triggers are not supported on inserts into compressed chunks. This change explicitly calls the cagg triggers so that continuous aggregates work as expected on inserts into compressed distributed hypertables. This fix is similar to PR 3724 that handles cagg triggers for non distrbuted hypertables.
1037,gayyappan,2022-01-12 16:06:57-06,4ebb5b72e95f788694855fafbb4fc5de732e72a6,Refactor ChunkInsertState struct,Move all compression related info into a separate struct.
1038,gayyappan,2022-01-12 10:32:17-06,07fd9a4298bde42297c28594e1c6f8b7120677a2,Support continuous agg trigger on copy into compressed chunks,AFTER ROW triggers are not supported on compressed chunks. Directly call the continuous aggregate trigger function for copies.  This fix is similar to PR 3764 that handles cagg triggers for inserts into compressed chunks.
1039,Sven Klemm,2022-01-18 04:03:31-06,afe3362778e0e950407e51291427e2b2524413f0,Don't REVOKE in runner cleanup function,This patch removes the REVOKEs added in commit e320679c. The REVOKEs were added to the cleanup function which gets executed for every individual test but the GRANTs only happens once during initial setup thereby creating an asymmetry. This lead to tests failing that relied on those GRANTs. This patch also makes those GRANTs only execute for PG >= 15.
1040,Sven Klemm,2022-01-17 11:45:26-06,8e6d3d44abe4163cba277a364155694b808531c0,Add support for wildcards to SKIPS,"This patch adds suport for wildcards to the SKIPS environment variable similar to wildcard support in TESTS.  This enables the following test invocations:  -- run all tsl tests expect background worker and multinode make regresscheck-t SKIPS=""*bgw* *dist*"" -- run cagg tests except background worker make regresscheck-t TESTS=""continuous_agg*"" SKIPS=""*bgw*"""
1041,Aleksander Alekseev,2022-01-11 03:06:34-06,9103d697fb3cdbae3450eaa596d66df4cf289b61,Don't allow using buckets like '1 month 15 days' + some refactorings,"This is in fact a backport from the ""Buckets with timezones"" feature branch. While working on the feature a bug was discovered. We allow creating buckets like '1 month 15 days', i.e. fixed-sized + variable-sized, which is supposed to be forbidden.  This patch fixes the bug and also simplifies code a little. timezone_in / timezone_out procedures are used instead of snprintf/scanf. Also, the CAggTimebucketInfo structure was changed slightly. These changes are going to be needed for timezones support anyway."
1042,Nikhil Sontakke,2022-01-12 05:54:52-06,2a2b394172109b693d2bc06c8c8df8f9ab0cd879,Avoid distributed chunks scan plans,"The current approach to planning queries on distributed hypertables first plans a ""naive"" plan for scanning each remote chunk individually (the chunks are children of an append) and then that plan gets replaced by a datanode scan plan (each data node is a child in the Append).  While we need to include each chunk in the planning for cost reasons (we need to know the stats for chunk rels) we need not actually create paths for scanning chunks. This unnecessary work is now avoided and we now plan datanode scans directly.  Fixes #3685"
1043,Mats Kindahl,2022-01-07 07:07:09-06,e320679c4cc5cd9fc0f35f59fd22284fc2ecfb89,Remove grants on data node bootstrap,"Starting with PG15, default permissions on the public schema is restricted for any non-superuser non-owner. This causes test failures since tables can no longer be created without explicitly adding permissions, so we remove grant when bootstrapping the data nodes and instead grant permissions to the users in the regression tests. This keeps the default permissions on data nodes, but allow regression tests to run.  Fixes #3957  Reference: https://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=b073c3cc"
1044,Fabrízio de Royes Mello,2021-12-10 07:40:10-06,342f848d90b08effa60aaa2c33ad1ceb16fcb4ce,Refactor invalidation log inclusion,Commit 97c2578ffa6b08f733a75381defefc176c91826b overcomplicated the `invalidate_add_entry` API by adding parameters related to the remote function call for multi-node on materialization hypertables.  Refactored it simplifying the function interface and adding a new function to deal with materialization hypertables on multi-node environment.  Fixes #3833
1045,Sven Klemm,2022-01-17 03:21:37-06,c5796c0f1d6135729c772029d188afe8a2b69b1b,Ignore memoize test in pg14 snapshot,Upstream changed the explain output for the memoize node to include an additional `Cache Mode` line. While we could adjust our test runner to always ignore that line this would prevent us from testing the cache mode in future tests.  https://github.com/postgres/postgres/commit/6c32c0977783fae217b5eaa1d22d26c96e5b0085
1046,Sven Klemm,2022-01-14 03:57:05-06,1f9bd81ee05e64f14db29f63b33f9d7d2ae712d7,Adjust CI ignore lists to isolation test renames,Commit 6a8c2b66 renamed cagg tests in isolationcheck-t to have a common prefix but did not adjust the CI ignorelists to reflect that change.
1047,Sven Klemm,2022-01-14 07:09:34-06,69b267071a9deab62ef204f4815c810da58b406e,Bump copyright year in license descriptions,Bump year in copyright information to 2022 and adjust same scripts to reference NOTICE that didn't have the reference yet. This patch also removes orphaned test/expected/utils.out.
1048,Rafia Sabih,2022-01-11 13:02:39-06,28a3895235023d1271f460dbb73ee733714e3ab0,Update syntactic attribute number,"When adjusting attribute numbers of chunk index, also adjust varattnosyn which is used at the time of reconstructing index definition.  Fixes #3794"
1049,Sven Klemm,2022-01-13 19:20:24-06,e2d578cfac52c2f2a3fc5a1053954cdf9a6b0201,Fix cagg_multi_dist_ht isolation test,Adjust cagg_multi_dist_ht isolation test to no longer include chunk names. isolation tests that expose chunk names cannot be run by themselves or in a custom test list because chunk numbering is depending on the spec file list.
1050,Sven Klemm,2022-01-13 05:55:00-06,6a8c2b666e23eac77cfb383301a29a0fbb1efe94,Shorten isolation test spec file names,Isolation test identifiers have a length limit and when the isolationtester encounters names that are too long they get truncated. More recent versions will produce a warning when this truncation is done leading to flaky tests. Only continuous_aggs_concurrent_refresh_dist_ht.spec exceeded the truncation limit but since all the continuous_aggs isolation tests have quite long name this patch shortens the names from continuous_aggs_* to cagg_* to prevent this problem from happening to other isolation tests as well.
1051,Sven Klemm,2022-01-13 05:57:41-06,22fd4d44263342ce650c1e344c2a9424f103254e,Fix compression_ddl isolation test,This patch gets rid of all hardcoded chunk names from the compression_ddl isolation test and also gets rid of chunk names from the output files. Chunk names in isolation test files are problematic as it prevents changing the order of execution of isolation test runs as the database is shared between the individual tests. Output will also differ when only a subset of the tests is run leading to flaky tests.
1052,Fabrízio de Royes Mello,2022-01-12 12:25:58-06,10b48858e1b03f96d53f690de1501934a586148b,Add exception to invalidation SQL functions,Per Sven's sqlsmith run we got some assertion errors adding invalidation for hypertables and continuous aggregates.  Fixed id adding the proper validation in the invalidation SQL functions to check properly the parameters `start_time` and `end_time`.
1053,Aleksander Alekseev,2022-01-11 03:06:34-06,82e3f055371b39158fbe4d1f81f412d99763e304,Fix the build against REL_14_STABLE,The number of arguments of pg_md5_hash() has changed in REL_14_STABLE. This patch adds a corresponding compat.h version of the procedure.  https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=3a0cced8
1054,Sven Klemm,2022-01-10 14:21:31-06,3b9beac5321390152843ad3df4bcc31cd3f34a2a,Adjust apt package test to match docs,Adjust the APT package test to closer match the instructions used in our documentation.
1055,Erik Nordström,2021-12-21 11:56:56-06,6d3f11cde0298a70c8f659c288e46d92ee273506,Avoid double PQclear() in data fetchers,"The macro `remote_result_elog` is used in the cursor and row-by-row fetchers and clears the result value using the function `PQclear()` before rethrowing its error. However, this error is later captured to clear the wrapping result value that holds a reference to the original result. To avoid this double-clearing, set the response to NULL before the error is thrown."
1056,Sven Klemm,2022-01-10 06:18:19-06,ab0f753e26a45e9a25cee3d243f431a5a15f4a91,Sort issue template options alphabetically,Change the bug report and enhancement issue template to have dropdown options sorted alphabetically.
1057,Nikhil Sontakke,2022-01-06 08:31:46-06,4aeb1330f379ed1e8ebbab2f93a6e73aac24e94a,Fix crash in EXPLAIN VERBOSE on dist table,"The crash is occurring in ""fdw_scan_explain"" function because the fsstate passed in is NULL and we try to access the ""fetcher_type"" field from it in VERBOSE mode.  When we are doing EXPLAIN, we short circuit the init foreign scan calls and that's why fsstate is not filled up.  Fixes #3960"
1059,Aleksander Alekseev,2021-12-23 02:43:30-06,0dd4329ab1e7451e2a50f466a05515e1336edc19,Don't display an error message when creating an empty CAGG,"This is a follow-up to 95804069, ""Monthly buckets support in CAGGs"". In that patch we didn't check the creation of a CAGG on top of an empty hypertable. It works, but displays ""ERROR: origin must be before the given date"", which is a bit misleading.  This happens because 95804069 didn't modify invalidation_threshold_compute() in the same way as tsl_process_continuous_agg_viewstmt(). As a result, compute_circumscribed_bucketed_refresh_window_for_months() calls time_bucket_ng() for ts_time_get_min() argument. It doesn't work because the minimum time is 4714-11-24 BC which is before any reasonable origin, and the user sees corresponding error message.  This patch fixes the problem in the same way as 95804069 did and adds a corresponding test."
1060,Aleksander Alekseev,2021-12-23 03:18:17-06,d7eaa55a476a0a1a2d4aa2d868db27c9e5a092d6,Override the default ACL for public schema on PG15,"Since PG15, by default, non-superuser accounts are not allowed to create tables in public schema of databases they don't own. This default can be changed manually. This patch ensures that the permissions are going to be the same regardless of the used PostgreSQL version.  Without this patch, none of our tests pass on PG15 because they fail with the ""access denied to schema public"" error. This is why runner.sh was modified. Then, some other tests keep failing because when we call create_distributed_hypertable() we create a new database on each of the data nodes, also not granting enough permissions to non-privileged users. This is what the fix of data_node.c addresses.  This is not necessarily the best approach possible, but it preserves the same behavior on PostgreSQL >= 15 and PostgreSQL < 15. Maybe one day we will come up with something better (especially when there will be no need to support PG < 15) but until then the patch seems to be good enough.  https://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=b073c3cc"
1061,Alexander Kuzmenkov,2021-12-14 10:56:20-06,13abd14c278418c4ccdc36630864c59486b5fdf7,Use moving average to estimate chunk size,"When no size statistics is available for a chunk because it haven't been ANALYZEd, we have to resort to some estimates. A good way to estimate the size of the chunk is to look at the sizes of the nearby chunk.  Before this commit, we would look up the ten nearby chunk for each chunk w/o statistics, which was slow. This commit adds the code to maintain exponential moving average of the recent chunk sizes in the parent RelOptInfo as we go through them, avoiding the additional lookups."
1062,Dmitry Simonenko,2021-12-23 05:01:06-06,4b3227663a8f0e189fe23a04ee44ecdfa94ab487,Add support for ALTER SCHEMA command on multi-node,This change adds support for ALTER SCHEMA RENAME TO and ALTER SCHEMA OWNER TO commands to distributed database.  Issue: #3909
1063,Dmitry Simonenko,2021-12-23 04:33:24-06,24d9243f8b80db905cd864736466cd66574ccb40,Add support for DROP SCHEMA on multi-node,"This PR changes logic and allows DROP SCHEMA command to be executed on each data node, even if the schema being dropped has no distributed hypertables in it.  Fix: #3909"
1064,James Guthrie,2021-12-17 06:48:03-06,a7dedf89c997099ec9598a964e87cf5096c74423,Fix weirdly formatted comment,It appears as though this comment had a run-in with auto-formatting which left it somewhat mangled.
1065,Sven Klemm,2021-12-19 17:22:17-06,ce73f25a87cdc54e911d7c84279be5a0dc48aa2f,Optimize first/last,This patch optimizes how first()/last() initialize the compare function. Previously the compare function would be looked up for every transition function call but since polymorphic types are resolved at parse time for a specific aggregate instance the compare function will not change during its lifetime. Additionally this patch also fixes a memory leak when using first()/last() on pointer types. These changes lead to a roughly 2x speed improvement for first()/ last() and make the memory usage near-constant.  Fixes #3935
1066,Sven Klemm,2021-12-20 02:21:12-06,b735fdcf85e06856c015191d5e47783ecca5d1a0,Fix segfault in add_compression_policy,Postgres snprintf before 89ad14cd787 will segfault on NULL char pointers. This patch changes add_compression_policy to check for NULL get_rel_name before reporting the error.  https://github.com/postgres/postgres/commit/89ad14cd787
1067,Erik Nordström,2021-12-13 02:27:42-06,7824b10a5ae7f4bcde5e874bc4be053e0ae357d3,Allow changing a data node via alter server,"Allow changing the name and configuration of a data node via `ALTER SERVER` since there is no `alter_data_node` command.  The functions `add_data_node` and `delete_data_node` are wrappers around `CREATE SERVER` and `DROP SERVER`, respectively. They block the PostgreSQL commmands since they do additional things, like data node bootstrapping.  However, there's currently no way to change a data node's configuration, and no additional functionality is required over standard `ALTER SERVER`, so we might as well allow it until a `alter_data_node` is implemented.  Fixes #3915"
1068,Sven Klemm,2021-12-17 22:11:41-06,0deffe2b643ea9ea51d65d73d656ddbe3e2aa2a2,Improve error handling in add_reorder_policy,When trying to add reorder policy to internal compressed hypertable add_reorder_policy would segfault.
1069,Sven Klemm,2021-12-17 19:37:16-06,1d40b2e4a621cf83d61e6849c1185f1dab43edce,Fix segfault when creating cagg,When trying to create a continuous aggregate on the internal compression hypertable a segfault would occur. This patch adds a check for this to the cagg validation logic and prevents creating cagg on the internal compressed hypertable.
1070,Sven Klemm,2021-12-17 18:40:11-06,d989e61b56bc97ea5fc014a964a3ef4d1ccf9e77,Improve show_chunks and drop_chunks error handling,This patch fixes a segfault when calling show_chunks on internal compressed hypertable and a cache lookup failure for drop_chunks when calling on internal compressed hypertable.
1071,Sven Klemm,2021-12-16 16:16:34-06,39645d56da87521c3491efa97bafece00805d2b4,Fix subtract_integer_from_now on 32-bit platforms,"This patch fixes subtract_integer_from_now on 32-bit platforms, improves error handling and adds some basic tests. subtract_integer_from_now would trigger an assert when called on a hypertable without integer time dimension (found by sqlsmith). Additionally subtract_integer_from_now would segfault when called on a hypertable without partitioning dimensions."
1072,Sven Klemm,2021-12-16 22:21:19-06,a7608871452bd685ae8f87c3b554f929bdb9e575,Fix projection handling in gapfill,When getting the next tuple from the subplan gapfill would apply the projection to it which was incorrect since the subplan already did the projection and the projection for the gapfill tuple has to be done when the tuple is handed to the parent node.  Fixes #3834
1073,Fabrízio de Royes Mello,2021-10-22 07:47:56-05,244568f23a7719283edc4f0a00c0e902b603c013,Add regression tests for caggs+compression,Closes timescale/timescaledb-private#962
1074,gayyappan,2021-09-27 13:42:06-05,d8d392914a7e37078c7a9589ad9b0b9e3859d755,Support for compression on continuous aggregates,Enable ALTER MATERIALIZED VIEW (timescaledb.compress) This enables compression on the underlying materialized hypertable. The segmentby and orderby columns for compression are based on the GROUP BY clause and time_bucket clause used while setting up the continuous aggregate.  timescaledb_information.continuous_aggregate view defn change  Add support for compression policy on continuous aggregates  Move code from job.c to policy_utils.c Add support functions to check compression policy validity for continuous aggregates.
1076,Aleksander Alekseev,2021-12-17 02:53:30-06,99521fef16d851a55f41b8e9d355774afe8beb8e,Fix compilation against PG15,Instead of `#if PG14` added in 519e5de21 it should have been `#if PG14_GE`.
1077,Alexander Kuzmenkov,2021-12-16 11:29:21-06,da5ee6fc414e2a4c6a1dd323a1fb799a1aa4434f,Add -Wundef to compiler flags,"Today was another time I used `#if PG14_LT` but forgot to include compat.h, spent a good deal of time debugging this. This flag will catch references to undefined macros."
1078,Erik Nordström,2021-12-14 11:25:36-06,e0f02c8c1a86eb4e4b913f39c34f53bc410dc4d7,Add option to drop database when deleting data node,"When deleting a data node, it is often convenient to be able to also drop the database on the data node so that the node can be added again using the same database name. However, dropping the database is optional since it should be possible to delete a data node even if it is no longer responding.  With the new functionality, a data node's database can be dropped as follows:  ```sql SELECT delete_data_node('dn1', drop_database=>true); ```  Note that the default behavior is still to not drop the database in order to be compatible with the old behavior. Enabling the option also makes the function non-transactional, since dropping a database is not transactional. Therefore, it is not possible to use this option in a transaction block.  Closes #3876"
1079,Aleksander Alekseev,2021-12-15 07:15:59-06,91f3edf609ac77523b982634d1fe6bebd7117662,Refactoring: get rid of max_bucket_width,"Our code occasionally mentions max_bucket_width. However, in practice, there is no such thing. For fixed-sized buckets, bucket_width and max_bucket_width are always the same, while for variable-sized buckets bucket_width is not used at all (except the fact that it equals -1 to indicate that the bucket size is variable).  This patch removes any use of max_bucket_width, except for arguments of:  - _timescaledb_internal.invalidation_process_hypertable_log() - _timescaledb_internal.invalidation_process_cagg_log()  The signatures of these functions were not changed for backward compatibility between access and data nodes, which can run different versions of TimescaleDB."
1080,Alexander Kuzmenkov,2021-12-10 08:32:30-06,beb8527defc766543fe0e22de0250a876e35fc42,Don't excessively look up the extension oid,Use the one from the static variable. This improves performance by avoiding excessive catalog lookups.
1081,Sven Klemm,2021-12-11 05:05:00-06,d437c70df8320a19ee43d9b2b809f624931d35c4,Enable downgrade test for PG14 packages,"When initially adding the package tests for PG14 Debian, Ubuntu and RPM packages we couldn't test downgrade with the packages because there was no previous version with PG14 support. Since we now have released a 2nd version with PG14 support we can enable the downgrade test for Debian, Ubuntu and RPM packages."
1082,Alexander Kuzmenkov,2021-12-10 08:29:59-06,1b6d2428bbb804bc72b4aeee7763b4d9cedab178,Don't do unnecessary planning for the relations proven to be empty,This imporves performance.
1083,Sven Klemm,2021-12-10 16:56:39-06,7f494077eda069502723c4e79feea8b34af21bf5,Fix DataNodeScan plans with One-Time Filter,When a query has a filter that only needs to be evaluated once per query it will be represented as a Result node with the filter condition on the Result node and the actual query as child of the result node. find_data_node_scan_state_child did not consider Result node as valid node to contain a DataNodeScan node leading to a `unexpected child node of Append or MergeAppend: 62` for queries that had one-time filter with a subquery.
1084,Aleksander Alekseev,2021-12-06 09:19:33-06,958040699c4986a98fab788bafd599d24281f1d8,Monthly buckets support in CAGGs,"This patch allows using time_bucket_ng(""N month"", ...) in CAGGs. Users can also specify years, or months AND years. CAGGs on top of distributed hypertables are supported as well."
1085,Sven Klemm,2021-12-10 16:01:20-06,1b4780df3127565991a7a6e7436f3743663c0a6d,Fix assertion failure in cursor_fetcher_rewind,The code in cursor_fetcher_rewind asserted that there always is an associated request which is not true if EOF was reached already. Found by sqlsmith.  Fixes #3786
1086,Alexander Kuzmenkov,2021-12-10 08:26:09-06,babcc8e9ee05219a638ff3a46b9779fac87d1778,Use function oids instead of names when transforming the sort keys,"This is more explicit, and looking up the function name each time has a performance impact."
1087,Dmitry Simonenko,2021-12-13 05:29:24-06,519e5de21ce7ee5b88fde98e1646f36c45b8e1bb,Fix role type deparsing for GRANT command,"This change fixes GRANT command deparsing by including handling of the special role types such as: PUBLIC, CURRENT_USER, SESSION_USER and CURRENT_ROLE (PG14).  Fix #3910"
1088,Sven Klemm,2021-12-12 19:14:24-06,765d7375cea124f9a39ab1497a0f062eefc493d5,Fix segfault on insert into internal compressed table,When trying to insert into the internal compressed hypertable timescaledb would segfault. This patch blocks direct inserts into the internal compressed hypertable through our tuple routing. Internally we don't use this code path for compression as we create chunks explicitly and insert directly into those chunks in compress_chunk.  Fixes #3920
1089,Mats Kindahl,2021-12-08 10:12:32-06,99746ed8baee7e3e6a874a584dd72c04f947dbf6,Check custom scan node registration,"When loading several different versions of shared libraries after a restart, it might attempt to register the custom scan nodes several times.  This is fixed by adding a function that checks if the custom scan method was already registered and skips registering it if it was.  Fixes #3901"
1090,Aleksander Alekseev,2021-12-10 07:02:29-06,fd5dc6002c1058b740e1d6695c2386341b03ede4,Support UPPERREL_PARTIAL_DISTINCT stage in fdw_create_upper_paths(),UpperRelationKind enum was extended in the upstream and now can be UPPERREL_PARTIAL_DISTINCT as well. This patch rewrites the switch-case expression in the fdw_create_upper_paths() procedure accordingly.  https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=22c4e88e
1091,Mats Kindahl,2021-12-07 06:45:27-06,b208f5276f16272c8840a0a37b3e741ede453ca8,Remove C language recompress_chunk,"Since we are re-implementing `recompress_chunk` as a PL/SQL function, there is no need to keep the C language version around any more, so we remove it from the code."
1092,Alexander Kuzmenkov,2021-12-10 05:22:10-06,0f81a60cbb7d0aa67a575782a6c1f66c86e5f9ec,Use row-by-row fetcher to enable parallel plans on data nodes,"The row-by-row fetcher is more efficient, so we want to use it when we can -- that is, when the have to read only one table from the data node, without interleaving it with anything else. This patch adds an option of choosing the fetcher type automatically. It detects the simplest case of only one distributed table in the entire query, and enables row-by-row fetcher. For other cases, the cursor fetcher is used."
1093,Nikhil Sontakke,2021-12-10 03:40:58-06,7f2f7e922e3506c0de86b8507611cced7b22b722,Fix use-after-free crash in remote heal code,Address sanitizer reported a crash due to a use-after-free code block. Fixed that and verified with a subsequent sanitizer local run.  Fixes #3907
