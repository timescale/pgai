--------------------------------------------------------------------------------
-- 0.4.0



set local search_path = pg_catalog, pg_temp;

/*
make sure that the user doing the install/upgrade is the same user who owns the
schema and migration table. abort the upgrade if different.
*/
do $bootstrap_extension$
declare
    _current_user_id oid = null;
    _schema_owner_id oid = null;
    _migration_table_owner_id oid = null;
begin
    select pg_catalog.to_regrole('@extowner@')::oid
    into strict _current_user_id;

    select pg_namespace.nspowner into strict _schema_owner_id
    from pg_catalog.pg_namespace
    where pg_namespace.nspname operator(pg_catalog.=) 'ai';

    if _schema_owner_id is null then
        -- this should NEVER happen
        -- we have `schema=ai` in the control file, so postgres creates the schema automatically
        -- but this line makes pgspot happy
        create schema ai;
    elseif _schema_owner_id operator(pg_catalog.!=) _current_user_id then
        raise exception 'only the owner of the ai schema may install/upgrade this extension';
        return;
    end if;

    select k.relowner into _migration_table_owner_id
    from pg_catalog.pg_class k
    inner join pg_catalog.pg_namespace n on (k.relnamespace = n.oid)
    where k.relname operator(pg_catalog.=) 'migration'
    and n.nspname operator(pg_catalog.=) 'ai';

    if _migration_table_owner_id is not null
    and _migration_table_owner_id is distinct from _current_user_id then
        raise exception 'only the owner of the ai.migration table can install/upgrade this extension';
        return;
    end if;

    if _migration_table_owner_id is null then
        create table ai.migration
        ( "name" text not null primary key
        , applied_at_version text not null
        , applied_at timestamptz not null default pg_catalog.clock_timestamp()
        , body text not null
        );
    end if;
end;
$bootstrap_extension$;




-------------------------------------------------------------------------------
-- 001-vectorizer.sql
do $outer_migration_block$ /*001-vectorizer.sql*/
declare
    _sql text;
    _migration record;
    _migration_name text = $migration_name$001-vectorizer.sql$migration_name$;
    _migration_body text =
$migration_body$

create table ai.vectorizer
( id int not null primary key generated by default as identity
, source_schema name not null
, source_table name not null
, source_pk jsonb not null
, target_schema name not null
, target_table name not null
, view_schema name not null
, view_name name not null
, trigger_name name not null
, queue_schema name
, queue_table name
, config jsonb not null
, unique (target_schema, target_table)
);
perform pg_catalog.pg_extension_config_dump('ai.vectorizer'::pg_catalog.regclass, '');
perform pg_catalog.pg_extension_config_dump('ai.vectorizer_id_seq'::pg_catalog.regclass, '');

create table ai.vectorizer_errors
( id int not null references ai.vectorizer (id) on delete cascade
, message text
, details jsonb
, recorded timestamptz not null default now()
);
create index on ai.vectorizer_errors (id, recorded);
perform pg_catalog.pg_extension_config_dump('ai.vectorizer'::pg_catalog.regclass, '');


$migration_body$;
begin
    select * into _migration from ai.migration where "name" operator(pg_catalog.=) _migration_name;
    if _migration is not null then
        raise notice 'migration %s already applied. skipping.', _migration_name;
        if _migration.body operator(pg_catalog.!=) _migration_body then
            raise warning 'the contents of migration "%s" have changed', _migration_name;
        end if;
        return;
    end if;
    _sql = pg_catalog.format(E'do /*%s*/ $migration_body$\nbegin\n%s\nend;\n$migration_body$;', _migration_name, _migration_body);
    execute _sql;
    insert into ai.migration ("name", body, applied_at_version)
    values (_migration_name, _migration_body, $version$0.4.0$version$);
end;
$outer_migration_block$;


--------------------------------------------------------------------------------
-- 001-openai.sql

-------------------------------------------------------------------------------
-- openai_tokenize
-- encode text as tokens for a given model
-- https://github.com/openai/tiktoken/blob/main/README.md
create or replace function ai.openai_tokenize(_model text, _text text) returns int[]
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import tiktoken
    encoding = tiktoken.encoding_for_model(_model)
    tokens = encoding.encode(_text)
    return tokens
$python$
language plpython3u strict immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- openai_detokenize
-- decode tokens for a given model back into text
-- https://github.com/openai/tiktoken/blob/main/README.md
create or replace function ai.openai_detokenize(_model text, _tokens int[]) returns text
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import tiktoken
    encoding = tiktoken.encoding_for_model(_model)
    content = encoding.decode(_tokens)
    return content
$python$
language plpython3u strict immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- openai_client_create
-- create the client and store it in the global dictionary for the session
CREATE OR REPLACE FUNCTION ai.openai_client_create(
    _api_key text DEFAULT NULL,
    _organization text DEFAULT NULL,
    _base_url text DEFAULT NULL,
    _timeout float8 DEFAULT NULL,
    _max_retries int DEFAULT NULL,
    _default_headers jsonb DEFAULT NULL,
    _default_query jsonb DEFAULT NULL,
    _http_client jsonb DEFAULT NULL,
    _strict_response_validation boolean DEFAULT NULL
) RETURNS void AS $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai

    if 'openai_client' not in GD:
        GD['openai_client'] = {}

    new_config = ai.openai.prepare_kwargs({
        'api_key': _api_key,
        'organization': _organization,
        'base_url': _base_url,
        'timeout': _timeout,
        'max_retries': _max_retries,
        'default_headers': ai.openai.process_json_input(_default_headers),
        'default_query': ai.openai.process_json_input(_default_query),
        'http_client': ai.openai.process_json_input(_http_client),
        '_strict_response_validation': _strict_response_validation
    })

    if 'config' not in GD['openai_client'] or ai.openai.client_config_changed(GD['openai_client']['config'], new_config):
        client = ai.openai.make_async_client(plpy, **new_config)
        GD['openai_client'] = {
            'client': client,
            'config': new_config
        }

$python$ LANGUAGE plpython3u VOLATILE SECURITY DEFINER PARALLEL UNSAFE;

-------------------------------------------------------------------------------
-- openai_client_destroy
-- remove the client object stored in the global dictionary for the session
CREATE OR REPLACE FUNCTION ai.openai_client_destroy() RETURNS void AS $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    if 'openai_client' in GD:
        del GD['openai_client']
$python$ LANGUAGE plpython3u VOLATILE SECURITY DEFINER PARALLEL UNSAFE;

-------------------------------------------------------------------------------
-- openai_list_models
-- list models supported on the openai platform
-- https://platform.openai.com/docs/api-reference/models/list
create or replace function ai.openai_list_models(
    _api_key text default null,
    _base_url text default null,
    _extra_headers jsonb default null,
    _extra_query jsonb default null,
    _extra_body jsonb default null,
    _timeout float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai
    import json

    # Create async client
    client = ai.openai.get_or_create_client(plpy, GD, _api_key, _base_url)

    # Prepare kwargs for the API call
    kwargs = {}
    # Add extra parameters if provided
    if _extra_headers is not None:
        kwargs['extra_headers'] = json.loads(_extra_headers)
    if _extra_query is not None:
        kwargs['extra_query'] = json.loads(_extra_query)
    if _extra_body is not None:
        kwargs['extra_body'] = json.loads(_extra_body)

    async def async_openai_call(client, kwargs):
        response = await client.models.with_raw_response.list(**kwargs)
        return response.text

    # Execute the API call with cancellation support
    result = ai.openai.execute_with_cancellation(plpy, client, async_openai_call, **kwargs)

    return result
$python$
    language plpython3u volatile parallel unsafe security invoker
                        set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- openai_embed
-- generate an embedding from a text value
-- https://platform.openai.com/docs/api-reference/embeddings/create
create or replace function ai.openai_embed
( _input text
, _model text
, _api_key text default null
, _base_url text default null
, _encoding_format text default null
, _dimensions int default null
, _user text default null
, _extra_headers jsonb default null
, _extra_query jsonb default null
, _extra_body jsonb default null
, _timeout float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai
    import json

    # Create async client
    client = ai.openai.get_or_create_client(plpy, GD, _api_key, _base_url)

    # Prepare kwargs for the API call
    kwargs = ai.openai.prepare_kwargs({
        "input": [_input],
        "model": _model,
        "encoding_format": _encoding_format,
        "dimensions": _dimensions,
        "user": _user,
    })

    # Add extra parameters if provided
    if _extra_headers is not None:
        kwargs['extra_headers'] = json.loads(_extra_headers)
    if _extra_query is not None:
        kwargs['extra_query'] = json.loads(_extra_query)
    if _extra_body is not None:
        kwargs['extra_body'] = json.loads(_extra_body)

    async def async_openai_call(client, kwargs):
        response = await client.embeddings.with_raw_response.create(**kwargs)
        return response.text

    # Execute the API call with cancellation support
    result = ai.openai.execute_with_cancellation(plpy, client, async_openai_call, **kwargs)

    return result
$python$
    language plpython3u volatile parallel unsafe security invoker
                        set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- openai_embed
-- generate embeddings from an array of text values
-- https://platform.openai.com/docs/api-reference/embeddings/create
create or replace function ai.openai_embed
( _input text[]
, _model text
, _api_key text default null
, _base_url text default null
, _encoding_format text default null
, _dimensions int default null
, _user text default null
, _extra_headers jsonb default null
, _extra_query jsonb default null
, _extra_body jsonb default null
, _timeout float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai
    import json

    # Create async client
    client = ai.openai.get_or_create_client(plpy, GD, _api_key, _base_url)

    # Prepare kwargs for the API call
    kwargs = ai.openai.prepare_kwargs({
        "input": _input,
        "model": _model,
        "encoding_format": _encoding_format,
        "dimensions": _dimensions,
        "user": _user,
    })

    # Add extra parameters if provided
    if _extra_headers is not None:
        kwargs['extra_headers'] = json.loads(_extra_headers)
    if _extra_query is not None:
        kwargs['extra_query'] = json.loads(_extra_query)
    if _extra_body is not None:
        kwargs['extra_body'] = json.loads(_extra_body)

    async def async_openai_call(client, kwargs):
        response = await client.embeddings.with_raw_response.create(**kwargs)
        return response.text

    # Execute the API call with cancellation support
    result = ai.openai.execute_with_cancellation(plpy, client, async_openai_call, **kwargs)

    return result
$python$
    language plpython3u volatile parallel unsafe security invoker
                        set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- openai_embed
-- generate embeddings from an array of tokens
-- https://platform.openai.com/docs/api-reference/embeddings/create
create or replace function ai.openai_embed
( _model text
, _input int[]
, _api_key text default null
, _base_url text default null
, _encoding_format text default null
, _dimensions int default null
, _user text default null
, _extra_headers jsonb default null
, _extra_query jsonb default null
, _extra_body jsonb default null
, _timeout float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai
    import json

    # Create async client
    client = ai.openai.get_or_create_client(plpy, GD, _api_key, _base_url)

    # Prepare kwargs for the API call
    kwargs = ai.openai.prepare_kwargs({
        "input": [_input],
        "model": _model,
        "encoding_format": _encoding_format,
        "dimensions": _dimensions,
        "user": _user,
    })

    # Add extra parameters if provided
    if _extra_headers is not None:
        kwargs['extra_headers'] = json.loads(_extra_headers)
    if _extra_query is not None:
        kwargs['extra_query'] = json.loads(_extra_query)
    if _extra_body is not None:
        kwargs['extra_body'] = json.loads(_extra_body)

    async def async_openai_call(client, kwargs):
        response = await client.embeddings.with_raw_response.create(**kwargs)
        return response.text

    # Execute the API call with cancellation support
    result = ai.openai.execute_with_cancellation(plpy, client, async_openai_call, **kwargs)

    return result
$python$
    language plpython3u volatile parallel unsafe security invoker
                        set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- openai_chat_complete
-- text generation / chat completion
-- https://platform.openai.com/docs/api-reference/chat/create
CREATE OR REPLACE FUNCTION ai.openai_chat_complete
( _messages jsonb
, _model text
, _api_key text default null
, _base_url text default null
, _frequency_penalty float8 default null
, _logit_bias jsonb default null
, _logprobs boolean default null
, _top_logprobs int default null
, _max_tokens int default null
, _max_completion_tokens int default null
, _n int default null
, _presence_penalty float8 default null
, _response_format jsonb default null
, _seed int default null
, _stop text default null
, _stream boolean default null
, _temperature float8 default null
, _top_p float8 default null
, _tools jsonb default null
, _tool_choice jsonb default null
, _user text default null
, _metadata jsonb default null
, _service_tier text default null
, _store boolean default null
, _parallel_tool_calls boolean default null
, _extra_headers jsonb default null
, _extra_query jsonb default null
, _extra_body jsonb default null
, _timeout float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai
    import json

    # Process JSON inputs
    messages = json.loads(_messages)
    if not isinstance(messages, list):
        plpy.error("_messages is not an array")

    # Handle _stream parameter since we cannot support it
    stream = False if _stream is None else _stream
    if stream:
        plpy.error("Streaming is not supported in this implementation")

    # Create async client
    client = ai.openai.get_or_create_client(plpy, GD, _api_key, _base_url)

    # Prepare kwargs for the API call
    kwargs = ai.openai.prepare_kwargs({
        "model": _model,
        "messages": messages,
        "frequency_penalty": _frequency_penalty,
        "logit_bias": ai.openai.process_json_input(_logit_bias),
        "logprobs": _logprobs,
        "top_logprobs": _top_logprobs,
        "max_tokens": _max_tokens,
        "max_completion_tokens": _max_completion_tokens,
        "n": _n,
        "presence_penalty": _presence_penalty,
        "response_format": ai.openai.process_json_input(_response_format),
        "seed": _seed,
        "stop": _stop,
        "temperature": _temperature,
        "top_p": _top_p,
        "tools": ai.openai.process_json_input(_tools),
        "tool_choice": ai.openai.process_json_input(_tool_choice),
        "user": _user,
        "metadata": ai.openai.process_json_input(_metadata),
        "service_tier": _service_tier,
        "store": _store,
        "parallel_tool_calls": _parallel_tool_calls,
        "timeout": _timeout,
    })

    # Add extra parameters if provided
    if _extra_headers is not None:
        kwargs['extra_headers'] = json.loads(_extra_headers)
    if _extra_query is not None:
        kwargs['extra_query'] = json.loads(_extra_query)
    if _extra_body is not None:
        kwargs['extra_body'] = json.loads(_extra_body)

    async def async_openai_call(client, kwargs):
        response = await client.chat.completions.with_raw_response.create(**kwargs)
        return response.text

    # Execute the API call with cancellation support
    result = ai.openai.execute_with_cancellation(plpy, client, async_openai_call, **kwargs)

    return result
$python$
    LANGUAGE plpython3u volatile parallel unsafe security invoker
                        SET search_path TO pg_catalog, pg_temp
;

------------------------------------------------------------------------------------
-- openai_chat_complete_simple
-- simple chat completion that only requires a message and only returns the response
create or replace function ai.openai_chat_complete_simple
( _message text
, _api_key text default null
) returns text
as $$
declare
    model text := 'gpt-4o';
    messages jsonb;
begin
    messages := pg_catalog.jsonb_build_array(
        pg_catalog.jsonb_build_object('role', 'system', 'content', 'you are a helpful assistant'),
        pg_catalog.jsonb_build_object('role', 'user', 'content', _message)
    );
    return ai.openai_chat_complete(model, messages, _api_key)
        operator(pg_catalog.->)'choices'
        operator(pg_catalog.->)0
        operator(pg_catalog.->)'message'
        operator(pg_catalog.->>)'content';
end;
$$ language plpgsql volatile parallel unsafe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- openai_moderate
-- classify text as potentially harmful or not
-- https://platform.openai.com/docs/api-reference/moderations/create
create or replace function ai.openai_moderate
(   _input text,
    _api_key text default null,
    _base_url text default null,
    _model text default null,
    _extra_headers jsonb default null,
    _extra_query jsonb default null,
    _extra_body jsonb default null,
    _timeout float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai
    import json

    # Create async client
    client = ai.openai.get_or_create_client(plpy, GD, _api_key, _base_url)

    # Prepare kwargs for the API call
    kwargs = ai.openai.prepare_kwargs({
        "model": _model,
        "input": _input,
    })

    # Add extra parameters if provided
    if _extra_headers is not None:
        kwargs['extra_headers'] = json.loads(_extra_headers)
    if _extra_query is not None:
        kwargs['extra_query'] = json.loads(_extra_query)
    if _extra_body is not None:
        kwargs['extra_body'] = json.loads(_extra_body)

    async def async_openai_call(client, kwargs):
        response = await client.moderations.with_raw_response.create(**kwargs)
        return response.text

    # Execute the API call with cancellation support
    result = ai.openai.execute_with_cancellation(plpy, client, async_openai_call, **kwargs)

    return result
$python$
    language plpython3u volatile parallel unsafe security invoker
                        set search_path to pg_catalog, pg_temp
;

create or replace function ai.openai_moderate
(   _input text[],
    _api_key text default null,
    _base_url text default null,
    _model text default null,
    _extra_headers jsonb default null,
    _extra_query jsonb default null,
    _extra_body jsonb default null,
    _timeout float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.openai
    import json

    # Create async client
    client = ai.openai.get_or_create_client(plpy, GD, _api_key, _base_url)

    # Prepare kwargs for the API call
    kwargs = ai.openai.prepare_kwargs({
        "model": _model,
        "input": _input,
    })

    # Add extra parameters if provided
    if _extra_headers is not None:
        kwargs['extra_headers'] = json.loads(_extra_headers)
    if _extra_query is not None:
        kwargs['extra_query'] = json.loads(_extra_query)
    if _extra_body is not None:
        kwargs['extra_body'] = json.loads(_extra_body)

    async def async_openai_call(client, kwargs):
        response = await client.moderations.with_raw_response.create(**kwargs)
        return response.text

    # Execute the API call with cancellation support
    result = ai.openai.execute_with_cancellation(plpy, client, async_openai_call, **kwargs)

    return result
$python$
    language plpython3u volatile parallel unsafe security invoker
                        set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 002-ollama.sql

-------------------------------------------------------------------------------
-- ollama_list_models
-- https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models
--
create or replace function ai.ollama_list_models(_host text default null)
returns table
( "name" text
, model text
, size bigint
, digest text
, family text
, format text
, families jsonb
, parent_model text
, parameter_size text
, quantization_level text
, modified_at timestamptz
)
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.ollama
    client = ai.ollama.make_client(plpy, _host)
    import json
    resp = client.list()
    models = resp.get("models")
    if models is None:
        raise StopIteration
    for m in models:
        d = m.get("details")
        yield ( m.get("name")
            , m.get("model")
            , m.get("size")
            , m.get("digest")
            , d.get("family") if d is not None else None
            , d.get("format") if d is not None else None
            , json.dumps(d.get("families")) if d is not None else None
            , d.get("parent_model") if d is not None else None
            , d.get("parameter_size") if d is not None else None
            , d.get("quantization_level") if d is not None else None
            , m.get("modified_at")
        )
$python$
language plpython3u volatile parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- ollama_ps
-- https://github.com/ollama/ollama/blob/main/docs/api.md#list-running-models
create or replace function ai.ollama_ps(_host text default null)
returns table
( "name" text
, model text
, size bigint
, digest text
, parent_model text
, format text
, family text
, families jsonb
, parameter_size text
, quantization_level text
, expires_at timestamptz
, size_vram bigint
)
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.ollama
    client = ai.ollama.make_client(plpy, _host)
    import json
    resp = client.ps()
    models = resp.get("models")
    if models is None:
        raise StopIteration
    for m in models:
        d = m.get("details")
        yield ( m.get("name")
            , m.get("model")
            , m.get("size")
            , m.get("digest")
            , d.get("parent_model") if d is not None else None
            , d.get("format") if d is not None else None
            , d.get("family") if d is not None else None
            , json.dumps(d.get("families")) if d is not None else None
            , d.get("parameter_size") if d is not None else None
            , d.get("quantization_level") if d is not None else None
            , m.get("expires_at")
            , m.get("size_vram")
        )
$python$
language plpython3u volatile parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- ollama_embed
-- https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings
create or replace function ai.ollama_embed
( _model text
, _input text
, _host text default null
, _keep_alive float8 default null
, _options jsonb default null
) returns @extschema:vector@.vector
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.ollama
    client = ai.ollama.make_client(plpy, _host)
    _options_1 = None
    if _options is not None:
        import json
        _options_1 = {k: v for k, v in json.loads(_options).items()}
    resp = client.embeddings(_model, _input, options=_options, keep_alive=_keep_alive)
    return resp.get("embedding")
$python$
language plpython3u immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- ollama_generate
-- https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion
create or replace function ai.ollama_generate
( _model text
, _prompt text
, _host text default null
, _images bytea[] default null
, _keep_alive float8 default null
, _options jsonb default null
, _system text default null
, _template text default null
, _context int[] default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.ollama
    client = ai.ollama.make_client(plpy, _host)

    import json
    args = {}

    if _keep_alive is not None:
        args["keep_alive"] = _keep_alive

    if _options is not None:
        args["options"] = {k: v for k, v in json.loads(_options).items()}

    if _system is not None:
        args["system"] = _system

    if _template is not None:
        args["template"] = _template

    if _context is not None:
        args["context"] = _context

    _images_1 = None
    if _images is not None:
        import base64
        _images_1 = []
        for image in _images:
            _images_1.append(base64.b64encode(image).decode('utf-8'))
        args["images"] = _images_1

    resp = client.generate(_model, _prompt, stream=False, **args)
    return json.dumps(resp)
$python$
language plpython3u volatile parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- ollama_chat_complete
-- https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion
create or replace function ai.ollama_chat_complete
( _model text
, _messages jsonb
, _host text default null
, _keep_alive float8 default null
, _options jsonb default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.ollama
    client = ai.ollama.make_client(plpy, _host)

    import json
    import base64
    args = {}

    if _keep_alive is not None:
        args["keep_alive"] = _keep_alive

    if _options is not None:
        args["options"] = {k: v for k, v in json.loads(_options).items()}

    _messages_1 = json.loads(_messages)
    if not isinstance(_messages_1, list):
        plpy.error("_messages is not an array")

    # the python api expects bytes objects for images
    # decode the base64 encoded images into raw binary
    for message in _messages_1:
        if 'images' in message:
            decoded = [base64.b64decode(image) for image in message["images"]]
            message["images"] = decoded

    resp = client.chat(_model, _messages_1, stream=False, **args)

    return json.dumps(resp)
$python$
language plpython3u volatile parallel safe security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 003-anthropic.sql

-------------------------------------------------------------------------------
-- anthropic_generate
-- https://docs.anthropic.com/en/api/messages
create or replace function ai.anthropic_generate
( _model text
, _messages jsonb
, _max_tokens int default 1024
, _api_key text default null
, _base_url text default null
, _timeout float8 default null
, _max_retries int default null
, _system text default null
, _user_id text default null
, _stop_sequences text[] default null
, _temperature float8 default null
, _tool_choice jsonb default null
, _tools jsonb default null
, _top_k int default null
, _top_p float8 default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.anthropic
    client = ai.anthropic.make_client(plpy, api_key=_api_key, base_url=_base_url, timeout=_timeout, max_retries=_max_retries)

    import json
    _messages_1 = json.loads(_messages)

    args = {}
    if _system is not None:
        args["system"] = _system
    if _user_id is not None:
        args["metadata"] = {"user_id", _user_id}
    if _stop_sequences is not None:
        args["stop_sequences"] = _stop_sequences
    if _temperature is not None:
        args["temperature"] = _temperature
    if _tool_choice is not None:
        args["tool_choice"] = json.dumps(_tool_choice)
    if _tools is not None:
        args["tools"] = json.dumps(_tools)
    if _top_k is not None:
        args["top_k"] = _top_k
    if _top_p is not None:
        args["top_p"] = _top_p

    message = client.messages.create(model=_model, messages=_messages_1, max_tokens=_max_tokens, **args)
    return message.to_json()
$python$
language plpython3u volatile parallel safe security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 004-cohere.sql

-------------------------------------------------------------------------------
-- cohere_list_models
-- https://docs.cohere.com/reference/list-models
create or replace function ai.cohere_list_models
( _api_key text default null
, _endpoint text default null
, _default_only bool default null
)
returns table
( "name" text
, endpoints text[]
, finetuned bool
, context_length int
, tokenizer_url text
, default_endpoints text[]
)
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)

    args = {}
    if _endpoint is not None:
        args["endpoint"] = _endpoint
    if _default_only is not None:
        args["default_only"] = _default_only
    page_token = None
    while True:
        resp = client.models.list(page_size=1000, page_token=page_token, **args)
        for model in resp.models:
            yield (model.name, model.endpoints, model.finetuned, model.context_length, model.tokenizer_url, model.default_endpoints)
        page_token = resp.next_page_token
        if page_token is None:
            break
$python$
language plpython3u volatile parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_tokenize
-- https://docs.cohere.com/reference/tokenize
create or replace function ai.cohere_tokenize(_model text, _text text, _api_key text default null) returns int[]
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)

    response = client.tokenize(text=_text, model=_model)
    return response.tokens
$python$
language plpython3u immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_detokenize
-- https://docs.cohere.com/reference/detokenize
create or replace function ai.cohere_detokenize(_model text, _tokens int[], _api_key text default null) returns text
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)

    response = client.detokenize(tokens=_tokens, model=_model)
    return response.text
$python$
language plpython3u immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_embed
-- https://docs.cohere.com/reference/embed-1
create or replace function ai.cohere_embed
( _model text
, _input text
, _api_key text default null
, _input_type text default null
, _truncate text default null
) returns @extschema:vector@.vector
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)

    args={}
    if _input_type is not None:
        args["input_type"] = _input_type
    if _truncate is not None:
        args["truncate"] = _truncate
    response = client.embed(texts=[_input], model=_model, **args)
    return response.embeddings[0]
$python$
language plpython3u immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_classify
-- https://docs.cohere.com/reference/classify
create or replace function ai.cohere_classify
( _model text
, _inputs text[]
, _api_key text default null
, _examples jsonb default null
, _truncate text default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)

    import json
    args = {}
    if _examples is not None:
        args["examples"] = json.loads(_examples)
    if _truncate is not None:
        args["truncate"] = _truncate

    response = client.classify(inputs=_inputs, model=_model, **args)
    return response.json()
$python$
language plpython3u immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_classify_simple
-- https://docs.cohere.com/reference/classify
create or replace function ai.cohere_classify_simple
( _model text
, _inputs text[]
, _api_key text default null
, _examples jsonb default null
, _truncate text default null
) returns table
( input text
, prediction text
, confidence float8
)
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)
    import json
    args = {}
    if _examples is not None:
        args["examples"] = json.loads(_examples)
    if _truncate is not None:
        args["truncate"] = _truncate
    response = client.classify(inputs=_inputs, model=_model, **args)
    for x in response.classifications:
        yield x.input, x.prediction, x.confidence
$python$
language plpython3u immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_rerank
-- https://docs.cohere.com/reference/rerank
create or replace function ai.cohere_rerank
( _model text
, _query text
, _documents jsonb
, _api_key text default null
, _top_n integer default null
, _rank_fields text[] default null
, _return_documents bool default null
, _max_chunks_per_doc int default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)
    import json
    args = {}
    if _top_n is not None:
        args["top_n"] = _top_n
    if _rank_fields is not None:
        args["rank_fields"] = _rank_fields
    if _return_documents is not None:
        args["return_documents"] = _return_documents
    if _max_chunks_per_doc is not None:
        args["max_chunks_per_doc"] = _max_chunks_per_doc
    _documents_1 = json.loads(_documents)
    response = client.rerank(model=_model, query=_query, documents=_documents_1, **args)
    return response.json()
$python$ language plpython3u immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_rerank_simple
-- https://docs.cohere.com/reference/rerank
create or replace function ai.cohere_rerank_simple
( _model text
, _query text
, _documents jsonb
, _api_key text default null
, _top_n integer default null
, _max_chunks_per_doc int default null
) returns table
( "index" int
, "document" jsonb
, relevance_score float8
)
as $func$
select *
from pg_catalog.jsonb_to_recordset
(
    ai.cohere_rerank
    ( _model
    , _query
    , _documents
    , _api_key=>_api_key
    , _top_n=>_top_n
    , _return_documents=>true
    , _max_chunks_per_doc=>_max_chunks_per_doc
    ) operator(pg_catalog.->) 'results'
) x("index" int, "document" jsonb, relevance_score float8)
$func$ language sql immutable parallel safe security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- cohere_chat_complete
-- https://docs.cohere.com/reference/chat
create or replace function ai.cohere_chat_complete
( _model text
, _message text
, _api_key text default null
, _preamble text default null
, _chat_history jsonb default null
, _conversation_id text default null
, _prompt_truncation text default null
, _connectors jsonb default null
, _search_queries_only bool default null
, _documents jsonb default null
, _citation_quality text default null
, _temperature float8 default null
, _max_tokens int default null
, _max_input_tokens int default null
, _k int default null
, _p float8 default null
, _seed int default null
, _stop_sequences text[] default null
, _frequency_penalty float8 default null
, _presence_penalty float8 default null
, _tools jsonb default null
, _tool_results jsonb default null
, _force_single_step bool default null
) returns jsonb
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.cohere
    client = ai.cohere.make_client(plpy, _api_key)

    import json
    args = {}
    if _preamble is not None:
        args["preamble"] = _preamble
    if _chat_history is not None:
        args["chat_history"] = json.loads(_chat_history)
    if _conversation_id is not None:
        args["conversation_id"] = _conversation_id
    if _prompt_truncation is not None:
        args["prompt_truncation"] = _prompt_truncation
    if _connectors is not None:
        args["connectors"] = json.loads(_connectors)
    if _search_queries_only is not None:
        args["search_queries_only"] = _search_queries_only
    if _documents is not None:
        args["documents"] = json.loads(_documents)
    if _citation_quality is not None:
        args["citation_quality"] = _citation_quality
    if _temperature is not None:
        args["temperature"] = _temperature
    if _max_tokens is not None:
        args["max_tokens"] = _max_tokens
    if _max_input_tokens is not None:
        args["max_input_tokens"] = _max_input_tokens
    if _k is not None:
        args["k"] = _k
    if _p is not None:
        args["p"] = _p
    if _seed is not None:
        args["seed"] = _seed
    if _stop_sequences is not None:
        args["stop_sequences"] = _stop_sequences
    if _frequency_penalty is not None:
        args["frequency_penalty"] = _frequency_penalty
    if _presence_penalty is not None:
        args["presence_penalty"] = _presence_penalty
    if _tools is not None:
        args["tools"] = json.loads(_tools)
    if _tool_results is not None:
        args["tool_results"] = json.loads(_tool_results)
    if _force_single_step is not None:
        args["force_single_step"] = _force_single_step

    response = client.chat(model=_model, message=_message, **args)
    return response.json()
$python$ language plpython3u volatile parallel safe security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 005-chunking.sql

-------------------------------------------------------------------------------
-- chunking_character_text_splitter
create or replace function ai.chunking_character_text_splitter
( chunk_column name
, chunk_size int default 800
, chunk_overlap int default 400
, separator text default E'\n\n'
, is_separator_regex bool default false
) returns jsonb
as $func$
    select json_object
    ( 'implementation': 'character_text_splitter'
    , 'config_type': 'chunking'
    , 'chunk_column': chunk_column
    , 'chunk_size': chunk_size
    , 'chunk_overlap': chunk_overlap
    , 'separator': separator
    , 'is_separator_regex': is_separator_regex
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- chunking_recursive_character_text_splitter
create or replace function ai.chunking_recursive_character_text_splitter
( chunk_column name
, chunk_size int default 800
, chunk_overlap int default 400
, separators text[] default array[E'\n\n', E'\n', '.', '?', '!', ' ', '']
, is_separator_regex bool default false
) returns jsonb
as $func$
    select json_object
    ( 'implementation': 'recursive_character_text_splitter'
    , 'config_type': 'chunking'
    , 'chunk_column': chunk_column
    , 'chunk_size': chunk_size
    , 'chunk_overlap': chunk_overlap
    , 'separators': separators
    , 'is_separator_regex': is_separator_regex
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_chunking
create or replace function ai._validate_chunking
( config jsonb
, source_schema name
, source_table name
) returns void
as $func$
declare
    _config_type text;
    _implementation text;
    _chunk_column text;
    _found bool;
begin
    if pg_catalog.jsonb_typeof(config) != 'object' then
        raise exception 'chunking config is not a jsonb object';
    end if;

    _config_type = config operator(pg_catalog.->>) 'config_type';
    if _config_type is null or _config_type != 'chunking' then
        raise exception 'invalid config_type for chunking config';
    end if;

    _implementation = config operator(pg_catalog.->>) 'implementation';
    if _implementation is null or _implementation not in ('character_text_splitter', 'recursive_character_text_splitter') then
        raise exception 'invalid chunking config implementation';
    end if;

    _chunk_column = config operator(pg_catalog.->>) 'chunk_column';

    select count(*) > 0 into strict _found
    from pg_catalog.pg_class k
    inner join pg_catalog.pg_namespace n on (k.relnamespace operator(pg_catalog.=) n.oid)
    inner join pg_catalog.pg_attribute a on (k.oid operator(pg_catalog.=) a.attrelid)
    inner join pg_catalog.pg_type y on (a.atttypid operator(pg_catalog.=) y.oid)
    where n.nspname operator(pg_catalog.=) source_schema
    and k.relname operator(pg_catalog.=) source_table
    and a.attnum operator(pg_catalog.>) 0
    and a.attname operator(pg_catalog.=) _chunk_column
    and y.typname in ('text', 'varchar', 'char', 'bpchar')
    ;
    if not _found then
        raise exception 'chunk column in config does not exist in the table: %', _chunk_column;
    end if;
end
$func$ language plpgsql stable security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 006-formatting.sql

-------------------------------------------------------------------------------
-- formatting_python_template
create or replace function ai.formatting_python_template(template text default '$chunk') returns jsonb
as $func$
    select json_object
    ( 'implementation': 'python_template'
    , 'config_type': 'formatting'
    , 'template': template
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_formatting_python_template
create or replace function ai._validate_formatting_python_template
( config jsonb
, source_schema name
, source_table name
) returns void
as $func$
declare
    _template text;
    _found bool;
begin
    select config operator(pg_catalog.->>) 'template'
    into strict _template
    ;
    if not pg_catalog.like(_template, '%$chunk%') then
        raise exception 'template must contain $chunk placeholder';
    end if;

    -- check that no columns on the source table are named "chunk"
    select count(*) > 0 into strict _found
    from pg_catalog.pg_class k
    inner join pg_catalog.pg_namespace n on (k.relnamespace = n.oid)
    inner join pg_catalog.pg_attribute a on (k.oid = a.attrelid)
    where n.nspname operator(pg_catalog.=) source_schema
    and k.relname operator(pg_catalog.=) source_table
    and a.attnum operator(pg_catalog.>) 0
    and a.attname operator(pg_catalog.=) 'chunk'
    ;
    if _found then
        raise exception 'formatting_python_template may not be used when source table has a column named "chunk"';
    end if;
end
$func$ language plpgsql stable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_formatting
create or replace function ai._validate_formatting
( config jsonb
, source_schema name
, source_table name
) returns void
as $func$
declare
    _config_type text;
begin
    if pg_catalog.jsonb_typeof(config) != 'object' then
        raise exception 'formatting config is not a jsonb object';
    end if;

    _config_type = config operator ( pg_catalog.->> ) 'config_type';
    if _config_type is null or _config_type != 'formatting' then
        raise exception 'invalid config_type for formatting config';
    end if;
    case config operator(pg_catalog.->>) 'implementation'
        when 'python_template' then
            perform ai._validate_formatting_python_template
            ( config
            , source_schema
            , source_table
            );
        else
            raise exception 'unrecognized formatting implementation';
    end case;
end
$func$ language plpgsql immutable security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 007-scheduling.sql

-------------------------------------------------------------------------------
-- scheduling_none
create or replace function ai.scheduling_none() returns jsonb
as $func$
    select pg_catalog.jsonb_build_object
    ( 'implementation', 'none'
    , 'config_type', 'scheduling'
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- scheduling_timescaledb
create or replace function ai.scheduling_timescaledb
( schedule_interval interval default interval '5m'
, initial_start timestamptz default null
, fixed_schedule bool default null
, timezone text default null
) returns jsonb
as $func$
    select json_object
    ( 'implementation': 'timescaledb'
    , 'config_type': 'scheduling'
    , 'schedule_interval': schedule_interval
    , 'initial_start': initial_start
    , 'fixed_schedule': fixed_schedule
    , 'timezone': timezone
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;


-------------------------------------------------------------------------------
-- _validate_scheduling
create or replace function ai._validate_scheduling(config jsonb) returns void
as $func$
declare
    _config_type text;
    _implementation text;
begin
    if pg_catalog.jsonb_typeof(config) != 'object' then
        raise exception 'scheduling config is not a jsonb object';
    end if;

    _config_type = config operator ( pg_catalog.->> ) 'config_type';
    if _config_type is null or _config_type != 'scheduling' then
        raise exception 'invalid config_type for scheduling config';
    end if;
    _implementation = config operator(pg_catalog.->>) 'implementation';
    case _implementation
        when 'none' then
            -- ok
        when 'timescaledb' then
            -- ok
        else
            if _implementation is null then
                raise exception 'scheduling implementation not specified';
            else
                raise exception 'unrecognized scheduling implementation: "%"', _implementation;
            end if;
    end case;
end
$func$ language plpgsql immutable security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 008-embedding.sql

-------------------------------------------------------------------------------
-- embedding_openai
create or replace function ai.embedding_openai
( model text
, dimensions int
, chat_user text default null
, api_key_name text default 'OPENAI_API_KEY'
) returns jsonb
as $func$
    select json_object
    ( 'implementation': 'openai'
    , 'config_type': 'embedding'
    , 'model': model
    , 'dimensions': dimensions
    , 'user': chat_user
    , 'api_key_name': api_key_name
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_embedding
create or replace function ai._validate_embedding(config jsonb) returns void
as $func$
declare
    _config_type text;
    _implementation text;
begin
    if pg_catalog.jsonb_typeof(config) != 'object' then
        raise exception 'embedding config is not a jsonb object';
    end if;

    _config_type = config operator ( pg_catalog.->> ) 'config_type';
    if _config_type is null or _config_type != 'embedding' then
        raise exception 'invalid config_type for embedding config';
    end if;
    _implementation = config operator(pg_catalog.->>) 'implementation';
    case _implementation
        when 'openai' then
            -- ok
        else
            if _implementation is null then
                raise exception 'embedding implementation not specified';
            else
                raise exception 'invalid embedding implementation: "%"', _implementation;
            end if;
    end case;
end
$func$ language plpgsql immutable security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 009-indexing.sql

-------------------------------------------------------------------------------
-- indexing_none
create or replace function ai.indexing_none() returns jsonb
as $func$
    select jsonb_build_object
    ( 'implementation', 'none'
    , 'config_type', 'indexing'
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- indexing_diskann
create or replace function ai.indexing_diskann
( min_rows int default 100000
, storage_layout text default null
, num_neighbors int default null
, search_list_size int default null
, max_alpha float8 default null
, num_dimensions int default null
, num_bits_per_dimension int default null
, create_when_queue_empty boolean default true
) returns jsonb
as $func$
    select json_object
    ( 'implementation': 'diskann'
    , 'config_type': 'indexing'
    , 'min_rows': min_rows
    , 'storage_layout': storage_layout
    , 'num_neighbors': num_neighbors
    , 'search_list_size': search_list_size
    , 'max_alpha': max_alpha
    , 'num_dimensions': num_dimensions
    , 'num_bits_per_dimension': num_bits_per_dimension
    , 'create_when_queue_empty': create_when_queue_empty
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_indexing_diskann
create or replace function ai._validate_indexing_diskann(config jsonb) returns void
as $func$
declare
    _storage_layout text;
begin
    _storage_layout = config operator(pg_catalog.->>) 'storage_layout';
    if _storage_layout is not null and not (_storage_layout operator(pg_catalog.=) any(array['memory_optimized', 'plain'])) then
        raise exception 'invalid storage_layout';
    end if;
end
$func$ language plpgsql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- indexing_hnsw
create or replace function ai.indexing_hnsw
( min_rows int default 100000
, opclass text default 'vector_cosine_ops'
, m int default null
, ef_construction int default null
, create_when_queue_empty boolean default true
) returns jsonb
as $func$
    select json_object
    ( 'implementation': 'hnsw'
    , 'config_type': 'indexing'
    , 'min_rows': min_rows
    , 'opclass': opclass
    , 'm': m
    , 'ef_construction': ef_construction
    , 'create_when_queue_empty': create_when_queue_empty
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_indexing_hnsw
create or replace function ai._validate_indexing_hnsw(config jsonb) returns void
as $func$
declare
    _opclass text;
begin
    _opclass = config operator(pg_catalog.->>) 'opclass';
    if _opclass is not null
    and not (_opclass operator(pg_catalog.=) any(array['vector_ip_ops', 'vector_cosine_ops', 'vector_l1_ops'])) then
        raise exception 'invalid opclass';
    end if;
end
$func$ language plpgsql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_indexing
create or replace function ai._validate_indexing(config jsonb) returns void
as $func$
declare
    _config_type text;
    _implementation text;
begin
    if pg_catalog.jsonb_typeof(config) != 'object' then
        raise exception 'indexing config is not a jsonb object';
    end if;

    _config_type = config operator ( pg_catalog.->> ) 'config_type';
    if _config_type is null or _config_type != 'indexing' then
        raise exception 'invalid config_type for indexing config';
    end if;
    _implementation = config operator(pg_catalog.->>) 'implementation';
    case _implementation
        when 'none' then
            -- ok
        when 'diskann' then
            perform ai._validate_indexing_diskann(config);
        when 'hnsw' then
            perform ai._validate_indexing_hnsw(config);
        else
            if _implementation is null then
                raise exception 'indexing implementation not specified';
            else
                raise exception 'invalid indexing implementation: "%"', _implementation;
            end if;
    end case;
end
$func$ language plpgsql immutable security invoker
set search_path to pg_catalog, pg_temp
;




--------------------------------------------------------------------------------
-- 010-processing.sql

-------------------------------------------------------------------------------
-- processing_default
create or replace function ai.processing_default
( batch_size int default null
, concurrency int default null
) returns jsonb
as $func$
    select json_object
    ( 'implementation': 'default'
    , 'config_type': 'processing'
    , 'batch_size': batch_size
    , 'concurrency': concurrency
    absent on null
    )
$func$ language sql immutable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _validate_processing
create or replace function ai._validate_processing(config jsonb) returns void
as $func$
declare
    _config_type text;
    _implementation text;
    _val jsonb;
begin
    if pg_catalog.jsonb_typeof(config) != 'object' then
        raise exception 'processing config is not a jsonb object';
    end if;

    _config_type = config operator ( pg_catalog.->> ) 'config_type';
    if _config_type is null or _config_type != 'processing' then
        raise exception 'invalid config_type for processing config';
    end if;
    _implementation = config operator(pg_catalog.->>) 'implementation';
    case _implementation
        when 'default' then
            _val = pg_catalog.jsonb_extract_path(config, 'batch_size');
            if _val is not null then
                if pg_catalog.jsonb_typeof(_val) operator(pg_catalog.!=) 'number' then
                    raise exception 'batch_size must be a number';
                end if;
                if cast(_val as int) > 2048 then
                    raise exception 'batch_size must be less than or equal to 2048';
                end if;
                if cast(_val as int) < 1 then
                    raise exception 'batch_size must be greater than 0';
                end if;
            end if;

            _val = pg_catalog.jsonb_extract_path(config, 'concurrency');
            if _val is not null then
                if pg_catalog.jsonb_typeof(_val) operator(pg_catalog.!=) 'number' then
                    raise exception 'concurrency must be a number';
                end if;
                if cast(_val as int) > 50 then
                    raise exception 'concurrency must be less than or equal to 50';
                end if;
                if cast(_val as int) < 1 then
                    raise exception 'concurrency must be greater than 0';
                end if;
            end if;
        else
            if _implementation is null then
                raise exception 'processing implementation not specified';
            else
                raise exception 'unrecognized processing implementation: "%"', _implementation;
            end if;
    end case;
end
$func$ language plpgsql immutable security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 011-vectorizer-int.sql

-------------------------------------------------------------------------------
-- _vectorizer_source_pk
create or replace function ai._vectorizer_source_pk(source_table regclass) returns jsonb as
$func$
    select pg_catalog.jsonb_agg(x)
    from
    (
        select e.attnum, e.pknum, a.attname, y.typname
        from pg_catalog.pg_constraint k
        cross join lateral pg_catalog.unnest(k.conkey) with ordinality e(attnum, pknum)
        inner join pg_catalog.pg_attribute a
            on (k.conrelid operator(pg_catalog.=) a.attrelid
                and e.attnum operator(pg_catalog.=) a.attnum)
        inner join pg_catalog.pg_type y on (a.atttypid operator(pg_catalog.=) y.oid)
        where k.conrelid operator(pg_catalog.=) source_table
        and k.contype operator(pg_catalog.=) 'p'
    ) x
$func$
language sql stable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_grant_to_source
create or replace function ai._vectorizer_grant_to_source
( source_schema name
, source_table name
, grant_to name[]
) returns void as
$func$
declare
    _sql text;
begin
    if grant_to is not null then
        -- grant usage on source schema to grant_to roles
        select pg_catalog.format
        ( $sql$grant usage on schema %I to %s$sql$
        , source_schema
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;

        -- grant select on source table to grant_to roles
        select pg_catalog.format
        ( $sql$grant select on %I.%I to %s$sql$
        , source_schema
        , source_table
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;
    end if;
end;
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_grant_to_vectorizer
create or replace function ai._vectorizer_grant_to_vectorizer(grant_to name[]) returns void as
$func$
declare
    _sql text;
begin
    if grant_to is not null then
        -- grant usage on schema ai to grant_to roles
        select pg_catalog.format
        ( $sql$grant usage on schema ai to %s$sql$
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;

        -- grant select on vectorizer table to grant_to roles
        select pg_catalog.format
        ( $sql$grant select on ai.vectorizer to %s$sql$
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;
    end if;
end;
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_create_target_table
create or replace function ai._vectorizer_create_target_table
( source_schema name
, source_table name
, source_pk jsonb
, target_schema name
, target_table name
, dimensions int
, grant_to name[]
) returns void as
$func$
declare
    _pk_cols text;
    _sql text;
begin
    select pg_catalog.string_agg(pg_catalog.format('%I', x.attname), ', ' order by x.pknum)
    into strict _pk_cols
    from pg_catalog.jsonb_to_recordset(source_pk) x(pknum int, attname name)
    ;
    select pg_catalog.format
    ( $sql$
    create table %I.%I
    ( embedding_uuid uuid not null primary key default pg_catalog.gen_random_uuid()
    , %s
    , chunk_seq int not null
    , chunk text not null
    , embedding @extschema:vector@.vector(%L) not null
    , unique (%s, chunk_seq)
    , foreign key (%s) references %I.%I (%s) on delete cascade
    )
    $sql$
    , target_schema, target_table
    , (
        select pg_catalog.string_agg
        (
            pg_catalog.format
            ( '%I %s not null'
            , x.attname
            , x.typname
            )
            , E'\n, '
            order by x.attnum
        )
        from pg_catalog.jsonb_to_recordset(source_pk)
            x(attnum int, attname name, typname name)
      )
    , dimensions
    , _pk_cols
    , _pk_cols
    , source_schema, source_table
    , _pk_cols
    ) into strict _sql
    ;
    execute _sql;

    if grant_to is not null then
        -- grant usage on target schema to grant_to roles
        select pg_catalog.format
        ( $sql$grant usage on schema %I to %s$sql$
        , target_schema
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;

        -- grant select, insert, update on target table to grant_to roles
        select pg_catalog.format
        ( $sql$grant select, insert, update on %I.%I to %s$sql$
        , target_schema
        , target_table
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;
    end if;
end;
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_create_view
create or replace function ai._vectorizer_create_view
( view_schema name
, view_name name
, source_schema name
, source_table name
, source_pk jsonb
, target_schema name
, target_table name
, grant_to name[]
) returns void as
$func$
declare
    _sql text;
begin
    select pg_catalog.format
    ( $sql$
    create view %I.%I as
    select
      t.embedding_uuid
    , t.chunk_seq
    , t.chunk
    , t.embedding
    , %s
    from %I.%I t
    left outer join %I.%I s
    on (%s)
    $sql$
    , view_schema, view_name
    , (
        -- take primary keys from the target table and other columns from source
        -- this allows for join removal optimization
        select pg_catalog.string_agg
        (
            pg_catalog.format
            ( '%s.%I'
            , case when x.attnum is not null then 't' else 's' end
            , a.attname
            )
            , E'\n    , '
            order by a.attnum
        )
        from pg_catalog.pg_attribute a
        left outer join pg_catalog.jsonb_to_recordset(source_pk) x(attnum int) on (a.attnum operator(pg_catalog.=) x.attnum)
        where a.attrelid operator(pg_catalog.=) pg_catalog.format('%I.%I', source_schema, source_table)::regclass::oid
        and a.attnum operator(pg_catalog.>) 0
      )
    , target_schema, target_table
    , source_schema, source_table
    , (
        select pg_catalog.string_agg
        (
            pg_catalog.format
            ( 't.%s = s.%s'
            , x.attname
            , x.attname
            )
            , ' and '
            order by x.pknum
        )
        from pg_catalog.jsonb_to_recordset(source_pk)
            x(pknum int, attname name)
      )
    ) into strict _sql;
    execute _sql;

    if grant_to is not null then
        -- grant usage on view schema to grant_to roles
        select pg_catalog.format
        ( $sql$grant usage on schema %I to %s$sql$
        , view_schema
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;

        -- grant select on view to grant_to roles
        select pg_catalog.format
        ( $sql$grant select on %I.%I to %s$sql$
        , view_schema
        , view_name
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;
    end if;
end
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_create_queue_table
create or replace function ai._vectorizer_create_queue_table
( queue_schema name
, queue_table name
, source_pk jsonb
, grant_to name[]
) returns void as
$func$
declare
    _sql text;
begin
    -- create the table
    select pg_catalog.format
    ( $sql$create table %I.%I(%s, queued_at timestamptz not null default now())$sql$
    , queue_schema, queue_table
    , (
        select pg_catalog.string_agg
        (
          pg_catalog.format
          ( '%I %s not null'
          , x.attname
          , x.typname
          )
          , E'\n, '
          order by x.attnum
        )
        from pg_catalog.jsonb_to_recordset(source_pk) x(attnum int, attname name, typname name)
      )
    ) into strict _sql
    ;
    execute _sql;

    -- create the index
    select pg_catalog.format
    ( $sql$create index on %I.%I (%s)$sql$
    , queue_schema, queue_table
    , (
        select pg_catalog.string_agg(pg_catalog.format('%I', x.attname), ', ' order by x.pknum)
        from pg_catalog.jsonb_to_recordset(source_pk) x(pknum int, attname name)
      )
    ) into strict _sql
    ;
    execute _sql;

    if grant_to is not null then
        -- grant usage on queue schema to grant_to roles
        select pg_catalog.format
        ( $sql$grant usage on schema %I to %s$sql$
        , queue_schema
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;

        -- grant select, update, delete on queue table to grant_to roles
        select pg_catalog.format
        ( $sql$grant select, insert, update, delete on %I.%I to %s$sql$
        , queue_schema
        , queue_table
        , (
            select pg_catalog.string_agg(pg_catalog.quote_ident(x), ', ')
            from pg_catalog.unnest(grant_to) x
          )
        ) into strict _sql;
        execute _sql;
    end if;
end;
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_create_source_trigger
create or replace function ai._vectorizer_create_source_trigger
( trigger_name name
, queue_schema name
, queue_table name
, source_schema name
, source_table name
, source_pk jsonb
) returns void as
$func$
declare
    _sql text;
begin
    -- create the trigger function
    -- the trigger function is security definer
    -- the owner of the source table is creating the trigger function
    -- so the trigger function is run as the owner of the source table
    -- who also owns the queue table
    -- this means anyone with insert/update on the source is able
    -- to enqueue rows in the queue table automatically
    -- since the trigger function only does inserts, this should be safe
    select pg_catalog.format
    ( $sql$
    create function %I.%I() returns trigger
    as $plpgsql$
    begin
        insert into %I.%I (%s)
        values (%s);
        return null;
    end;
    $plpgsql$ language plpgsql volatile parallel safe security definer
    set search_path to pg_catalog, pg_temp
    $sql$
    , queue_schema, trigger_name
    , queue_schema, queue_table
    , (
        select pg_catalog.string_agg(pg_catalog.format('%I', x.attname), ', ' order by x.attnum)
        from pg_catalog.jsonb_to_recordset(source_pk) x(attnum int, attname name)
      )
    , (
        select pg_catalog.string_agg(pg_catalog.format('new.%I', x.attname), ', ' order by x.attnum)
        from pg_catalog.jsonb_to_recordset(source_pk) x(attnum int, attname name)
      )
    ) into strict _sql
    ;
    execute _sql;

    -- revoke all on trigger function from public
    select pg_catalog.format
    ( $sql$
    revoke all on function %I.%I() from public
    $sql$
    , queue_schema, trigger_name
    ) into strict _sql
    ;
    execute _sql;

    -- create the trigger on the source table
    select pg_catalog.format
    ( $sql$
    create trigger %I
    after insert or update
    on %I.%I
    for each row execute function %I.%I();
    $sql$
    , trigger_name
    , source_schema, source_table
    , queue_schema, trigger_name
    ) into strict _sql
    ;
    execute _sql;
end;
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_vector_index_exists
create or replace function ai._vectorizer_vector_index_exists
( target_schema name
, target_table name
, indexing jsonb
) returns bool as
$func$
declare
    _implementation text;
    _found bool;
begin
    _implementation = pg_catalog.jsonb_extract_path_text(indexing, 'implementation');
    if _implementation not in ('diskann', 'hnsw') then
        raise exception 'unrecognized index implementation: %s', _implementation;
    end if;

    -- look for an index on the target table where the indexed column is the "embedding" column
    -- and the index is using the correct implementation
    select pg_catalog.count(*) filter
    ( where pg_catalog.pg_get_indexdef(i.indexrelid)
      ilike pg_catalog.concat('% using ', _implementation, ' %')
    ) > 0 into _found
    from pg_catalog.pg_class k
    inner join pg_catalog.pg_namespace n on (k.relnamespace operator(pg_catalog.=) n.oid)
    inner join pg_index i on (k.oid operator(pg_catalog.=) i.indrelid)
    inner join pg_catalog.pg_attribute a
        on (k.oid operator(pg_catalog.=) a.attrelid
        and a.attname operator(pg_catalog.=) 'embedding'
        and a.attnum operator(pg_catalog.=) i.indkey[0]
        )
    where n.nspname operator(pg_catalog.=) target_schema
    and k.relname operator(pg_catalog.=) target_table
    ;
    return coalesce(_found, false);
end
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_should_create_vector_index
create or replace function ai._vectorizer_should_create_vector_index(vectorizer ai.vectorizer) returns boolean
as $func$
declare
    _indexing jsonb;
    _implementation text;
    _create_when_queue_empty bool;
    _sql text;
    _count bigint;
    _min_rows bigint;
begin
    -- grab the indexing config
    _indexing = pg_catalog.jsonb_extract_path(vectorizer.config, 'indexing');
    if _indexing is null then
        return false;
    end if;

    -- grab the indexing config's implementation
    _implementation = pg_catalog.jsonb_extract_path_text(_indexing, 'implementation');
    -- if implementation is missing or none, exit
    if _implementation is null or _implementation = 'none' then
        return false;
    end if;

    -- see if the index already exists. if so, exit
    if ai._vectorizer_vector_index_exists(vectorizer.target_schema, vectorizer.target_table, _indexing) then
        return false;
    end if;

    -- if flag set, only attempt to create the vector index if the queue table is empty
    _create_when_queue_empty = coalesce(pg_catalog.jsonb_extract_path(_indexing, 'create_when_queue_empty')::boolean, true);
    if _create_when_queue_empty then
        -- count the rows in the queue table
        select pg_catalog.format
        ( $sql$select pg_catalog.count(1) from %I.%I limit 1$sql$
        , vectorizer.queue_schema
        , vectorizer.queue_table
        ) into strict _sql
        ;
        execute _sql into _count;
        if _count operator(pg_catalog.>) 0 then
            raise notice 'queue for %.% is not empty. skipping vector index creation', vectorizer.target_schema, vectorizer.target_table;
            return false;
        end if;
    end if;

    -- if min_rows has a value
    _min_rows = coalesce(pg_catalog.jsonb_extract_path_text(_indexing, 'min_rows')::bigint, 0);
    if _min_rows > 0 then
        -- count the rows in the target table
        select pg_catalog.format
        ( $sql$select pg_catalog.count(*) from (select 1 from %I.%I limit %L) x$sql$
        , vectorizer.target_schema
        , vectorizer.target_table
        , _min_rows
        ) into strict _sql
        ;
        execute _sql into _count;
    end if;

    -- if we have met or exceeded min_rows, create the index
    return coalesce(_count, 0) >= _min_rows;
end
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_create_vector_index
create or replace function ai._vectorizer_create_vector_index
( target_schema name
, target_table name
, indexing jsonb
) returns void as
$func$
declare
    _key1 int = 1982010642;
    _key2 int;
    _implementation text;
    _with_count bigint;
    _with text;
    _ext_schema name;
    _sql text;
begin

    -- use the target table's oid as the second key for the advisory lock
    select k.oid::int into strict _key2
    from pg_catalog.pg_class k
    inner join pg_catalog.pg_namespace n on (k.relnamespace operator(pg_catalog.=) n.oid)
    where k.relname operator(pg_catalog.=) target_table
    and n.nspname operator(pg_catalog.=) target_schema
    ;

    -- try to grab a transaction-level advisory lock specific to the target table
    -- if we get it, no one else is building the vector index. proceed
    -- if we don't get it, someone else is already working on it. abort
    if not pg_catalog.pg_try_advisory_xact_lock(_key1, _key2) then
        raise warning 'another process is already building a vector index on %.%', target_schema, target_table;
        return;
    end if;

    -- double-check that the index doesn't exist now that we're holding the advisory lock
    -- nobody likes redundant indexes
    if ai._vectorizer_vector_index_exists(target_table, target_schema, indexing) then
        raise notice 'the vector index on %.% already exists', target_schema, target_table;
        return;
    end if;

    _implementation = pg_catalog.jsonb_extract_path_text(indexing, 'implementation');
    case _implementation
        when 'diskann' then
            select
              pg_catalog.count(*)
            , pg_catalog.string_agg
              ( case w.key
                  when 'storage_layout' then pg_catalog.format('%s=%L', w.key, w.value)
                  when 'max_alpha' then pg_catalog.format('%s=%s', w.key, w.value::float8)
                  else pg_catalog.format('%s=%s', w.key, w.value::int)
                end
              , ', '
              )
            into strict
              _with_count
            , _with
            from pg_catalog.jsonb_each_text(indexing) w
            where w.key in
            ( 'storage_layout'
            , 'num_neighbors'
            , 'search_list_size'
            , 'max_alpha'
            , 'num_dimensions'
            , 'num_bits_per_dimension'
            )
            ;

            select pg_catalog.format
            ( $sql$create index on %I.%I using diskann (embedding)%s$sql$
            , target_schema, target_table
            , case when _with_count operator(pg_catalog.>) 0
                then pg_catalog.format(' with (%s)', _with)
                else ''
              end
            ) into strict _sql;
            execute _sql;
        when 'hnsw' then
            select
              pg_catalog.count(*)
            , pg_catalog.string_agg(pg_catalog.format('%s=%s', w.key, w.value::int), ', ')
            into strict
              _with_count
            , _with
            from pg_catalog.jsonb_each_text(indexing) w
            where w.key in ('m', 'ef_construction')
            ;

            select n.nspname into strict _ext_schema
            from pg_catalog.pg_extension x
            inner join pg_catalog.pg_namespace n on (x.extnamespace operator(pg_catalog.=) n.oid)
            where x.extname operator(pg_catalog.=) 'vector'
            ;

            select pg_catalog.format
            ( $sql$create index on %I.%I using hnsw (embedding %I.%s)%s$sql$
            , target_schema, target_table
            , _ext_schema
            , indexing operator(pg_catalog.->>) 'opclass'
            , case when _with_count operator(pg_catalog.>) 0
                then pg_catalog.format(' with (%s)', _with)
                else ''
              end
            ) into strict _sql;
            execute _sql;
        else
            raise exception 'unrecognized index implementation: %s', _implementation;
    end case;
end
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- _vectorizer_job
create or replace procedure ai._vectorizer_job
( job_id int default null
, config jsonb default null
) as
$func$
declare
    _vectorizer_id int;
    _vec ai.vectorizer%rowtype;
    _sql text;
    _found bool;
    _count bigint;
begin
    set local search_path = pg_catalog, pg_temp;
    if config is null then
        raise exception 'config is null';
    end if;

    -- get the vectorizer id from the config
    select pg_catalog.jsonb_extract_path_text(config, 'vectorizer_id')::int
    into strict _vectorizer_id
    ;

    -- get the vectorizer
    select * into strict _vec
    from ai.vectorizer v
    where v.id operator(pg_catalog.=) _vectorizer_id
    ;

    commit;
    set local search_path = pg_catalog, pg_temp;

    -- if the conditions are right, create the vectorizer index
    if ai._vectorizer_should_create_vector_index(_vec) then
        commit;
        set local search_path = pg_catalog, pg_temp;
        perform ai._vectorizer_create_vector_index
        (_vec.target_schema
        , _vec.target_table
        , pg_catalog.jsonb_extract_path(_vec.config, 'indexing')
        );
    end if;

    commit;
    set local search_path = pg_catalog, pg_temp;

    -- if there is at least one item in the queue, we need to execute the vectorizer
    select pg_catalog.format
    ( $sql$
    select true
    from %I.%I
    for update skip locked
    limit 1
    $sql$
    , _vec.queue_schema, _vec.queue_table
    ) into strict _sql
    ;
    execute _sql into _found;
    commit;
    set local search_path = pg_catalog, pg_temp;
    if coalesce(_found, false) is true then
        -- count total items in the queue
        select pg_catalog.format
        ( $sql$select pg_catalog.count(1) from (select 1 from %I.%I limit 501) $sql$
        , _vec.queue_schema, _vec.queue_table
        ) into strict _sql
        ;
        execute _sql into strict _count;
        commit;
        set local search_path = pg_catalog, pg_temp;
        -- for every 50 items in the queue, execute a vectorizer max out at 10 vectorizers
        _count = least(pg_catalog.ceil(_count::float8 / 50.0::float8), 10::float8)::bigint;
        raise debug 'job_id %: executing % vectorizers...', job_id, _count;
        while _count > 0 loop
            -- execute the vectorizer
            perform ai.execute_vectorizer(_vectorizer_id);
            _count = _count - 1;
        end loop;
    end if;
    commit;
    set local search_path = pg_catalog, pg_temp;
end
$func$
language plpgsql security invoker
;

-------------------------------------------------------------------------------
-- _vectorizer_schedule_job
create or replace function ai._vectorizer_schedule_job
( vectorizer_id int
, scheduling jsonb
) returns bigint as
$func$
declare
    _implementation text;
    _sql text;
    _extension_schema name;
    _job_id bigint;
begin
    select pg_catalog.jsonb_extract_path_text(scheduling, 'implementation')
    into strict _implementation
    ;
    case
        when _implementation = 'timescaledb' then
            -- look up schema/name of the extension for scheduling. may be null
            select n.nspname into _extension_schema
            from pg_catalog.pg_extension x
            inner join pg_catalog.pg_namespace n on (x.extnamespace operator(pg_catalog.=) n.oid)
            where x.extname operator(pg_catalog.=) _implementation
            ;
            if _extension_schema is null then
                raise exception 'timescaledb extension not found';
            end if;
        when _implementation = 'none' then
            return null;
        else
            raise exception 'scheduling implementation not recognized';
    end case;

    -- schedule the job using the implementation chosen
    case _implementation
        when 'timescaledb' then
            -- schedule the work proc with timescaledb background jobs
            select pg_catalog.format
            ( $$select %I.add_job('ai._vectorizer_job'::regproc, %s, config=>%L)$$
            , _extension_schema
            , ( -- gather up the arguments
                select string_agg
                ( pg_catalog.format('%s=>%L', s.key, s.value)
                , ', '
                order by x.ord
                )
                from pg_catalog.jsonb_each_text(scheduling) s
                inner join
                unnest(array['schedule_interval', 'initial_start', 'fixed_schedule', 'timezone']) with ordinality x(key, ord)
                on (s.key = x.key)
              )
            , pg_catalog.jsonb_build_object('vectorizer_id', vectorizer_id)::text
            ) into strict _sql
            ;
            execute _sql into strict _job_id;
    end case;
    return _job_id;
end
$func$
language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;



--------------------------------------------------------------------------------
-- 012-vectorizer-api.sql


-------------------------------------------------------------------------------
-- execute_vectorizer
create or replace function ai.execute_vectorizer(vectorizer_id int) returns void
as $python$
    if "ai.version" not in GD:
        r = plpy.execute("select coalesce(pg_catalog.current_setting('ai.python_lib_dir', true), '/usr/local/lib/pgai') as python_lib_dir")
        python_lib_dir = r[0]["python_lib_dir"]
        from pathlib import Path
        python_lib_dir = Path(python_lib_dir).joinpath("0.4.0")
        import site
        site.addsitedir(str(python_lib_dir))
        from ai import __version__ as ai_version
        assert("0.4.0" == ai_version)
        GD["ai.version"] = "0.4.0"
    else:
        if GD["ai.version"] != "0.4.0":
            plpy.fatal("the pgai extension version has changed. start a new session")
    import ai.vectorizer
    ai.vectorizer.execute_vectorizer(plpy, vectorizer_id)
$python$
language plpython3u volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- create_vectorizer
create or replace function ai.create_vectorizer
( source regclass
, embedding jsonb
, chunking jsonb
, indexing jsonb default ai.indexing_diskann()
, formatting jsonb default ai.formatting_python_template()
, scheduling jsonb default ai.scheduling_timescaledb()
, processing jsonb default ai.processing_default()
, target_schema name default null
, target_table name default null
, view_schema name default null
, view_name name default null
, queue_schema name default null
, queue_table name default null
, grant_to name[] default array['tsdbadmin']
, enqueue_existing bool default true
) returns int
as $func$
declare
    _missing_roles name[];
    _source_table name;
    _source_schema name;
    _trigger_name name;
    _is_owner bool;
    _dimensions int;
    _source_pk jsonb;
    _vectorizer_id int;
    _sql text;
    _job_id bigint;
begin
    -- make sure all the roles listed in _grant_to exist
    if grant_to is not null then
        select
          pg_catalog.array_agg(r) filter (where pg_catalog.to_regrole(r) is null) -- missing
        , pg_catalog.array_agg(r) filter (where pg_catalog.to_regrole(r) is not null) -- real roles
        into strict
          _missing_roles
        , grant_to
        from pg_catalog.unnest(grant_to) r
        ;
        if pg_catalog.array_length(_missing_roles, 1) > 0 then
            raise warning 'one or more grant_to roles do not exist: %', _missing_roles;
        end if;
    end if;

    -- get source table name and schema name
    select k.relname, n.nspname, k.relowner operator(pg_catalog.=) current_user::regrole
    into strict _source_table, _source_schema, _is_owner
    from pg_catalog.pg_class k
    inner join pg_catalog.pg_namespace n on (k.relnamespace operator(pg_catalog.=) n.oid)
    where k.oid operator(pg_catalog.=) source
    ;
    -- TODO: consider allowing (in)direct members of the role that owns the source table
    if not _is_owner then
        raise exception 'only the owner of the source table may create a vectorizer on it';
    end if;

    select (embedding operator(pg_catalog.->) 'dimensions')::int into _dimensions;
    if _dimensions is null then
        raise exception '_dimensions argument is required';
    end if;

    -- get the source table's primary key definition
    select ai._vectorizer_source_pk(source) into strict _source_pk;
    if _source_pk is null or pg_catalog.jsonb_array_length(_source_pk) = 0 then
        raise exception 'source table must have a primary key constraint';
    end if;

    _vectorizer_id = pg_catalog.nextval('ai.vectorizer_id_seq'::pg_catalog.regclass);
    target_schema = coalesce(target_schema, _source_schema);
    target_table = coalesce(target_table, pg_catalog.concat(_source_table, '_embedding_store'));
    view_schema = coalesce(view_schema, _source_schema);
    view_name = coalesce(view_name, pg_catalog.concat(_source_table, '_embedding'));
    _trigger_name = pg_catalog.concat('_vectorizer_src_trg_', _vectorizer_id);
    queue_schema = coalesce(queue_schema, 'ai');
    queue_table = coalesce(queue_table, pg_catalog.concat('_vectorizer_q_', _vectorizer_id));

    -- make sure target table name is available
    if pg_catalog.to_regclass(pg_catalog.format('%I.%I', target_schema, target_table)) is not null then
        raise exception 'an object named %.% already exists. specify an alternate target_table explicitly', target_schema, target_schema;
    end if;

    -- make sure queue table name is available
    if pg_catalog.to_regclass(pg_catalog.format('%I.%I', queue_schema, queue_table)) is not null then
        raise exception 'an object named %.% already exists. specify an alternate queue_table explicitly', queue_schema, queue_table;
    end if;

    -- make sure view name is available
    if pg_catalog.to_regclass(pg_catalog.format('%I.%I', view_schema, view_name)) is not null then
        raise exception 'an object named %.% already exists. specify an alternate view_name explicitly', view_schema, view_name;
    end if;

    -- validate the embedding config
    perform ai._validate_embedding(embedding);

    -- validate the chunking config
    perform ai._validate_chunking(chunking, _source_schema, _source_table);

    -- validate the indexing config
    perform ai._validate_indexing(indexing);

    -- validate the formatting config
    perform ai._validate_formatting(formatting, _source_schema, _source_table);

    -- validate the scheduling config
    perform ai._validate_scheduling(scheduling);

    -- validate the processing config
    perform ai._validate_processing(processing);

    -- if scheduling is none then indexing must also be none
    if scheduling operator(pg_catalog.->>) 'implementation' = 'none'
    and indexing operator(pg_catalog.->>) 'implementation' != 'none' then
        raise exception 'automatic indexing is not supported without scheduling. set indexing=>ai.indexing_none() when scheduling=>ai.scheduling_none()';
    end if;

    -- grant select to source table
    perform ai._vectorizer_grant_to_source
    ( _source_schema
    , _source_table
    , grant_to
    );

    -- create the target table
    perform ai._vectorizer_create_target_table
    ( _source_schema
    , _source_table
    , _source_pk
    , target_schema
    , target_table
    , _dimensions
    , grant_to
    );

    -- create queue table
    perform ai._vectorizer_create_queue_table
    ( queue_schema
    , queue_table
    , _source_pk
    , grant_to
    );

    -- create trigger on source table to populate queue
    perform ai._vectorizer_create_source_trigger
    ( _trigger_name
    , queue_schema
    , queue_table
    , _source_schema
    , _source_table
    , _source_pk
    );

    -- create view
    perform ai._vectorizer_create_view
    ( view_schema
    , view_name
    , _source_schema
    , _source_table
    , _source_pk
    , target_schema
    , target_table
    , grant_to
    );

    -- schedule the async ext job
    select ai._vectorizer_schedule_job
    (_vectorizer_id
    , scheduling
    ) into _job_id
    ;
    if _job_id is not null then
        scheduling = pg_catalog.jsonb_insert(scheduling, array['job_id'], to_jsonb(_job_id));
    end if;

    insert into ai.vectorizer
    ( id
    , source_schema
    , source_table
    , source_pk
    , target_schema
    , target_table
    , view_schema
    , view_name
    , trigger_name
    , queue_schema
    , queue_table
    , config
    )
    values
    ( _vectorizer_id
    , _source_schema
    , _source_table
    , _source_pk
    , target_schema
    , target_table
    , view_schema
    , view_name
    , _trigger_name
    , queue_schema
    , queue_table
    , pg_catalog.jsonb_build_object
      ( 'version', '0.4.0'
      , 'embedding', embedding
      , 'chunking', chunking
      , 'indexing', indexing
      , 'formatting', formatting
      , 'scheduling', scheduling
      , 'processing', processing
      )
    );

    -- grant select on the vectorizer table
    perform ai._vectorizer_grant_to_vectorizer(grant_to);

    -- insert into queue any existing rows from source table
    if enqueue_existing is true then
        select pg_catalog.format
        ( $sql$
        insert into %I.%I (%s)
        select %s
        from %I.%I x
        ;
        $sql$
        , queue_schema, queue_table
        , (
            select pg_catalog.string_agg(pg_catalog.format('%I', x.attname), ', ' order by x.attnum)
            from pg_catalog.jsonb_to_recordset(_source_pk) x(attnum int, attname name)
          )
        , (
            select pg_catalog.string_agg(pg_catalog.format('x.%I', x.attname), ', ' order by x.attnum)
            from pg_catalog.jsonb_to_recordset(_source_pk) x(attnum int, attname name)
          )
        , _source_schema, _source_table
        ) into strict _sql
        ;
        execute _sql;
    end if;
    return _vectorizer_id;
end
$func$ language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- disable_vectorizer_schedule
create or replace function ai.disable_vectorizer_schedule(vectorizer_id int) returns void
as $func$
declare
    _vec ai.vectorizer%rowtype;
    _schedule jsonb;
    _job_id bigint;
    _sql text;
begin
    select * into strict _vec
    from ai.vectorizer v
    where v.id operator(pg_catalog.=) vectorizer_id
    ;
    -- enable the scheduled job if exists
    _schedule = _vec.config operator(pg_catalog.->) 'scheduling';
    if _schedule is not null then
        case _schedule operator(pg_catalog.->>) 'implementation'
            when 'none' then -- ok
            when 'timescaledb' then
                _job_id = (_schedule operator(pg_catalog.->) 'job_id')::bigint;
                select pg_catalog.format
                ( $$select %I.alter_job(job_id, scheduled=>false) from timescaledb_information.jobs where job_id = %L$$
                , n.nspname
                , _job_id
                ) into _sql
                from pg_catalog.pg_extension x
                inner join pg_catalog.pg_namespace n on (x.extnamespace = n.oid)
                where x.extname = 'timescaledb'
                ;
                if _sql is not null then
                    execute _sql;
                end if;
        end case;
    end if;
end;
$func$ language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- enable_vectorizer_schedule
create or replace function ai.enable_vectorizer_schedule(vectorizer_id int) returns void
as $func$
declare
    _vec ai.vectorizer%rowtype;
    _schedule jsonb;
    _job_id bigint;
    _sql text;
begin
    select * into strict _vec
    from ai.vectorizer v
    where v.id operator(pg_catalog.=) vectorizer_id
    ;
    -- enable the scheduled job if exists
    _schedule = _vec.config operator(pg_catalog.->) 'scheduling';
    if _schedule is not null then
        case _schedule operator(pg_catalog.->>) 'implementation'
            when 'none' then -- ok
            when 'timescaledb' then
                _job_id = (_schedule operator(pg_catalog.->) 'job_id')::bigint;
                select pg_catalog.format
                ( $$select %I.alter_job(job_id, scheduled=>true) from timescaledb_information.jobs where job_id = %L$$
                , n.nspname
                , _job_id
                ) into _sql
                from pg_catalog.pg_extension x
                inner join pg_catalog.pg_namespace n on (x.extnamespace = n.oid)
                where x.extname = 'timescaledb'
                ;
                if _sql is not null then
                    execute _sql;
                end if;
        end case;
    end if;
end;
$func$ language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- drop_vectorizer
create or replace function ai.drop_vectorizer(vectorizer_id int) returns void
as $func$
/* drop_vectorizer
This function does the following:
1. deletes the scheduled job if any
2. drops the trigger from the source table
3. drops the trigger function
4. drops the queue table
5. deletes the vectorizer row

It does NOT:
1. drop the target table containing the embeddings
2. drop the view joining the target and source
*/
declare
    _vec ai.vectorizer%rowtype;
    _schedule jsonb;
    _job_id bigint;
    _trigger pg_catalog.pg_trigger%rowtype;
    _sql text;
begin
    -- grab the vectorizer we need to drop
    select v.* into strict _vec
    from ai.vectorizer v
    where v.id operator(pg_catalog.=) vectorizer_id
    ;

    -- delete the scheduled job if exists
    _schedule = _vec.config operator(pg_catalog.->) 'scheduling';
    if _schedule is not null then
        case _schedule operator(pg_catalog.->>) 'implementation'
            when 'none' then -- ok
            when 'timescaledb' then
                _job_id = (_schedule operator(pg_catalog.->) 'job_id')::bigint;
                select pg_catalog.format
                ( $$select %I.delete_job(job_id) from timescaledb_information.jobs where job_id = %L$$
                , n.nspname
                , _job_id
                ) into _sql
                from pg_catalog.pg_extension x
                inner join pg_catalog.pg_namespace n on (x.extnamespace = n.oid)
                where x.extname = 'timescaledb'
                ;
                if _sql is not null then
                    execute _sql;
                end if;
        end case;
    end if;

    -- look up the trigger so we can find the function/procedure backing the trigger
    select * into strict _trigger
    from pg_catalog.pg_trigger g
    where g.tgname operator(pg_catalog.=) _vec.trigger_name
    and g.tgrelid operator(pg_catalog.=) pg_catalog.format('%I.%I', _vec.source_schema, _vec.source_table)::regclass::oid
    ;

    -- drop the trigger on the source table
    select pg_catalog.format
    ( $sql$drop trigger %I on %I.%I$sql$
    , _trigger.tgname
    , _vec.source_schema
    , _vec.source_table
    ) into strict _sql
    ;
    execute _sql;

    -- drop the function/procedure backing the trigger
    select pg_catalog.format
    ( $sql$drop %s %I.%I()$sql$
    , case p.prokind when 'f' then 'function' when 'p' then 'procedure' end
    , n.nspname
    , p.proname
    ) into strict _sql
    from pg_catalog.pg_proc p
    inner join pg_catalog.pg_namespace n on (n.oid operator(pg_catalog.=) p.pronamespace)
    where p.oid operator(pg_catalog.=) _trigger.tgfoid
    ;
    execute _sql;

    -- drop the queue table
    select pg_catalog.format
    ( $sql$drop table %I.%I$sql$
    , n.nspname
    , k.relname
    ) into strict _sql
    from pg_catalog.pg_class k
    inner join pg_catalog.pg_namespace n on (k.relnamespace operator(pg_catalog.=) n.oid)
    where k.relname operator(pg_catalog.=) _vec.queue_table
    and n.nspname operator(pg_catalog.=) _vec.queue_schema
    ;
    execute _sql;

    -- delete the vectorizer row
    delete from ai.vectorizer v
    where v.id operator(pg_catalog.=) vectorizer_id
    ;

end;
$func$ language plpgsql volatile security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- vectorizer_queue_pending
create or replace function ai.vectorizer_queue_pending(vectorizer_id int) returns bigint
as $func$
declare
    _queue_schema name;
    _queue_table name;
    _sql text;
    _queue_depth bigint;
begin
    select v.queue_schema, v.queue_table into _queue_schema, _queue_table
    from ai.vectorizer v
    where v.id operator(pg_catalog.=) vectorizer_id
    ;
    if _queue_schema is null or _queue_table is null then
        raise exception 'vectorizer has no queue table';
    end if;
    select format
    ( $sql$select count(*) from %I.%I$sql$
    , _queue_schema, _queue_table
    ) into strict _sql
    ;
    execute _sql into strict _queue_depth;
    return _queue_depth;
end;
$func$ language plpgsql stable security invoker
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- vectorizer_status
create or replace view ai.vectorizer_status as
select
  v.id
, pg_catalog.format('%I.%I', v.source_schema, v.source_table) as source_table
, pg_catalog.format('%I.%I', v.target_schema, v.target_table) as target_table
, pg_catalog.format('%I.%I', v.view_schema, v.view_name) as "view"
, case when v.queue_table is not null then
    ai.vectorizer_queue_pending(v.id)
  else 0
  end as pending_items
from ai.vectorizer v
;



--------------------------------------------------------------------------------
-- 999-privileges.sql

-------------------------------------------------------------------------------
-- grant_ai_usage
create or replace function ai.grant_ai_usage(to_user name, admin bool default false) returns void
as $func$
declare
    _sql text;
begin
    -- schema
    select pg_catalog.format
    ( 'grant %s on schema ai to %I%s'
    , case when admin then 'all privileges' else 'usage, create' end
    , to_user
    , case when admin then ' with grant option' else '' end
    ) into strict _sql
    ;
    raise debug '%', _sql;
    execute _sql;

    -- tables, sequences, and views
    for _sql in
    (
        select pg_catalog.format
        ( 'grant %s on %s %I.%I to %I%s'
        , case
            when admin then 'all privileges'
            else
                case
                    when k.relkind in ('r', 'p') then 'select, insert, update, delete'
                    when k.relkind in ('S') then 'usage, select, update'
                    when k.relkind in ('v') then 'select'
                end
          end
        , case
            when k.relkind in ('r', 'p') then 'table'
            when k.relkind in ('S') then 'sequence'
            when k.relkind in ('v') then ''
          end
        , n.nspname
        , k.relname
        , to_user
        , case when admin then ' with grant option' else '' end
        )
        from pg_catalog.pg_depend d
        inner join pg_catalog.pg_extension e on (d.refobjid operator(pg_catalog.=) e.oid)
        inner join pg_catalog.pg_class k on (d.objid operator(pg_catalog.=) k.oid)
        inner join pg_namespace n on (k.relnamespace operator(pg_catalog.=) n.oid)
        where d.refclassid operator(pg_catalog.=) 'pg_catalog.pg_extension'::pg_catalog.regclass
        and d.deptype operator(pg_catalog.=) 'e'
        and e.extname operator(pg_catalog.=) 'ai'
        and k.relkind in ('r', 'p', 'S', 'v') -- tables, sequences, and views
        and (admin, n.nspname, k.relname) not in
        (
            (false, 'ai', 'migration') -- only admins get any access to this table
        )
        order by n.nspname, k.relname
    )
    loop
        raise debug '%', _sql;
        execute _sql;
    end loop;

    -- procedures and functions
    for _sql in
    (
        select pg_catalog.format
        ( 'grant %s on %s %I.%I(%s) to %I%s'
        , case when admin then 'all privileges' else 'execute' end
        , case k.prokind
              when 'f' then 'function'
              when 'p' then 'procedure'
          end
        , n.nspname
        , k.proname
        , pg_catalog.pg_get_function_identity_arguments(k.oid)
        , to_user
        , case when admin then ' with grant option' else '' end
        )
        from pg_catalog.pg_depend d
        inner join pg_catalog.pg_extension e on (d.refobjid operator(pg_catalog.=) e.oid)
        inner join pg_catalog.pg_proc k on (d.objid operator(pg_catalog.=) k.oid)
        inner join pg_namespace n on (k.pronamespace operator(pg_catalog.=) n.oid)
        where d.refclassid operator(pg_catalog.=) 'pg_catalog.pg_extension'::pg_catalog.regclass
        and d.deptype operator(pg_catalog.=) 'e'
        and e.extname operator(pg_catalog.=) 'ai'
        and k.prokind in ('f', 'p')
        and case
              when k.proname operator(pg_catalog.=) 'grant_ai_usage' then admin -- only admins get this function
              else true
            end
    )
    loop
        raise debug '%', _sql;
        execute _sql;
    end loop;
end
$func$ language plpgsql volatile
security invoker -- gotta have privs to give privs
set search_path to pg_catalog, pg_temp
;

-------------------------------------------------------------------------------
-- grant admin usage to session user and pg_database_owner
select ai.grant_ai_usage(pg_catalog."session_user"(), admin=>true);
select ai.grant_ai_usage('pg_database_owner', admin=>true);

-------------------------------------------------------------------------------
-- revoke everything from public
do language plpgsql $func$
declare
    _sql text;
begin
    -- schema
    revoke all privileges on schema ai from public;

    -- tables, sequences, and views
    for _sql in
    (
        select pg_catalog.format
        ( 'revoke all privileges on %s %I.%I from public'
        , case
            when k.relkind in ('r', 'p') then 'table'
            when k.relkind in ('S') then 'sequence'
            when k.relkind in ('v') then ''
          end
        , n.nspname
        , k.relname
        )
        from pg_catalog.pg_depend d
        inner join pg_catalog.pg_extension e on (d.refobjid operator(pg_catalog.=) e.oid)
        inner join pg_catalog.pg_class k on (d.objid operator(pg_catalog.=) k.oid)
        inner join pg_namespace n on (k.relnamespace operator(pg_catalog.=) n.oid)
        where d.refclassid operator(pg_catalog.=) 'pg_catalog.pg_extension'::pg_catalog.regclass
        and d.deptype operator(pg_catalog.=) 'e'
        and e.extname operator(pg_catalog.=) 'ai'
        and k.relkind in ('r', 'p', 'S', 'v') -- tables, sequences, and views
        order by n.nspname, k.relname
    )
    loop
        raise debug '%', _sql;
        execute _sql;
    end loop;

    -- procedures and functions
    for _sql in
    (
        select pg_catalog.format
        ( 'revoke all privileges on %s %I.%I(%s) from public'
        , case k.prokind
              when 'f' then 'function'
              when 'p' then 'procedure'
          end
        , n.nspname
        , k.proname
        , pg_catalog.pg_get_function_identity_arguments(k.oid)
        )
        from pg_catalog.pg_depend d
        inner join pg_catalog.pg_extension e on (d.refobjid operator(pg_catalog.=) e.oid)
        inner join pg_catalog.pg_proc k on (d.objid operator(pg_catalog.=) k.oid)
        inner join pg_namespace n on (k.pronamespace operator(pg_catalog.=) n.oid)
        where d.refclassid operator(pg_catalog.=) 'pg_catalog.pg_extension'::pg_catalog.regclass
        and d.deptype operator(pg_catalog.=) 'e'
        and e.extname operator(pg_catalog.=) 'ai'
        and k.prokind in ('f', 'p')
    )
    loop
        raise debug '%', _sql;
        execute _sql;
    end loop;
end
$func$;




